<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: https://leo-editor.github.io/leo-editor/leo_toc.html -->
<leo_file xmlns:leo="https://leo-editor.github.io/leo-editor/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20240927151701.1" descendentVnodeUnknownAttributes="7d7100285803000000302e3071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a73735805000000302e302e3771067d71075808000000616e6e6f7461746571087d71092858080000007072696f72697479710a4d0f27580a00000070726973657464617465710b580a000000323032312d30332d3330710c75735803000000302e31710d7d710e580b0000005f5f626f6f6b6d61726b73710f7d7110580700000069735f6475706571114930300a7373752e"><vh>Startup</vh>
<v t="ekr.20240927151701.9" descendentVnodeUnknownAttributes="7d71002858010000003071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a73735803000000302e3771067d71075808000000616e6e6f7461746571087d71092858080000007072696f72697479710a4d0f27580a00000070726973657464617465710b580a000000323032312d30332d3330710c7573752e"><vh>@settings</vh>
<v t="ekr.20240927151701.44"><vh>@bool allow-text-zoom = True</vh></v>
<v t="ekr.20240927151701.45"><vh>@bool check-python-code-on-write = False</vh></v>
<v t="ekr.20240927151701.46"><vh>@bool use-german-keyboard = False</vh></v>
<v t="ekr.20240927151701.47"><vh>@bool use-mouse-expand-gestures = False</vh></v>
<v t="ekr.20240927151701.48"><vh>@data exec-script-commands</vh></v>
<v t="ekr.20240927151701.49"><vh>@data exec-script-patterns</vh></v>
<v t="ekr.20240927151701.50"><vh>@data history-list</vh></v>
<v t="ekr.20240927151701.170" descendentVnodeUnknownAttributes="7d710058010000003071017d71025808000000616e6e6f7461746571037d71042858080000007072696f7269747971054d0f27580a000000707269736574646174657106580a000000323032312d30332d333071077573732e"><vh>@enabled-plugins</vh></v>
<v t="ekr.20240927151701.51"><vh>@string qt-layout-name = legacy</vh></v>
<v t="ekr.20240927151701.63"><vh>Abbreviation settings</vh>
<v t="ekr.20240927151701.64"><vh>@bool enable-abbreviations = True</vh></v>
<v t="ekr.20240927151701.65"><vh>@outline-data tree-abbreviations</vh>
<v t="ekr.20240927151701.66"><vh>@organizer 1</vh>
<v t="ekr.20240927151701.67"><vh>@organizer 2</vh>
<v t="ekr.20240927151701.68"><vh>demo;;</vh>
<v t="ekr.20240927151701.69"><vh>@@button MyDemo @key=Ctrl-9</vh>
<v t="ekr.20240927151701.70"><vh>&lt;&lt; imports &gt;&gt;</vh></v>
<v t="ekr.20240927151701.71"><vh>script_string</vh></v>
<v t="ekr.20240927151701.72"><vh>class myDemo</vh></v>
<v t="ekr.20240927151701.73"><vh>wrappers</vh></v>
</v>
</v>
</v>
<v t="ekr.20240927151701.74"><vh>per-commander-plugin;;</vh>
<v t="ekr.20240927151701.75"><vh>@@file pluginname.py</vh>
<v t="ekr.20240927151701.76"><vh>&lt;&lt; docstring &gt;&gt;</vh></v>
<v t="ekr.20240927151701.77"><vh>&lt;&lt; version history &gt;&gt;</vh></v>
<v t="ekr.20240927151701.78"><vh>&lt;&lt; imports &gt;&gt;</vh></v>
<v t="ekr.20240927151701.79"><vh>init</vh></v>
<v t="ekr.20240927151701.80"><vh>onCreate</vh></v>
<v t="ekr.20240927151701.81"><vh>class pluginController</vh>
<v t="ekr.20240927151701.82"><vh>__init__</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20240927151701.83"><vh>importer;;</vh>
<v t="ekr.20240927151701.84"><vh>@@file importers/{|{x=name}|}.py</vh>
<v t="ekr.20240927151701.85"><vh>class {|{x=cap_name}|}_Importer</vh>
<v t="ekr.20240927151701.86"><vh>{|{x=name}|}.Overrides</vh>
<v t="ekr.20240927151701.87"><vh>{|{x=name}|}.clean_headline</vh></v>
<v t="ekr.20240927151701.88"><vh>{|{x=name}|}.clean_nodes</vh></v>
</v>
</v>
<v t="ekr.20240927151701.89"><vh>class class {|{x=cap_name}|}_ScanState</vh>
<v t="ekr.20240927151701.90"><vh>{|{x=name}|}_state.level</vh></v>
<v t="ekr.20240927151701.91"><vh>{|{x=name}|}_state.update</vh></v>
</v>
</v>
</v>
</v>
</v>
<v t="ekr.20240927151701.92"><vh>Appearance settings</vh>
<v t="ekr.20240927151701.93"><vh>@bool log-pane-wraps = False</vh></v>
<v t="ekr.20240927151701.94"><vh>@bool recent-files-group-always = True</vh></v>
<v t="ekr.20240927151701.95"><vh>@bool show-iconbar = True</vh></v>
<v t="ekr.20240927151701.96"><vh>@bool show-tips = False</vh></v>
<v t="ekr.20240927151701.97"><vh>@bool stayInTreeAfterSelect = True</vh></v>
<v t="ekr.20240927151701.98"><vh>@bool use-chapter-tabs = False</vh></v>
<v t="ekr.20240927151701.99"><vh>@bool use-chapters = False</vh></v>
<v t="ekr.20240927151701.100"><vh>@bool use-gutter = False</vh></v>
<v t="ekr.20240927151701.101"><vh>@int qweb-view-font-size = 30</vh></v>
<v t="ekr.20240927151701.102"><vh>@string initial-split-orientation = v</vh></v>
</v>
<v t="ekr.20240927151701.103"><vh>Coloring settings</vh>
<v t="ekr.20240927151701.104"><vh>@bool color-doc-parts-as-rest = True</vh></v>
<v t="ekr.20240927151701.105"><vh>@bool use-pygments = False</vh></v>
<v t="ekr.20240927151701.106"><vh>@bool use-pygments-styles = False</vh></v>
<v t="ekr.20240927151701.107"><vh>@color head-bg = @mistyrose2</vh></v>
<v t="ekr.20240927151701.108"><vh>@string pygments-style-name = leonine</vh></v>
<v t="ekr.20240927151701.109"><vh>@string target-language = rust</vh></v>
</v>
<v t="ekr.20240927151701.110"><vh>Command settings</vh>
<v t="ekr.20240927151701.111"><vh>@bool create-at-persistence-nodes-automatically = False</vh></v>
<v t="ekr.20240927151701.112"><vh>@bool enable-persistence = False</vh></v>
<v t="ekr.20240927151701.113"><vh>@bool make-node-conflicts-node = True</vh></v>
<v t="ekr.20240927151701.184"><vh>@bool qt-use-scintilla = False</vh></v>
<v t="ekr.20240927151701.114"><vh>@bool run-pyflakes-on-write = False</vh></v>
<v t="ekr.20240927151701.126"><vh>@bool tree-declutter = False</vh></v>
<v t="ekr.20240927151701.115"><vh>@bool use-jedi = False</vh></v>
<v t="ekr.20240927151701.116"><vh>@bool use-qcompleter = False</vh></v>
<v t="ekr.20240927151701.202"><vh>@bool vim-mode = False</vh></v>
<v t="ekr.20240927151701.117"><vh>@bool warn-about-redefined-shortcuts = True</vh></v>
<v t="ekr.20240927151701.118"><vh>@int auto-justify = 80</vh></v>
<v t="ekr.20240927151701.119"><vh>rst3 path options</vh>
<v t="ekr.20240927151701.120"><vh>@string rst3-write-intermediate-extension = .txt</vh></v>
<v t="ekr.20240927151701.121"><vh>@string rst3-default-path = None</vh></v>
<v t="ekr.20240927151701.122"><vh>@string rst3-stylesheet-name = default.css</vh></v>
<v t="ekr.20240927151701.123"><vh>@string rst3-stylesheet-path = None</vh></v>
<v t="ekr.20240927151701.124"><vh>@string rst3-publish-argv-for-missing-stylesheets = None</vh></v>
</v>
</v>
<v t="ekr.20240927151701.144"><vh>File settings</vh>
<v t="ekr.20240927151701.145"><vh>@bool open-with-clean-filenames = True</vh></v>
<v t="ekr.20240927151701.146"><vh>@bool check-for-changed-external-files = True</vh></v>
<v t="ekr.20240927151701.147"><vh>@bool open-with-save-on-update = False</vh></v>
<v t="ekr.20240927151701.148"><vh>@bool open-with-uses-derived-file-extensions = True</vh></v>
</v>
<v t="ekr.20240927151701.149"><vh>Find settings</vh>
<v t="ekr.20240927151701.150"><vh>@bool auto-scroll-find-tab = False</vh></v>
<v t="ekr.20240927151701.151"><vh>@bool close-find-dialog-after-search = False</vh></v>
<v t="ekr.20240927151701.152"><vh>@bool find-ignore-duplicates = False</vh></v>
<v t="ekr.20240927151701.153"><vh>@bool minibuffer-find-mode = True</vh></v>
<v t="ekr.20240927151701.154"><vh>@bool use-find-dialog = False</vh></v>
</v>
<v t="ekr.20240927151701.155"><vh>Importer settings</vh>
<v t="ekr.20240927151701.156"><vh>@data import-html-tags</vh></v>
<v t="ekr.20240927151701.157"><vh>@data import-xml-tags</vh></v>
</v>
<v t="ekr.20240927153018.1"><vh>Scripts</vh>
<v t="ekr.20240927151701.229"><vh> Recursive import script</vh>
<v t="ekr.20240927151701.230"><vh>&lt;&lt; rust dir_list &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20240927151701.190"><vh>Syntax coloring settings</vh>
<v t="ekr.20240927151701.191"><vh>@@color rest.keyword2 = red</vh></v>
<v t="ekr.20240927151701.192"><vh>@@color rest.keyword4 = blue</vh></v>
<v t="ekr.20240927151701.193"><vh>@@color rest.leokeyword = green</vh></v>
<v t="ekr.20240927151701.194"><vh>@color forth.keyword3 = black</vh></v>
<v t="ekr.20240927151701.195"><vh>@color python.name = @solarized-yellow</vh></v>
<v t="ekr.20240927151701.196"><vh>@font rest.comment1</vh></v>
</v>
<v t="ekr.20240927151701.176"><vh>VR settings</vh>
<v t="ekr.20240927151701.177"><vh>@bool view-rendered-auto-create = False</vh></v>
<v t="ekr.20240927151701.178"><vh>@bool view-rendered-auto-hide = False</vh></v>
<v t="ekr.20240927151701.179"><vh>@string view-rendered-default-kind = rst</vh></v>
</v>
</v>
<v t="ekr.20240927151701.203" descendentVnodeUnknownAttributes="7d710058010000003071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a7373732e"><vh>Buttons &amp; commands</vh>
<v t="ekr.20240927151701.206"><vh>@button backup</vh></v>
<v t="ekr.20240928073118.1"><vh>@button cargo-fmt</vh></v>
<v t="ekr.20240927152759.1"><vh>@button cargo-run</vh></v>
<v t="ekr.20240930063740.1"><vh>@@command ga</vh></v>
<v t="ekr.20240930101156.1"><vh>@@command ga-leo</vh></v>
<v t="ekr.20240930063514.1"><vh>@@command gc</vh></v>
<v t="ekr.20240930063546.1"><vh>@command gs</vh></v>
<v t="ekr.20240930064435.1"><vh>@@command git-reset</vh></v>
<v t="ekr.20240930064622.1"><vh>@@command push</vh></v>
<v t="ekr.20240927151701.207"><vh>@@button print-gnx</vh></v>
</v>
</v>
<v t="ekr.20240927164013.1"><vh>--- Pevious code</vh>
<v t="ekr.20240927163405.1"><vh>class Beautifier (from backup/rust_beautifier)</vh>
<v t="ekr.20240927163405.2"><vh>Beautifier::add_input_token</vh></v>
<v t="ekr.20240927163405.3"><vh>Beautifier::add_output_token</vh></v>
<v t="ekr.20240927163405.4"><vh>Beautifier::beautify_all_files</vh></v>
<v t="ekr.20240927163405.5"><vh>Beautifier::beautify_one_file</vh>
<v t="ekr.20240927163405.6"><vh>&lt;&lt; show output_list &gt;&gt;</vh></v>
</v>
<v t="ekr.20240927163405.7"><vh>Beautifier::do_*</vh>
<v t="ekr.20240927163405.8"><vh>Handlers with values</vh>
<v t="ekr.20240927163405.9"><vh>do_Comment</vh></v>
<v t="ekr.20240927163405.10"><vh>do_Complex</vh></v>
<v t="ekr.20240927163405.11"><vh>do_Float</vh></v>
<v t="ekr.20240927163405.12"><vh>do_Int</vh></v>
<v t="ekr.20240927163405.13"><vh>do_Name</vh></v>
<v t="ekr.20240927163405.14"><vh>do_String</vh></v>
</v>
<v t="ekr.20240927163405.15"><vh>Handlers using lws</vh>
<v t="ekr.20240927163405.16"><vh>do_Dedent</vh></v>
<v t="ekr.20240927163405.17"><vh>do_Indent</vh></v>
<v t="ekr.20240927163405.18"><vh>do_Newline</vh></v>
<v t="ekr.20240927163405.19"><vh>do_NonLogicalNewline</vh></v>
</v>
<v t="ekr.20240927163405.20"><vh>Handlers w/o values</vh>
<v t="ekr.20240927163405.21"><vh>do_Amper</vh></v>
<v t="ekr.20240927163405.22"><vh>do_AmperEqual</vh></v>
<v t="ekr.20240927163405.23"><vh>do_And</vh></v>
<v t="ekr.20240927163405.24"><vh>do_As</vh></v>
<v t="ekr.20240927163405.25"><vh>do_Assert</vh></v>
<v t="ekr.20240927163405.26"><vh>do_Async</vh></v>
<v t="ekr.20240927163405.27"><vh>do_At</vh></v>
<v t="ekr.20240927163405.28"><vh>do_AtEqual</vh></v>
<v t="ekr.20240927163405.29"><vh>do_Await</vh></v>
<v t="ekr.20240927163405.30"><vh>do_Break</vh></v>
<v t="ekr.20240927163405.31"><vh>do_Case</vh></v>
<v t="ekr.20240927163405.32"><vh>do_CircumFlex</vh></v>
<v t="ekr.20240927163405.33"><vh>do_CircumflexEqual</vh></v>
<v t="ekr.20240927163405.34"><vh>do_Class</vh></v>
<v t="ekr.20240927163405.35"><vh>do_Colon</vh></v>
<v t="ekr.20240927163405.36"><vh>do_ColonEqual</vh></v>
<v t="ekr.20240927163405.37"><vh>do_Comma</vh></v>
<v t="ekr.20240927163405.38"><vh>do_Continue</vh></v>
<v t="ekr.20240927163405.39"><vh>do_Def</vh></v>
<v t="ekr.20240927163405.40"><vh>do_Del</vh></v>
<v t="ekr.20240927163405.41"><vh>do_Dot</vh></v>
<v t="ekr.20240927163405.42"><vh>do_DoubleSlash</vh></v>
<v t="ekr.20240927163405.43"><vh>do_DoubleSlashEqual</vh></v>
<v t="ekr.20240927163405.44"><vh>do_DoubleStar</vh></v>
<v t="ekr.20240927163405.45"><vh>do_DoubleStarEqual</vh></v>
<v t="ekr.20240927163405.46"><vh>do_Elif</vh></v>
<v t="ekr.20240927163405.47"><vh>do_Ellipsis</vh></v>
<v t="ekr.20240927163405.48"><vh>do_Else</vh></v>
<v t="ekr.20240927163405.49"><vh>do_EndOfFile</vh></v>
<v t="ekr.20240927163405.50"><vh>do_EqEqual</vh></v>
<v t="ekr.20240927163405.51"><vh>do_Equal</vh></v>
<v t="ekr.20240927163405.52"><vh>do_Except</vh></v>
<v t="ekr.20240927163405.53"><vh>do_False</vh></v>
<v t="ekr.20240927163405.54"><vh>do_Finally</vh></v>
<v t="ekr.20240927163405.55"><vh>do_For</vh></v>
<v t="ekr.20240927163405.56"><vh>do_From</vh></v>
<v t="ekr.20240927163405.57"><vh>do_Global</vh></v>
<v t="ekr.20240927163405.58"><vh>do_Greater</vh></v>
<v t="ekr.20240927163405.59"><vh>do_GreaterEqual</vh></v>
<v t="ekr.20240927163405.60"><vh>do_If</vh></v>
<v t="ekr.20240927163405.61"><vh>do_Import</vh></v>
<v t="ekr.20240927163405.62"><vh>do_In</vh></v>
<v t="ekr.20240927163405.63"><vh>do_Is</vh></v>
<v t="ekr.20240927163405.64"><vh>do_Lambda</vh></v>
<v t="ekr.20240927163405.65"><vh>do_Lbrace</vh></v>
<v t="ekr.20240927163405.66"><vh>do_LeftShift</vh></v>
<v t="ekr.20240927163405.67"><vh>do_LeftShiftEqual</vh></v>
<v t="ekr.20240927163405.68"><vh>do_Less</vh></v>
<v t="ekr.20240927163405.69"><vh>do_LessEqual</vh></v>
<v t="ekr.20240927163405.70"><vh>do_Lpar</vh></v>
<v t="ekr.20240927163405.71"><vh>do_Lsqb</vh></v>
<v t="ekr.20240927163405.72"><vh>do_Match</vh></v>
<v t="ekr.20240927163405.73"><vh>do_Minus</vh></v>
<v t="ekr.20240927163405.74"><vh>do_MinusEqual</vh></v>
<v t="ekr.20240927163405.75"><vh>do_None</vh></v>
<v t="ekr.20240927163405.76"><vh>do_Nonlocal</vh></v>
<v t="ekr.20240927163405.77"><vh>do_Not</vh></v>
<v t="ekr.20240927163405.78"><vh>do_NotEqual</vh></v>
<v t="ekr.20240927163405.79"><vh>do_Or</vh></v>
<v t="ekr.20240927163405.80"><vh>do_Pass</vh></v>
<v t="ekr.20240927163405.81"><vh>do_Percent</vh></v>
<v t="ekr.20240927163405.82"><vh>do_PercentEqual</vh></v>
<v t="ekr.20240927163405.83"><vh>do_Plus</vh></v>
<v t="ekr.20240927163405.84"><vh>do_PlusEqual</vh></v>
<v t="ekr.20240927163405.85"><vh>do_Raise</vh></v>
<v t="ekr.20240927163405.86"><vh>do_Rarrow</vh></v>
<v t="ekr.20240927163405.87"><vh>do_Rbrace</vh></v>
<v t="ekr.20240927163405.88"><vh>do_Return</vh></v>
<v t="ekr.20240927163405.89"><vh>do_RightShift</vh></v>
<v t="ekr.20240927163405.90"><vh>do_RightShiftEqual</vh></v>
<v t="ekr.20240927163405.91"><vh>do_Rpar</vh></v>
<v t="ekr.20240927163405.92"><vh>do_Rsqb</vh></v>
<v t="ekr.20240927163405.93"><vh>do_Semi</vh></v>
<v t="ekr.20240927163405.94"><vh>do_Slash</vh></v>
<v t="ekr.20240927163405.95"><vh>do_SlashEqual</vh></v>
<v t="ekr.20240927163405.96"><vh>do_Star</vh></v>
<v t="ekr.20240927163405.97"><vh>do_StarEqual</vh></v>
<v t="ekr.20240927163405.98"><vh>do_StartExpression</vh></v>
<v t="ekr.20240927163405.99"><vh>do_StartInteractive</vh></v>
<v t="ekr.20240927163405.100"><vh>do_StarModule</vh></v>
<v t="ekr.20240927163405.101"><vh>do_Tilde</vh></v>
<v t="ekr.20240927163405.102"><vh>do_True</vh></v>
<v t="ekr.20240927163405.103"><vh>do_Try</vh></v>
<v t="ekr.20240927163405.104"><vh>do_Type</vh></v>
<v t="ekr.20240927163405.105"><vh>do_Vbar</vh></v>
<v t="ekr.20240927163405.106"><vh>do_VbarEqual</vh></v>
<v t="ekr.20240927163405.107"><vh>do_While</vh></v>
<v t="ekr.20240927163405.108"><vh>do_With</vh></v>
<v t="ekr.20240927163405.109"><vh>do_Yield</vh></v>
</v>
</v>
<v t="ekr.20240927163405.110"><vh>Beautifier::enabled</vh></v>
<v t="ekr.20240927163405.111"><vh>Beautifier::get_args</vh></v>
<v t="ekr.20240927163405.112"><vh>Beautifier::make_input_list</vh></v>
<v t="ekr.20240927163405.113"><vh>Beautifier::make_output_list</vh></v>
<v t="ekr.20240927163405.114"><vh>Beautifier::make_output_token</vh></v>
<v t="ekr.20240927163405.115"><vh>Beautifier::new</vh></v>
<v t="ekr.20240927163405.116"><vh>Beautifier::show_args</vh></v>
<v t="ekr.20240927163405.117"><vh>Beautifier::show_help</vh></v>
<v t="ekr.20240927163405.118"><vh>Beautifier::show_output_list</vh></v>
<v t="ekr.20240927163405.119"><vh>Beautifier::tokenize_contents</vh></v>
</v>
<v t="ekr.20240927163812.1"><vh>from leo-editor-contrib rust_beautifier.leo</vh>
<v t="ekr.20240927163855.1"><vh>@command test</vh></v>
<v t="ekr.20240927163907.1"><vh>Read Me</vh></v>
<v t="ekr.20240927163942.1"><vh>## @file src/leotokens.rs</vh>
<v t="ekr.20240927163942.2"><vh>&lt;&lt; leoTokens.rs: global suppressions &gt;&gt;</vh></v>
<v t="ekr.20240927163942.3"><vh>&lt;&lt; leoTokens.rs: use statements &gt;&gt;</vh></v>
<v t="ekr.20240927163942.4"><vh>Classes (leo-editor-contrib)</vh>
<v t="ekr.20240927163942.5"><vh>class Beautifier </vh>
<v t="ekr.20240927163942.6"><vh>Beautifier::add_output_string</vh></v>
<v t="ekr.20240927163942.7"><vh>Beautifier::add_input_token</vh></v>
<v t="ekr.20240927163942.8"><vh>Beautifier::beautify_all_files</vh></v>
<v t="ekr.20240927163942.9"><vh>Beautifier::beautify_one_file</vh>
<v t="ekr.20240927163942.10"><vh>&lt;&lt; show output_list &gt;&gt;</vh></v>
</v>
<v t="ekr.20240927163942.11"><vh>Beautifier::do_*</vh>
<v t="ekr.20240927163942.12"><vh>Handlers with values</vh>
<v t="ekr.20240927163942.13"><vh>do_Comment</vh></v>
<v t="ekr.20240927163942.14"><vh>do_Complex</vh></v>
<v t="ekr.20240927163942.15"><vh>do_Float</vh></v>
<v t="ekr.20240927163942.16"><vh>do_Int</vh></v>
<v t="ekr.20240927163942.17"><vh>do_Name</vh></v>
<v t="ekr.20240927163942.18"><vh>do_String</vh></v>
</v>
<v t="ekr.20240927163942.19"><vh>Handlers using lws</vh>
<v t="ekr.20240927163942.20"><vh>do_Dedent</vh></v>
<v t="ekr.20240927163942.21"><vh>do_Indent</vh></v>
<v t="ekr.20240927163942.22"><vh>do_Newline</vh></v>
<v t="ekr.20240927163942.23"><vh>do_NonLogicalNewline</vh></v>
</v>
<v t="ekr.20240927163942.24"><vh>Handlers w/o values</vh>
<v t="ekr.20240927163942.25"><vh>do_Amper</vh></v>
<v t="ekr.20240927163942.26"><vh>do_AmperEqual</vh></v>
<v t="ekr.20240927163942.27"><vh>do_And</vh></v>
<v t="ekr.20240927163942.28"><vh>do_As</vh></v>
<v t="ekr.20240927163942.29"><vh>do_Assert</vh></v>
<v t="ekr.20240927163942.30"><vh>do_Async</vh></v>
<v t="ekr.20240927163942.31"><vh>do_At</vh></v>
<v t="ekr.20240927163942.32"><vh>do_AtEqual</vh></v>
<v t="ekr.20240927163942.33"><vh>do_Await</vh></v>
<v t="ekr.20240927163942.34"><vh>do_Break</vh></v>
<v t="ekr.20240927163942.35"><vh>do_Case</vh></v>
<v t="ekr.20240927163942.36"><vh>do_CircumFlex</vh></v>
<v t="ekr.20240927163942.37"><vh>do_CircumflexEqual</vh></v>
<v t="ekr.20240927163942.38"><vh>do_Class</vh></v>
<v t="ekr.20240927163942.39"><vh>do_Colon</vh></v>
<v t="ekr.20240927163942.40"><vh>do_ColonEqual</vh></v>
<v t="ekr.20240927163942.41"><vh>do_Comma</vh></v>
<v t="ekr.20240927163942.42"><vh>do_Continue</vh></v>
<v t="ekr.20240927163942.43"><vh>do_Def</vh></v>
<v t="ekr.20240927163942.44"><vh>do_Del</vh></v>
<v t="ekr.20240927163942.45"><vh>do_Dot</vh></v>
<v t="ekr.20240927163942.46"><vh>do_DoubleSlash</vh></v>
<v t="ekr.20240927163942.47"><vh>do_DoubleSlashEqual</vh></v>
<v t="ekr.20240927163942.48"><vh>do_DoubleStar</vh></v>
<v t="ekr.20240927163942.49"><vh>do_DoubleStarEqual</vh></v>
<v t="ekr.20240927163942.50"><vh>do_Elif</vh></v>
<v t="ekr.20240927163942.51"><vh>do_Ellipsis</vh></v>
<v t="ekr.20240927163942.52"><vh>do_Else</vh></v>
<v t="ekr.20240927163942.53"><vh>do_EndOfFile</vh></v>
<v t="ekr.20240927163942.54"><vh>do_EqEqual</vh></v>
<v t="ekr.20240927163942.55"><vh>do_Equal</vh></v>
<v t="ekr.20240927163942.56"><vh>do_Except</vh></v>
<v t="ekr.20240927163942.57"><vh>do_False</vh></v>
<v t="ekr.20240927163942.58"><vh>do_Finally</vh></v>
<v t="ekr.20240927163942.59"><vh>do_For</vh></v>
<v t="ekr.20240927163942.60"><vh>do_From</vh></v>
<v t="ekr.20240927163942.61"><vh>do_Global</vh></v>
<v t="ekr.20240927163942.62"><vh>do_Greater</vh></v>
<v t="ekr.20240927163942.63"><vh>do_GreaterEqual</vh></v>
<v t="ekr.20240927163942.64"><vh>do_If</vh></v>
<v t="ekr.20240927163942.65"><vh>do_Import</vh></v>
<v t="ekr.20240927163942.66"><vh>do_In</vh></v>
<v t="ekr.20240927163942.67"><vh>do_Is</vh></v>
<v t="ekr.20240927163942.68"><vh>do_Lambda</vh></v>
<v t="ekr.20240927163942.69"><vh>do_Lbrace</vh></v>
<v t="ekr.20240927163942.70"><vh>do_LeftShift</vh></v>
<v t="ekr.20240927163942.71"><vh>do_LeftShiftEqual</vh></v>
<v t="ekr.20240927163942.72"><vh>do_Less</vh></v>
<v t="ekr.20240927163942.73"><vh>do_LessEqual</vh></v>
<v t="ekr.20240927163942.74"><vh>do_Lpar</vh></v>
<v t="ekr.20240927163942.75"><vh>do_Lsqb</vh></v>
<v t="ekr.20240927163942.76"><vh>do_Match</vh></v>
<v t="ekr.20240927163942.77"><vh>do_Minus</vh></v>
<v t="ekr.20240927163942.78"><vh>do_MinusEqual</vh></v>
<v t="ekr.20240927163942.79"><vh>do_None</vh></v>
<v t="ekr.20240927163942.80"><vh>do_Nonlocal</vh></v>
<v t="ekr.20240927163942.81"><vh>do_Not</vh></v>
<v t="ekr.20240927163942.82"><vh>do_NotEqual</vh></v>
<v t="ekr.20240927163942.83"><vh>do_Or</vh></v>
<v t="ekr.20240927163942.84"><vh>do_Pass</vh></v>
<v t="ekr.20240927163942.85"><vh>do_Percent</vh></v>
<v t="ekr.20240927163942.86"><vh>do_PercentEqual</vh></v>
<v t="ekr.20240927163942.87"><vh>do_Plus</vh></v>
<v t="ekr.20240927163942.88"><vh>do_PlusEqual</vh></v>
<v t="ekr.20240927163942.89"><vh>do_Raise</vh></v>
<v t="ekr.20240927163942.90"><vh>do_Rarrow</vh></v>
<v t="ekr.20240927163942.91"><vh>do_Rbrace</vh></v>
<v t="ekr.20240927163942.92"><vh>do_Return</vh></v>
<v t="ekr.20240927163942.93"><vh>do_RightShift</vh></v>
<v t="ekr.20240927163942.94"><vh>do_RightShiftEqual</vh></v>
<v t="ekr.20240927163942.95"><vh>do_Rpar</vh></v>
<v t="ekr.20240927163942.96"><vh>do_Rsqb</vh></v>
<v t="ekr.20240927163942.97"><vh>do_Semi</vh></v>
<v t="ekr.20240927163942.98"><vh>do_Slash</vh></v>
<v t="ekr.20240927163942.99"><vh>do_SlashEqual</vh></v>
<v t="ekr.20240927163942.100"><vh>do_Star</vh></v>
<v t="ekr.20240927163942.101"><vh>do_StarEqual</vh></v>
<v t="ekr.20240927163942.102"><vh>do_StartExpression</vh></v>
<v t="ekr.20240927163942.103"><vh>do_StartInteractive</vh></v>
<v t="ekr.20240927163942.104"><vh>do_StarModule</vh></v>
<v t="ekr.20240927163942.105"><vh>do_Tilde</vh></v>
<v t="ekr.20240927163942.106"><vh>do_True</vh></v>
<v t="ekr.20240927163942.107"><vh>do_Try</vh></v>
<v t="ekr.20240927163942.108"><vh>do_Type</vh></v>
<v t="ekr.20240927163942.109"><vh>do_Vbar</vh></v>
<v t="ekr.20240927163942.110"><vh>do_VbarEqual</vh></v>
<v t="ekr.20240927163942.111"><vh>do_While</vh></v>
<v t="ekr.20240927163942.112"><vh>do_With</vh></v>
<v t="ekr.20240927163942.113"><vh>do_Yield</vh></v>
</v>
</v>
<v t="ekr.20240927163942.114"><vh>Beautifier::enabled</vh></v>
<v t="ekr.20240927163942.115"><vh>Beautifier::get_args</vh></v>
<v t="ekr.20240927163942.116"><vh>Beautifier::make_input_list</vh></v>
<v t="ekr.20240927163942.117"><vh>Beautifier::make_output_list</vh></v>
<v t="ekr.20240927163942.118"><vh>Beautifier::new</vh></v>
<v t="ekr.20240927163942.119"><vh>Beautifier::show_args</vh></v>
<v t="ekr.20240927163942.120"><vh>Beautifier::show_help</vh></v>
<v t="ekr.20240927163942.121"><vh>Beautifier::show_output_list</vh></v>
<v t="ekr.20240927163942.122"><vh>Beautifier::tokenize_contents (prototype)</vh></v>
</v>
<v t="ekr.20240927163942.123"><vh>class InputTok</vh></v>
</v>
<v t="ekr.20240927163942.124"><vh>pub fn entry &amp; helpers</vh>
<v t="ekr.20240927163942.125"><vh>fn tokenize</vh>
<v t="ekr.20240927163942.126"><vh>&lt;&lt; tokenize: define contents &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20240927163942.127"><vh>fn print_type &amp; type_of</vh></v>
</v>
</v>
<v t="ekr.20241001073040.1"><vh>--- No longer used</vh>
<v t="ekr.20240930100636.1"><vh>&lt;&lt; 2: lex &gt;&gt;</vh></v>
<v t="ekr.20240930085546.1"><vh>function: lex_contents</vh></v>
<v t="ekr.20240929031635.1"><vh>function: scan_input_list</vh></v>
<v t="ekr.20241001071914.1"><vh>function: test_loop</vh></v>
<v t="ekr.20241001093308.2"><vh>fn tokenize</vh>
<v t="ekr.20241001093308.3"><vh>&lt;&lt; tokenize: define contents &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20240929032636.1"><vh>OLD function: entry &amp; helpers</vh>
<v t="ekr.20240930100625.1"><vh>&lt;&lt; 1: read &gt;&gt;</vh></v>
<v t="ekr.20240930100707.1"><vh>&lt;&lt; 2: make input_list &gt;&gt;</vh></v>
<v t="ekr.20240930100553.1"><vh>&lt;&lt; 3: print stats &gt;&gt;</vh></v>
<v t="ekr.20240929033044.1"><vh>function: add_input_token</vh></v>
<v t="ekr.20240929032710.1"><vh>function: fmt_ms</vh></v>
<v t="ekr.20240929024648.113"><vh>function: make_input_list (works)</vh></v>
<v t="ekr.20240930084648.1"><vh>function: read</vh></v>
</v>
</v>
<v t="ekr.20240927154009.1"><vh>Files</vh>
<v t="ekr.20240105140814.1"><vh>COPY leoTokens.py</vh>
<v t="ekr.20240105140814.2"><vh>&lt;&lt; leoTokens.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20240105140814.3"><vh>&lt;&lt; leoTokens.py: imports &amp; annotations &gt;&gt;</vh></v>
<v t="ekr.20240214065940.1"><vh>top-level functions (leoTokens.py)</vh>
<v t="ekr.20240105140814.41"><vh>function: dump_contents</vh></v>
<v t="ekr.20240105140814.42"><vh>function: dump_lines</vh></v>
<v t="ekr.20240105140814.43"><vh>function: dump_results</vh></v>
<v t="ekr.20240105140814.44"><vh>function: dump_tokens</vh></v>
<v t="ekr.20240105140814.27"><vh>function: input_tokens_to_string</vh></v>
<v t="ekr.20240926050431.1"><vh>function: beautify_file (leoTokens.py) (new)</vh></v>
<v t="ekr.20240105140814.121"><vh>function: main (leoTokens.py)</vh></v>
<v t="ekr.20240105140814.5"><vh>function: orange_command (leoTokens.py)</vh></v>
<v t="ekr.20240105140814.10"><vh>function: scan_args (leoTokens.py)</vh></v>
</v>
<v t="ekr.20240105140814.52"><vh>Classes</vh>
<v t="ekr.20240105140814.51"><vh>class InternalBeautifierError(Exception)</vh></v>
<v t="ekr.20240105140814.53"><vh>class InputToken</vh>
<v t="ekr.20240105140814.54"><vh>itoken.brief_dump</vh></v>
<v t="ekr.20240105140814.55"><vh>itoken.dump</vh></v>
<v t="ekr.20240105140814.56"><vh>itoken.dump_header</vh></v>
<v t="ekr.20240105140814.57"><vh>itoken.error_dump</vh></v>
<v t="ekr.20240105140814.58"><vh>itoken.show_val</vh></v>
</v>
<v t="ekr.20240105143307.1"><vh>class Tokenizer</vh>
<v t="ekr.20240105143307.2"><vh>Tokenizer.add_token</vh></v>
<v t="ekr.20240105143214.2"><vh>Tokenizer.check_results</vh></v>
<v t="ekr.20240105143214.3"><vh>Tokenizer.check_round_trip</vh></v>
<v t="ekr.20240105143214.4"><vh>Tokenizer.create_input_tokens</vh></v>
<v t="ekr.20240105143214.5"><vh>Tokenizer.do_token (the gem)</vh></v>
<v t="ekr.20240105143214.6"><vh>Tokenizer.make_input_tokens (entry)</vh></v>
<v t="ekr.20240105143214.7"><vh>Tokenizer.tokens_to_string</vh></v>
</v>
<v t="ekr.20240105140814.108"><vh>class ParseState</vh></v>
<v t="ekr.20240128114842.1"><vh>class ScanState</vh></v>
<v t="ekr.20240105145241.1"><vh>class TokenBasedOrange</vh>
<v t="ekr.20240119062227.1"><vh>&lt;&lt; TokenBasedOrange: docstring &gt;&gt;</vh></v>
<v t="ekr.20240111035404.1"><vh>&lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;</vh></v>
<v t="ekr.20240116040458.1"><vh>&lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;</vh></v>
<v t="ekr.20240105145241.2"><vh>tbo.ctor</vh></v>
<v t="ekr.20240126012433.1"><vh>tbo: Checking &amp; dumping</vh>
<v t="ekr.20240106220724.1"><vh>tbo.dump_token_range</vh></v>
<v t="ekr.20240112082350.1"><vh>tbo.internal_error_message</vh></v>
<v t="ekr.20240226131015.1"><vh>tbo.user_error_message</vh></v>
<v t="ekr.20240117053310.1"><vh>tbo.oops</vh></v>
</v>
<v t="ekr.20240105145241.4"><vh>tbo: Entries &amp; helpers</vh>
<v t="ekr.20240105145241.5"><vh>tbo.beautify (main token loop)</vh>
<v t="ekr.20240112023403.1"><vh>&lt;&lt; tbo.beautify: init ivars &gt;&gt;</vh></v>
</v>
<v t="ekr.20240105145241.6"><vh>tbo.beautify_file (entry) (stats &amp; diffs)</vh></v>
<v t="ekr.20240105145241.8"><vh>tbo.init_tokens_from_file</vh></v>
<v t="ekr.20240105140814.12"><vh>tbo.regularize_newlines</vh></v>
<v t="ekr.20240105140814.17"><vh>tbo.write_file</vh></v>
<v t="ekr.20200107040729.1"><vh>tbo.show_diffs</vh></v>
</v>
<v t="ekr.20240105145241.9"><vh>tbo: Visitors &amp; generators</vh>
<v t="ekr.20240105145241.10"><vh>tbo.do_comment</vh>
<v t="ekr.20240420034216.1"><vh>&lt;&lt; do_comment: update comment-related state &gt;&gt;</vh></v>
</v>
<v t="ekr.20240111051726.1"><vh>tbo.do_dedent</vh></v>
<v t="ekr.20240105145241.11"><vh>tbo.do_encoding</vh></v>
<v t="ekr.20240105145241.12"><vh>tbo.do_endmarker</vh></v>
<v t="ekr.20240105145241.14"><vh>tbo.do_indent</vh></v>
<v t="ekr.20240105145241.16"><vh>tbo.do_name &amp; generators</vh>
<v t="ekr.20240418050017.1"><vh>tbo.do_name</vh></v>
<v t="ekr.20240105145241.40"><vh>tbo.gen_word</vh></v>
<v t="ekr.20240107141830.1"><vh>tbo.gen_word_op</vh></v>
</v>
<v t="ekr.20240105145241.17"><vh>tbo.do_newline, do_nl &amp; generators</vh>
<v t="ekr.20240418043826.1"><vh>tbo.do_newline</vh></v>
<v t="ekr.20240418043827.1"><vh>tbo.do_nl</vh></v>
</v>
<v t="ekr.20240105145241.18"><vh>tbo.do_number</vh></v>
<v t="ekr.20240105145241.19"><vh>tbo.do_op &amp; generators</vh>
<v t="ekr.20240418045924.1"><vh>tbo.do_op</vh></v>
<v t="ekr.20240105145241.31"><vh>tbo.gen_colon &amp; helper</vh></v>
<v t="ekr.20240109035004.1"><vh>tbo.gen_dot_op &amp; _next</vh>
<v t="ekr.20240105145241.43"><vh>tbo._next</vh></v>
</v>
<v t="ekr.20240105145241.20"><vh>tbo.gen_equal_op</vh></v>
<v t="ekr.20240105145241.35"><vh>tbo.gen_lt</vh></v>
<v t="ekr.20240105145241.37"><vh>tbo.gen_possible_unary_op &amp; helper</vh>
<v t="ekr.20240109082712.1"><vh>tbo.is_unary_op &amp; _prev</vh>
<v t="ekr.20240115233050.1"><vh>tbo._prev</vh></v>
</v>
</v>
<v t="ekr.20240105145241.36"><vh>tbo.gen_rt</vh></v>
<v t="ekr.20240105145241.38"><vh>tbo.gen_star_op</vh></v>
<v t="ekr.20240105145241.39"><vh>tbo.gen_star_star_op</vh></v>
<v t="ekr.20240105145241.3"><vh>tbo.push_state</vh></v>
</v>
<v t="ekr.20240105145241.21"><vh>tbo.do_string</vh></v>
<v t="ekr.20240105145241.22"><vh>tbo.do_verbatim</vh></v>
<v t="ekr.20240105145241.23"><vh>tbo.do_ws</vh></v>
<v t="ekr.20240105145241.27"><vh>tbo.gen_blank</vh></v>
<v t="ekr.20240105145241.26"><vh>tbo.gen_token</vh></v>
</v>
<v t="ekr.20240110205127.1"><vh>tbo: Scanning</vh>
<v t="ekr.20240128114622.1"><vh>tbo.pre_scan &amp; helpers</vh>
<v t="ekr.20240128230812.1"><vh>&lt;&lt; pre-scan 'newline' tokens &gt;&gt;</vh></v>
<v t="ekr.20240128123117.1"><vh>&lt;&lt; pre-scan 'op' tokens &gt;&gt;</vh></v>
<v t="ekr.20240128231119.1"><vh>&lt;&lt; pre-scan 'name' tokens &gt;&gt;</vh></v>
<v t="ekr.20240129041304.1"><vh>tbo.finish_arg</vh></v>
<v t="ekr.20240128233406.1"><vh>tbo.finish_slice</vh></v>
<v t="ekr.20240129040347.1"><vh>tbo.finish_dict</vh></v>
</v>
<v t="ekr.20240129034209.1"><vh>tbo.is_unary_op_with_prev</vh></v>
<v t="ekr.20240129035336.1"><vh>tbo.is_python_keyword</vh></v>
<v t="ekr.20240106170746.1"><vh>tbo.set_context</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20240927151245.1"><vh>@edit Cargo.toml</vh></v>
<v t="ekr.20240927151332.1"><vh>@file src/main.rs</vh></v>
</v>
<v t="ekr.20240927154016.1"><vh>Notes</vh>
<v t="ekr.20240929084852.1"><vh>Ownership and mutation</vh></v>
<v t="ekr.20241001055017.1"><vh>Traits</vh></v>
<v t="ekr.20241001060848.1"><vh>Lifetimes (to do)</vh></v>
<v t="ekr.20240928185643.1"><vh>Stats</vh></v>
</v>
<v t="ekr.20240927154323.1"><vh>** To do</vh></v>
<v t="ekr.20240928161210.1"><vh>@file src/tbo.rs</vh></v>
<v t="ekr.20241001104914.1"><vh>--- classes</vh>
<v t="ekr.20240105145241.1"></v>
<v t="ekr.20240929024648.120"><vh>class InputTok</vh></v>
<v t="ekr.20240929074037.1"><vh>class LeoBeautifier (new)</vh>
<v t="ekr.20240929074037.114"><vh> LB::new</vh></v>
<v t="ekr.20240929074037.3"><vh>LB::add_input_token</vh></v>
<v t="ekr.20240929074037.2"><vh>LB::add_output_string (works)</vh></v>
<v t="ekr.20240929074037.4"><vh>LB::beautify_all_files</vh></v>
<v t="ekr.20240929074037.5"><vh>LB::beautify_one_file</vh></v>
<v t="ekr.20240929074037.7"><vh>LB::do_*</vh>
<v t="ekr.20240929074037.8"><vh>LB:Handlers with values</vh>
<v t="ekr.20240929074037.9"><vh>LB::do_Comment</vh></v>
<v t="ekr.20240929074037.10"><vh>LB::do_Complex</vh></v>
<v t="ekr.20240929074037.11"><vh>LB::do_Float</vh></v>
<v t="ekr.20240929074037.12"><vh>LB::do_Int</vh></v>
<v t="ekr.20240929074037.13"><vh>LB::do_Name</vh></v>
<v t="ekr.20240929074037.14"><vh>LB::do_String</vh></v>
</v>
<v t="ekr.20240929074037.15"><vh>LB:Handlers using lws</vh>
<v t="ekr.20240929074037.16"><vh>LB::do_Dedent</vh></v>
<v t="ekr.20240929074037.17"><vh>LB::do_Indent</vh></v>
<v t="ekr.20240929074037.18"><vh>LB::do_Newline</vh></v>
<v t="ekr.20240929074037.19"><vh>LB::do_NonLogicalNewline</vh></v>
</v>
<v t="ekr.20240929074037.20"><vh>LB:Handlers w/o values</vh>
<v t="ekr.20240929074037.21"><vh>LB::do_Amper</vh></v>
<v t="ekr.20240929074037.22"><vh>LB::do_AmperEqual</vh></v>
<v t="ekr.20240929074037.23"><vh>LB::do_And</vh></v>
<v t="ekr.20240929074037.24"><vh>LB::do_As</vh></v>
<v t="ekr.20240929074037.25"><vh>LB::do_Assert</vh></v>
<v t="ekr.20240929074037.26"><vh>LB::do_Async</vh></v>
<v t="ekr.20240929074037.27"><vh>LB::do_At</vh></v>
<v t="ekr.20240929074037.28"><vh>LB::do_AtEqual</vh></v>
<v t="ekr.20240929074037.29"><vh>LB::do_Await</vh></v>
<v t="ekr.20240929074037.30"><vh>LB::do_Break</vh></v>
<v t="ekr.20240929074037.31"><vh>LB::do_Case</vh></v>
<v t="ekr.20240929074037.32"><vh>LB::do_CircumFlex</vh></v>
<v t="ekr.20240929074037.33"><vh>LB::do_CircumflexEqual</vh></v>
<v t="ekr.20240929074037.34"><vh>LB::do_Class</vh></v>
<v t="ekr.20240929074037.35"><vh>LB::do_Colon</vh></v>
<v t="ekr.20240929074037.36"><vh>LB::do_ColonEqual</vh></v>
<v t="ekr.20240929074037.37"><vh>LB::do_Comma</vh></v>
<v t="ekr.20240929074037.38"><vh>LB::do_Continue</vh></v>
<v t="ekr.20240929074037.39"><vh>LB::do_Def</vh></v>
<v t="ekr.20240929074037.40"><vh>LB::do_Del</vh></v>
<v t="ekr.20240929074037.41"><vh>LB::do_Dot</vh></v>
<v t="ekr.20240929074037.42"><vh>LB::do_DoubleSlash</vh></v>
<v t="ekr.20240929074037.43"><vh>LB::do_DoubleSlashEqual</vh></v>
<v t="ekr.20240929074037.44"><vh>LB::do_DoubleStar</vh></v>
<v t="ekr.20240929074037.45"><vh>LB::do_DoubleStarEqual</vh></v>
<v t="ekr.20240929074037.46"><vh>LB::do_Elif</vh></v>
<v t="ekr.20240929074037.47"><vh>LB::do_Ellipsis</vh></v>
<v t="ekr.20240929074037.48"><vh>LB::do_Else</vh></v>
<v t="ekr.20240929074037.49"><vh>LB::do_EndOfFile</vh></v>
<v t="ekr.20240929074037.50"><vh>LB::do_EqEqual</vh></v>
<v t="ekr.20240929074037.51"><vh>LB::do_Equal</vh></v>
<v t="ekr.20240929074037.52"><vh>LB::do_Except</vh></v>
<v t="ekr.20240929074037.53"><vh>LB::do_False</vh></v>
<v t="ekr.20240929074037.54"><vh>LB::do_Finally</vh></v>
<v t="ekr.20240929074037.55"><vh>LB::do_For</vh></v>
<v t="ekr.20240929074037.56"><vh>LB::do_From</vh></v>
<v t="ekr.20240929074037.57"><vh>LB::do_Global</vh></v>
<v t="ekr.20240929074037.58"><vh>LB::do_Greater</vh></v>
<v t="ekr.20240929074037.59"><vh>LB::do_GreaterEqual</vh></v>
<v t="ekr.20240929074037.60"><vh>LB::do_If</vh></v>
<v t="ekr.20240929074037.61"><vh>LB::do_Import</vh></v>
<v t="ekr.20240929074037.62"><vh>LB::do_In</vh></v>
<v t="ekr.20240929074037.63"><vh>LB::do_Is</vh></v>
<v t="ekr.20240929074037.64"><vh>LB::do_Lambda</vh></v>
<v t="ekr.20240929074037.65"><vh>LB::do_Lbrace</vh></v>
<v t="ekr.20240929074037.66"><vh>LB::do_LeftShift</vh></v>
<v t="ekr.20240929074037.67"><vh>LB::do_LeftShiftEqual</vh></v>
<v t="ekr.20240929074037.68"><vh>LB::do_Less</vh></v>
<v t="ekr.20240929074037.69"><vh>LB::do_LessEqual</vh></v>
<v t="ekr.20240929074037.70"><vh>LB::do_Lpar</vh></v>
<v t="ekr.20240929074037.71"><vh>LB::do_Lsqb</vh></v>
<v t="ekr.20240929074037.72"><vh>LB::do_Match</vh></v>
<v t="ekr.20240929074037.73"><vh>LB::do_Minus</vh></v>
<v t="ekr.20240929074037.74"><vh>LB::do_MinusEqual</vh></v>
<v t="ekr.20240929074037.75"><vh>LB::do_None</vh></v>
<v t="ekr.20240929074037.76"><vh>LB::do_Nonlocal</vh></v>
<v t="ekr.20240929074037.77"><vh>LB::do_Not</vh></v>
<v t="ekr.20240929074037.78"><vh>LB::do_NotEqual</vh></v>
<v t="ekr.20240929074037.79"><vh>LB::do_Or</vh></v>
<v t="ekr.20240929074037.80"><vh>LB::do_Pass</vh></v>
<v t="ekr.20240929074037.81"><vh>LB::do_Percent</vh></v>
<v t="ekr.20240929074037.82"><vh>LB::do_PercentEqual</vh></v>
<v t="ekr.20240929074037.83"><vh>LB::do_Plus</vh></v>
<v t="ekr.20240929074037.84"><vh>LB::do_PlusEqual</vh></v>
<v t="ekr.20240929074037.85"><vh>LB::do_Raise</vh></v>
<v t="ekr.20240929074037.86"><vh>LB::do_Rarrow</vh></v>
<v t="ekr.20240929074037.87"><vh>LB::do_Rbrace</vh></v>
<v t="ekr.20240929074037.88"><vh>LB::do_Return</vh></v>
<v t="ekr.20240929074037.89"><vh>LB::do_RightShift</vh></v>
<v t="ekr.20240929074037.90"><vh>LB::do_RightShiftEqual</vh></v>
<v t="ekr.20240929074037.91"><vh>LB::do_Rpar</vh></v>
<v t="ekr.20240929074037.92"><vh>LB::do_Rsqb</vh></v>
<v t="ekr.20240929074037.93"><vh>LB::do_Semi</vh></v>
<v t="ekr.20240929074037.94"><vh>LB::do_Slash</vh></v>
<v t="ekr.20240929074037.95"><vh>LB::do_SlashEqual</vh></v>
<v t="ekr.20240929074037.96"><vh>LB::do_Star</vh></v>
<v t="ekr.20240929074037.97"><vh>LB::do_StarEqual</vh></v>
<v t="ekr.20240929074037.98"><vh>LB::do_StartExpression</vh></v>
<v t="ekr.20240929074037.99"><vh>LB::do_StartInteractive</vh></v>
<v t="ekr.20240929074037.100"><vh>LB::do_StarModule</vh></v>
<v t="ekr.20240929074037.101"><vh>LB::do_Tilde</vh></v>
<v t="ekr.20240929074037.102"><vh>LB::do_True</vh></v>
<v t="ekr.20240929074037.103"><vh>LB::do_Try</vh></v>
<v t="ekr.20240929074037.104"><vh>LB::do_Type</vh></v>
<v t="ekr.20240929074037.105"><vh>LB::do_Vbar</vh></v>
<v t="ekr.20240929074037.106"><vh>LB::do_VbarEqual</vh></v>
<v t="ekr.20240929074037.107"><vh>LB::do_While</vh></v>
<v t="ekr.20240929074037.108"><vh>LB::do_With</vh></v>
<v t="ekr.20240929074037.109"><vh>LB::do_Yield</vh></v>
</v>
</v>
<v t="ekr.20240929074037.110"><vh>LB::enabled</vh></v>
<v t="ekr.20240929074037.111"><vh>LB::get_args</vh></v>
<v t="ekr.20240929074037.112"><vh>LB::make_input_list</vh></v>
<v t="ekr.20240929074037.113"><vh>LB::beautify (to do)</vh>
<v t="ekr.20241001213329.1"><vh>&lt;&lt; LB::beautify: init ivars &gt;&gt;</vh></v>
</v>
<v t="ekr.20240929074037.115"><vh>LB::show_args</vh></v>
<v t="ekr.20240929074037.116"><vh>LB::show_help</vh></v>
<v t="ekr.20240929074037.117"><vh>LB::show_output_list</vh></v>
</v>
<v t="ekr.20241001215023.1"><vh>class ParseState (to do)</vh></v>
<v t="ekr.20240929074547.1"><vh>class Stats</vh>
<v t="ekr.20241001100954.1"><vh> Stats::new</vh></v>
<v t="ekr.20240929080242.1"><vh>Stats::fmt_ns</vh></v>
<v t="ekr.20240929075236.1"><vh>Stats::report</vh></v>
<v t="ekr.20240929074941.1"><vh>Stats::update_times</vh></v>
</v>
</v>
<v t="ekr.20241001164547.1"><vh>--- recent</vh>
<v t="ekr.20240928185643.1"></v>
<v t="ekr.20241001093308.1"><vh>pub fn entry &amp; helpers</vh></v>
<v t="ekr.20240929074037.112"></v>
</v>
<v t="ekr.20240929074037.5"></v>
<v t="ekr.20241001213229.1"><vh>from tbo.beautify (finish: do not delete)</vh></v>
<v t="ekr.20241002054443.1"><vh>--- Creating output list</vh></v>
<v t="ekr.20240929074037.113"></v>
<v t="ekr.20240929074037.2"></v>
<v t="ekr.20240929074037.9"></v>
</vnodes>
<tnodes>
<t tx="ekr.20200107040729.1">def show_diffs(self, s1: str, s2: str) -&gt; None:  # pragma: no cover
    """Print diffs between strings s1 and s2."""
    filename = self.filename
    lines = list(difflib.unified_diff(
        g.splitLines(s1),
        g.splitLines(s2),
        fromfile=f"Old {filename}",
        tofile=f"New {filename}",
    ))
    print('')
    print(f"Diffs for {filename}")
    for line in lines:
        print(line)
</t>
<t tx="ekr.20240105140814.1"># This file is part of Leo: https://leo-editor.github.io/leo-editor
# Leo's copyright notice is based on the MIT license:
# https://leo-editor.github.io/leo-editor/license.html

# This file may be compiled with mypyc as follows:
# python -m mypyc leo\core\leoTokens.py --strict-optional

&lt;&lt; leoTokens.py: docstring &gt;&gt;
&lt;&lt; leoTokens.py: imports &amp; annotations &gt;&gt;

@others

if __name__ == '__main__' or 'leoTokens' in __name__:
    main()  # pragma: no cover

@language python
@tabwidth -4
@pagewidth 70
</t>
<t tx="ekr.20240105140814.10">def scan_args() -&gt; tuple[Any, dict[str, Any], list[str]]:  # pragma: no cover
    description = textwrap.dedent(
    """Beautify or diff files""")
    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument('PATHS', nargs='*', help='directory or list of files')
    add2 = parser.add_argument

    # Arguments.
    add2('-a', '--all', dest='all', action='store_true',
        help='Beautify all files, even unchanged files')
    add2('-b', '--beautified', dest='beautified', action='store_true',
        help='Report beautified files individually, even if not written')
    add2('-d', '--diff', dest='diff', action='store_true',
        help='show diffs instead of changing files')
    add2('-r', '--report', dest='report', action='store_true',
        help='show summary report')
    add2('-w', '--write', dest='write', action='store_true',
        help='write beautifed files (dry-run mode otherwise)')

    # Create the return values, using EKR's prefs as the defaults.
    parser.set_defaults(
        all=False, beautified=False, diff=False, report=False, write=False,
        tab_width=4,
    )
    args: Any = parser.parse_args()
    files = args.PATHS

    # Create the settings dict, ensuring proper values.
    settings_dict: dict[str, Any] = {
        'all': bool(args.all),
        'beautified': bool(args.beautified),
        'diff': bool(args.diff),
        'report': bool(args.report),
        'write': bool(args.write)
    }
    return args, settings_dict, files
</t>
<t tx="ekr.20240105140814.108">class ParseState:
    """
    A class representing items in the parse state stack.

    The present states:

    'file-start': Ensures the stack stack is never empty.

    'decorator': The last '@' was a decorator.

        do_op():    push_state('decorator')
        do_name():  pops the stack if state.kind == 'decorator'.

    'indent': The indentation level for 'class' and 'def' names.

        do_name():      push_state('indent', self.level)
        do_dendent():   pops the stack once or
                        twice if state.value == self.level.
    """

    __slots__ = ('kind', 'value')

    def __init__(self, kind: str, value: Union[int, str, None]) -&gt; None:
        self.kind = kind
        self.value = value

    def __repr__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover

    def __str__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover
</t>
<t tx="ekr.20240105140814.12">def regularize_newlines(self, s: str) -&gt; str:
    """Regularize newlines within s."""
    return s.replace('\r\n', '\n').replace('\r', '\n')
</t>
<t tx="ekr.20240105140814.121">def main() -&gt; None:  # pragma: no cover
    """Run commands specified by sys.argv."""
    args, settings_dict, arg_files = scan_args()
    cwd = os.getcwd()

    # Calculate requested files.
    requested_files: list[str] = []
    for path in arg_files:
        if path.endswith('.py'):
            requested_files.append(os.path.join(cwd, path))
        else:
            root_dir = os.path.join(cwd, path)
            requested_files.extend(
                glob.glob(f'{root_dir}**{os.sep}*.py', recursive=True)
            )
    if not requested_files:
        # print(f"No files in {arg_files!r}")
        return

    # Calculate the actual list of files.
    modified_files = g.getModifiedFiles(cwd)

    def is_dirty(path: str) -&gt; bool:
        return os.path.abspath(path) in modified_files

    # Compute the files to be checked.
    if args.all:
        # Handle all requested files.
        to_be_checked_files = requested_files
    else:
        # Handle only modified files.
        to_be_checked_files = [z for z in requested_files if is_dirty(z)]

    # Compute the dirty files among the to-be-checked files.
    dirty_files = [z for z in to_be_checked_files if is_dirty(z)]

    # Do the command.
    if to_be_checked_files:
        orange_command(arg_files, requested_files, dirty_files, to_be_checked_files, settings_dict)
</t>
<t tx="ekr.20240105140814.17">def write_file(self, filename: str, s: str) -&gt; None:  # pragma: no cover
    """
    Write the string s to the file whose name is given.

    Handle all exceptions.

    Before calling this function, the caller should ensure
    that the file actually has been changed.
    """
    try:
        s2 = g.toEncodedString(s)  # May raise exception.
        with open(filename, 'wb') as f:
            f.write(s2)
    except Exception as e:  # pragma: no cover
        print(f"Error {e!r}: {filename!r}")
</t>
<t tx="ekr.20240105140814.2">"""
leoTokens.py: A beautifier for Python that uses *only* tokens.

For help: `python -m leo.core.leoTokens --help`

Use Leo https://leo-editor.github.io/leo-editor/ to study this code!

Without Leo, you will see special **sentinel comments** that create
Leo's outline structure. These comments have the form::

    `#@&lt;comment-kind&gt;:&lt;user-id&gt;.&lt;timestamp&gt;.&lt;number&gt;: &lt;outline-level&gt; &lt;headline&gt;`
"""
</t>
<t tx="ekr.20240105140814.27">def input_tokens_to_string(tokens: list[InputToken]) -&gt; str:  # pragma: no cover
    """Return the string represented by the list of tokens."""
    if tokens is None:
        # This indicates an internal error.
        print('')
        print('===== input token list is None ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20240105140814.3">from __future__ import annotations
import argparse
import difflib
import glob
import keyword
import io
import os
import re
import textwrap
import time
import tokenize
from typing import Any, Generator, Optional, Union

# Leo Imports.
from leo.core import leoGlobals as g
assert g

Settings = dict[str, Union[int, bool]]
</t>
<t tx="ekr.20240105140814.41">def dump_contents(contents: str, tag: str = 'Contents') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    for i, z in enumerate(g.splitLines(contents)):
        print(f"{i+1:&lt;3} ", z.rstrip())
    print('')
</t>
<t tx="ekr.20240105140814.42">def dump_lines(tokens: list[InputToken], tag: str = 'lines') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    for z in tokens:
        if z.line.strip():
            print(z.line.rstrip())
        else:
            print(repr(z.line))
    print('')
</t>
<t tx="ekr.20240105140814.43">def dump_results(results: list[str], tag: str = 'Results') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    print(''.join(results))
    print('')
</t>
<t tx="ekr.20240105140814.44">def dump_tokens(tokens: list[InputToken], tag: str = 'Tokens') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    if not tokens:
        return
    print(
        "Note: values shown are repr(value) "
        "*except* for 'string' and 'fstring*' tokens."
    )
    tokens[0].dump_header()
    for z in tokens:
        print(z.dump())
    print('')
</t>
<t tx="ekr.20240105140814.5">def orange_command(
    arg_files: list[str],
    requested_files: list[str],
    dirty_files: list[str],
    to_be_checked_files: list[str],
    settings: Optional[Settings] = None,
) -&gt; None:  # pragma: no cover
    """The outer level of the 'tbo/orange' command."""
    t1 = time.process_time()
    # n_tokens = 0
    n_beautified = 0
    if settings is None:
        settings = {}
    for filename in to_be_checked_files:
        if os.path.exists(filename):
            tbo = TokenBasedOrange(settings)
            beautified = tbo.beautify_file(filename)
            if beautified:
                n_beautified += 1
            # n_tokens += len(tbo.input_tokens)
        else:
            print(f"file not found: {filename}")
    # Report the results.
    t2 = time.process_time()
    if n_beautified or settings.get('report'):
        print(
            f"tbo: {t2-t1:4.2f} sec. "
            f"dirty: {len(dirty_files):&lt;3} "
            f"checked: {len(to_be_checked_files):&lt;3} "
            f"beautified: {n_beautified:&lt;3} in {','.join(arg_files)}"
        )
</t>
<t tx="ekr.20240105140814.51">class InternalBeautifierError(Exception):
    """
    An internal error in the beautifier.

    Errors in the user's source code may raise standard Python errors
    such as IndentationError or SyntaxError.
    """
</t>
<t tx="ekr.20240105140814.52"></t>
<t tx="ekr.20240105140814.53">class InputToken:  # leoTokens.py.
    """A class representing a TBO input token."""

    __slots__ = (
        'context',
        'index',
        'kind',
        'line',
        'line_number',
        'value',
    )

    def __init__(
        self, kind: str, value: str, index: int, line: str, line_number: int,
    ) -&gt; None:
        self.context: Optional[str] = None
        self.index = index
        self.kind = kind
        self.line = line  # The entire line containing the token.
        self.line_number = line_number
        self.value = value

    def __repr__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;5} {self.kind:&gt;8}"
        return f"Token {s}: {self.show_val(20):22}"

    def __str__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;5} {self.kind:&gt;8}"
        return f"Token {s}: {self.show_val(20):22}"

    def to_string(self) -&gt; str:
        """Return the contribution of the token to the source file."""
        return self.value if isinstance(self.value, str) else ''

    @others
</t>
<t tx="ekr.20240105140814.54">def brief_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token."""
    token_s = f"{self.kind:&gt;10} : {self.show_val(10):12}"
    return f"&lt;line: {self.line_number} index: {self.index:3} {token_s}&gt;"

</t>
<t tx="ekr.20240105140814.55">def dump(self) -&gt; str:  # pragma: no cover
    """Dump a token and related links."""
    return (
        f"{self.line_number:4} "
        f"{self.index:&gt;5} {self.kind:&gt;15} "
        f"{self.show_val(100)}"
    )
</t>
<t tx="ekr.20240105140814.56">def dump_header(self) -&gt; None:  # pragma: no cover
    """Print the header for token.dump"""
    print(
        f"\n"
        f"         node    {'':10} token {'':10}   token\n"
        f"line index class {'':10} index {'':10} kind value\n"
        f"==== ===== ===== {'':10} ===== {'':10} ==== =====\n")
</t>
<t tx="ekr.20240105140814.57">def error_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token for error message."""
    return f"index: {self.index:&lt;3} {self.kind:&gt;12} {self.show_val(20):&lt;20}"
</t>
<t tx="ekr.20240105140814.58">def show_val(self, truncate_n: int = 8) -&gt; str:  # pragma: no cover
    """Return the token.value field."""
    if self.kind in ('dedent', 'indent', 'newline', 'ws'):
        # val = str(len(self.value))
        val = repr(self.value)
    elif self.kind == 'string' or self.kind.startswith('fstring'):
        # repr would be confusing.
        val = g.truncate(self.value, truncate_n)
    else:
        val = g.truncate(repr(self.value), truncate_n)
    return val
</t>
<t tx="ekr.20240105143214.2">def check_results(self, contents: str) -&gt; None:

    # Split the results into lines.
    result = ''.join([z.to_string() for z in self.token_list])
    result_lines = g.splitLines(result)
    # Check.
    ok = result == contents and result_lines == self.lines
    assert ok, (
        f"\n"
        f"      result: {result!r}\n"
        f"    contents: {contents!r}\n"
        f"result_lines: {result_lines}\n"
        f"       lines: {self.lines}"
    )
</t>
<t tx="ekr.20240105143214.3">def check_round_trip(self, contents: str, tokens: list[InputToken]) -&gt; bool:
    result = self.tokens_to_string(tokens)
    ok = result == contents
    if not ok:  # pragma: no cover
        print('\nRound-trip check FAILS')
        print('Contents...\n')
        print(contents)
        print('\nResult...\n')
        print(result)
    return ok
</t>
<t tx="ekr.20240105143214.4">def create_input_tokens(
    self,
    contents: str,
    five_tuples: Generator,
) -&gt; list[InputToken]:
    """
    InputTokenizer.create_input_tokens.

    Return list of InputToken's from tokens, a list of 5-tuples.
    """
    # Remember the contents for debugging.
    self.contents = contents

    # Create the physical lines.
    self.lines = contents.splitlines(True)

    # Create the list of character offsets of the start of each physical line.
    last_offset = 0
    for line in self.lines:
        last_offset += len(line)
        self.offsets.append(last_offset)

    # Create self.token_list.
    for five_tuple in five_tuples:
        self.do_token(contents, five_tuple)

    # Print the token list when tracing.
    self.check_results(contents)
    return self.token_list
</t>
<t tx="ekr.20240105143214.5">def do_token(self, contents: str, five_tuple: tuple) -&gt; None:
    """
    Handle the given token, optionally including between-token whitespace.

    https://docs.python.org/3/library/tokenize.html
    https://docs.python.org/3/library/token.html

    five_tuple is a named tuple with these fields:
    - type:     The token type;
    - string:   The token string.
    - start:    (srow: int, scol: int) The row (line_number!) and column
                where the token begins in the source.
    - end:      (erow: int, ecol: int)) The row (line_number!) and column
                where the token ends in the source;
    - line:     The *physical line on which the token was found.
    """
    import token as token_module

    # Unpack..
    tok_type, val, start, end, line = five_tuple
    s_row, s_col = start  # row/col offsets of start of token.
    e_row, e_col = end  # row/col offsets of end of token.
    line_number = s_row
    kind = token_module.tok_name[tok_type].lower()
    # Calculate the token's start/end offsets: character offsets into contents.
    s_offset = self.offsets[max(0, s_row - 1)] + s_col
    e_offset = self.offsets[max(0, e_row - 1)] + e_col
    # tok_s is corresponding string in the line.
    tok_s = contents[s_offset:e_offset]
    # Add any preceding between-token whitespace.
    ws = contents[self.prev_offset:s_offset]
    if ws:
        # Create the 'ws' pseudo-token.
        self.add_token('ws', line, line_number, ws)
    # Always add token, even if it contributes no text!
    self.add_token(kind, line, line_number, tok_s)
    # Update the ending offset.
    self.prev_offset = e_offset
</t>
<t tx="ekr.20240105143214.6">def make_input_tokens(self, contents: str) -&gt; list[InputToken]:
    """
    Return a list  of InputToken objects using tokenize.tokenize.

    Perform consistency checks and handle all exceptions.
    """
    try:
        five_tuples = tokenize.tokenize(
            io.BytesIO(contents.encode('utf-8')).readline)
    except Exception as e:  # pragma: no cover
        print(f"make_input_tokens: exception {e!r}")
        return []
    tokens = self.create_input_tokens(contents, five_tuples)
    if 1:
        # True: 2.9 sec. False: 2.8 sec.
        assert self.check_round_trip(contents, tokens)
    return tokens
</t>
<t tx="ekr.20240105143214.7">def tokens_to_string(self, tokens: list[InputToken]) -&gt; str:
    """Return the string represented by the list of tokens."""
    if tokens is None:  # pragma: no cover
        # This indicates an internal error.
        print('')
        print('===== No tokens ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20240105143307.1">class Tokenizer:
    """
    Use Python's tokenizer module to create InputTokens
    See: https://docs.python.org/3/library/tokenize.html
    """

    __slots__ = (
        'contents',
        'fstring_line',
        'fstring_line_number',
        'fstring_values',
        'lines',
        'offsets',
        'prev_offset',
        'token_index',
        'token_list',
    )

    def __init__(self) -&gt; None:
        self.contents: str = ''
        self.offsets: list[int] = [0]  # Index of start of each line.
        self.prev_offset = -1
        self.token_index = 0
        self.token_list: list[InputToken] = []
        # Describing the scanned f-string...
        self.fstring_line: Optional[str] = None
        self.fstring_line_number: Optional[int] = None
        self.fstring_values: Optional[list[str]] = None

    @others
</t>
<t tx="ekr.20240105143307.2">def add_token(self, kind: str, line: str, line_number: int, value: str,) -&gt; None:
    """
    Add an InputToken to the token list.

    Convert fstrings to simple strings.
    """
    if self.fstring_values is None:
        if kind == 'fstring_start':
            self.fstring_line = line
            self.fstring_line_number = line_number
            self.fstring_values = [value]
            return
    else:
        # Accumulating an f-string.
        self.fstring_values.append(value)
        if kind != 'fstring_end':
            return
        # Create a single 'string' token from the saved values.
        kind = 'string'
        value = ''.join(self.fstring_values)
        # Use the line and line number of the 'string-start' token.
        line = self.fstring_line or ''
        line_number = self.fstring_line_number or 0
        # Clear the saved values.
        self.fstring_line = None
        self.fstring_line_number = None
        self.fstring_values = None

    tok = InputToken(kind, value, self.token_index, line, line_number)
    self.token_index += 1
    self.token_list.append(tok)
</t>
<t tx="ekr.20240105145241.1">class TokenBasedOrange:  # Orange is the new Black.

    &lt;&lt; TokenBasedOrange: docstring &gt;&gt;
    &lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;
    &lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;

    @others
</t>
<t tx="ekr.20240105145241.10">def do_comment(self) -&gt; None:
    """Handle a comment token."""
    val = self.input_token.value
    &lt;&lt; do_comment: update comment-related state &gt;&gt;

    # Generate the comment.
    self.pending_lws = ''
    self.pending_ws = ''
    entire_line = self.input_token.line.lstrip().startswith('#')

    if entire_line:
        # The comment includes all ws.
        # #1496: No further munging needed.
        val = self.input_token.line.rstrip()
        # #3056: Insure one space after '#' in non-sentinel comments.
        #        Do not change bang lines or '##' comments.
        if m := self.comment_pat.match(val):
            i = len(m.group(1))
            val = val[:i] + '# ' + val[i + 1 :]
    else:
        # Exactly two spaces before trailing comments.
        val = '  ' + val.rstrip()
    self.gen_token('comment', val)
</t>
<t tx="ekr.20240105145241.11">def do_encoding(self) -&gt; None:
    """Handle the encoding token."""
</t>
<t tx="ekr.20240105145241.12">def do_endmarker(self) -&gt; None:
    """Handle an endmarker token."""

    # Ensure exactly one newline at the end of file.
    if self.prev_output_kind not in (
        'indent', 'dedent', 'line-indent', 'newline',
    ):
        self.output_list.append('\n')
    self.pending_lws = ''  # Defensive.
    self.pending_ws = ''  # Defensive.
</t>
<t tx="ekr.20240105145241.14">consider_message = 'consider using python/Tools/scripts/reindent.py'

def do_indent(self) -&gt; None:
    """Handle indent token."""

    # Only warn about indentation errors.
    if '\t' in self.input_token.value:  # pragma: no cover
        print(f"Found tab character in {self.filename}")
        print(self.consider_message)
    elif (len(self.input_token.value) % self.tab_width) != 0:  # pragma: no cover
        print(f"Indentation error in {self.filename}")
        print(self.consider_message)

    # Handle the token!
    new_indent = self.input_token.value
    old_indent = self.indent_level * self.tab_width * ' '
    if new_indent &gt; old_indent:
        self.indent_level += 1
    elif new_indent &lt; old_indent:  # pragma: no cover (defensive)
        print(f"\n===== do_indent: can not happen {new_indent!r}, {old_indent!r}")

    self.lws = new_indent
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'indent'
</t>
<t tx="ekr.20240105145241.16"></t>
<t tx="ekr.20240105145241.17"></t>
<t tx="ekr.20240105145241.18">def do_number(self) -&gt; None:
    """Handle a number token."""
    self.gen_blank()
    self.gen_token('number', self.input_token.value)
</t>
<t tx="ekr.20240105145241.19"></t>
<t tx="ekr.20240105145241.2">def __init__(self, settings: Optional[Settings] = None):
    """Ctor for Orange class."""

    # Set default settings.
    if settings is None:
        settings = {}

    # Hard-code 4-space tabs.
    self.tab_width = 4

    # Define tokens even for empty files.
    self.input_token: InputToken = None
    self.input_tokens: list[InputToken] = []
    self.lws: str = ""  # Set only by Indent/Dedent tokens.
    self.pending_lws: str = ""
    self.pending_ws: str = ""

    # Set by gen_token and all do_* methods that bypass gen_token.
    self.prev_output_kind: str = None
    self.prev_output_value: str = None

    # Set ivars from the settings dict *without* using setattr.
    self.all = settings.get('all', False)
    self.beautified = settings.get('beautified', False)
    self.diff = settings.get('diff', False)
    self.report = settings.get('report', False)
    self.write = settings.get('write', False)

    # The list of tokens that tbo._next/_prev skip.
    self.insignificant_tokens = (
        'comment', 'dedent', 'indent', 'newline', 'nl', 'ws',
    )

    # General patterns.
    self.beautify_pat = re.compile(
        r'#\s*pragma:\s*beautify\b|#\s*@@beautify|#\s*@\+node|#\s*@[+-]others|#\s*@[+-]&lt;&lt;')
    self.comment_pat = re.compile(r'^(\s*)#[^@!# \n]')
    self.nobeautify_pat = re.compile(r'\s*#\s*pragma:\s*no\s*beautify\b|#\s*@@nobeautify')

    # Patterns from FastAtRead class, specialized for python delims.
    self.node_pat = re.compile(r'^(\s*)#@\+node:([^:]+): \*(\d+)?(\*?) (.*)$')  # @node
    self.start_doc_pat = re.compile(r'^\s*#@\+(at|doc)?(\s.*?)?$')  # @doc or @
    self.at_others_pat = re.compile(r'^(\s*)#@(\+|-)others\b(.*)$')  # @others

    # Doc parts end with @c or a node sentinel. Specialized for python.
    self.end_doc_pat = re.compile(r"^\s*#@(@(c(ode)?)|([+]node\b.*))$")
</t>
<t tx="ekr.20240105145241.20">def gen_equal_op(self) -&gt; None:

    val = self.input_token.value
    context = self.input_token.context

    if context == 'initializer':
        # Pep 8: Don't use spaces around the = sign when used to indicate
        #        a keyword argument or a default parameter value.
        #        However, when combining an argument annotation with a default value,
        #        *do* use spaces around the = sign.
        self.pending_ws = ''
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.21">def do_string(self) -&gt; None:
    """
    Handle a 'string' token.

    The Tokenizer converts all f-string tokens to a single 'string' token.
    """
    # Careful: continued strings may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    self.gen_token('string', val)
    self.gen_blank()
</t>
<t tx="ekr.20240105145241.22">def do_verbatim(self) -&gt; None:
    """
    Handle one token in verbatim mode.
    End verbatim mode when the appropriate comment is seen.
    """
    kind = self.input_token.kind
    #
    # Careful: tokens may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    if kind == 'comment':
        if self.beautify_pat.match(val):
            self.verbatim = False
        val = val.rstrip()
        self.gen_token('comment', val)
        return
    if kind == 'indent':
        self.indent_level += 1
        self.lws = self.indent_level * self.tab_width * ' '
    if kind == 'dedent':
        self.indent_level -= 1
        self.lws = self.indent_level * self.tab_width * ' '
    self.gen_token('verbatim', val)
</t>
<t tx="ekr.20240105145241.23">def do_ws(self) -&gt; None:
    """
    Handle the "ws" pseudo-token.  See Tokenizer.itok.do_token (the gem).

    Put the whitespace only if if ends with backslash-newline.
    """
    val = self.input_token.value
    last_token = self.input_tokens[self.index - 1]

    if last_token.kind in ('nl', 'newline'):
        self.pending_lws = val
        self.pending_ws = ''
    elif '\\\n' in val:
        self.pending_lws = ''
        self.pending_ws = val
    else:
        self.pending_ws = val
</t>
<t tx="ekr.20240105145241.26">def gen_token(self, kind: str, value: Any) -&gt; None:
    """Add an output token to the code list."""

    if self.pending_lws:
        self.output_list.append(self.pending_lws)
    elif self.pending_ws:
        self.output_list.append(self.pending_ws)

    self.output_list.append(value)
    self.pending_lws = ''
    self.pending_ws = ''
    self.prev_output_value = value
    self.prev_output_kind = kind
</t>
<t tx="ekr.20240105145241.27">def gen_blank(self) -&gt; None:
    """
    Queue a *request* a blank.
    Change *neither* prev_output_kind *nor* pending_lws.
    """

    prev_kind = self.prev_output_kind
    if prev_kind == 'op-no-blanks':
        # A demand that no blank follows this op.
        self.pending_ws = ''
    elif prev_kind == 'hard-blank':
        # Eat any further blanks.
        self.pending_ws = ''
    elif prev_kind in (
        'dedent',
        'file-start',
        'indent',
        'line-indent',
        'newline',
    ):
        # Suppress the blank, but do *not* change the pending ws.
        pass
    elif self.pending_ws:
        # Use the existing pending ws.
        pass
    else:
        self.pending_ws = ' '
</t>
<t tx="ekr.20240105145241.3">def push_state(self, kind: str, value: Union[int, str, None] = None) -&gt; None:
    """Append a state to the state stack."""
    state = ParseState(kind, value)
    self.state_stack.append(state)
</t>
<t tx="ekr.20240105145241.31">def gen_colon(self) -&gt; None:
    """Handle a colon."""
    val = self.input_token.value
    context = self.input_token.context

    self.pending_ws = ''
    if context == 'complex-slice':
        if self.prev_output_value not in '[:':
            self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
    elif context == 'simple-slice':
        self.gen_token('op-no-blanks', val)
    elif context == 'dict':
        self.gen_token('op', val)
        self.gen_blank()
    else:
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.35">def gen_lt(self) -&gt; None:
    """Generate code for a left paren or curly/square bracket."""
    val = self.input_token.value
    assert val in '([{', repr(val)

    # Update state vars.
    if val == '(':
        self.paren_level += 1
    elif val == '[':
        self.square_brackets_stack.append(False)
    else:
        self.curly_brackets_level += 1

    # Generate or suppress the leading blank.
    # Update self.in_arg_list if necessary.
    if self.input_token.context == 'import':
        self.gen_blank()
    elif self.prev_output_kind in ('op', 'word-op'):
        self.gen_blank()
    elif self.prev_output_kind == 'word':
        # Only suppress blanks before '(' or '[' for non-keywords.
        if val == '{' or self.prev_output_value in (
            'if', 'else', 'elif', 'return', 'for', 'while',
        ):
            self.gen_blank()
        elif val == '(':
            self.in_arg_list += 1
            self.pending_ws = ''
        else:
            self.pending_ws = ''
    elif self.prev_output_kind != 'line-indent':
        self.pending_ws = ''

    # Output the token!
    self.gen_token('op-no-blanks', val)
</t>
<t tx="ekr.20240105145241.36">def gen_rt(self) -&gt; None:
    """Generate code for a right paren or curly/square bracket."""
    val = self.input_token.value
    assert val in ')]}', repr(val)

    # Update state vars.
    if val == ')':
        self.paren_level -= 1
        self.in_arg_list = max(0, self.in_arg_list - 1)
    elif val == ']':
        self.square_brackets_stack.pop()
    else:
        self.curly_brackets_level -= 1

    if self.prev_output_kind != 'line-indent':
        self.pending_ws = ''
    self.gen_token('rt', val)
</t>
<t tx="ekr.20240105145241.37">def gen_possible_unary_op(self) -&gt; None:
    """Add a unary or binary op to the token list."""
    val = self.input_token.value
    if self.is_unary_op(self.index, val):
        prev = self.input_token
        if prev.kind == 'lt':
            self.gen_token('op-no-blanks', val)
        else:
            self.gen_blank()
            self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()

</t>
<t tx="ekr.20240105145241.38">def gen_star_op(self) -&gt; None:
    """Put a '*' op, with special cases for *args."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.39">def gen_star_star_op(self) -&gt; None:
    """Put a ** operator, with a special case for **kwargs."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.4"></t>
<t tx="ekr.20240105145241.40">def gen_word(self, s: str) -&gt; None:
    """Add a word request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word', s)
    self.gen_blank()
</t>
<t tx="ekr.20240105145241.43">def _next(self, i: int) -&gt; Optional[int]:
    """
    Return the next *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.

    The **Global Token Ratio** is tbo.n_scanned_tokens / len(tbo.tokens),
    where tbo.n_scanned_tokens is the total number of calls calls to
    tbo.next or tbo.prev.

    For Leo's sources, this ratio ranges between 0.48 and 1.51!

    The orange_command function warns if this ratio is greater than 2.5.
    Previous versions of this code suffered much higher ratios.
    """
    i += 1
    while i &lt; len(self.input_tokens):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            # g.trace(f"token: {token!r}")
            return i
        i += 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20240105145241.5">def no_visitor(self) -&gt; None:  # pragma: no cover
    self.oops(f"Unknown kind: {self.input_token.kind!r}")

def beautify(self,
    contents: str, filename: str, input_tokens: list[InputToken],
) -&gt; str:
    """
    The main line. Create output tokens and return the result as a string.

    beautify_file and beautify_file_def call this method.
    """
    &lt;&lt; tbo.beautify: init ivars &gt;&gt;

    try:
        # Pre-scan the token list, setting context.s
        self.pre_scan()

        # Init ivars first.
        self.input_token = None
        self.pending_lws = ''
        self.pending_ws = ''
        self.prev_output_kind = None
        self.prev_output_value = None

        # Init state.
        self.gen_token('file-start', '')
        self.push_state('file-start')

        # The main loop:
        prev_line_number: int = 0
        for self.index, self.input_token in enumerate(input_tokens):
            # Set global for visitors.
            if prev_line_number != self.input_token.line_number:
                prev_line_number = self.input_token.line_number
            # Call the proper visitor.
            if self.verbatim:
                self.do_verbatim()
            else:
                func = getattr(self, f"do_{self.input_token.kind}", self.no_visitor)
                func()

        # Return the result.
        result = ''.join(self.output_list)
        return result

    # Make no change if there is any error.
    except InternalBeautifierError as e:  # pragma: no cover
        # oops calls self.internal_error_message to creates e.
        print(repr(e))
    except AssertionError as e:  # pragma: no cover
        print(self.internal_error_message(repr(e)))
    return contents
</t>
<t tx="ekr.20240105145241.6">def beautify_file(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    TokenBasedOrange: Beautify the the given external file.

    Return True if the file was beautified.
    """
    if 0:
        print(
            f"all: {int(self.all)} "
            f"beautified: {int(self.beautified)} "
            f"diff: {int(self.diff)} "
            f"report: {int(self.report)} "
            f"write: {int(self.write)} "
            f"{g.shortFileName(filename)}"
        )
    self.filename = filename
    contents, tokens = self.init_tokens_from_file(filename)
    if not (contents and tokens):
        return False  # Not an error.
    if not isinstance(tokens[0], InputToken):
        self.oops(f"Not an InputToken: {tokens[0]!r}")

    # Beautify the contents, returning the original contents on any error.
    results = self.beautify(contents, filename, tokens)

    # Ignore changes only to newlines.
    if self.regularize_newlines(contents) == self.regularize_newlines(results):
        return False

    # Print reports.
    if self.beautified:  # --beautified.
        print(f"tbo: beautified: {g.shortFileName(filename)}")
    if self.diff:  # --diff.
        print(f"Diffs: {filename}")
        self.show_diffs(contents, results)

    # Write the (changed) file .
    if self.write:  # --write.
        self.write_file(filename, results)
    return True
</t>
<t tx="ekr.20240105145241.8">def init_tokens_from_file(self, filename: str) -&gt; tuple[
    str, list[InputToken]
]:  # pragma: no cover
    """
    Create the list of tokens for the given file.
    Return (contents, encoding, tokens).
    """
    self.indent_level = 0
    self.filename = filename
    contents = g.readFile(filename)
    if not contents:
        self.input_tokens = []
        return '', []
    self.input_tokens = input_tokens = Tokenizer().make_input_tokens(contents)
    return contents, input_tokens
</t>
<t tx="ekr.20240105145241.9"># Visitors (tbo.do_* methods) handle input tokens.
# Generators (tbo.gen_* methods) create zero or more output tokens.
</t>
<t tx="ekr.20240106170746.1">def set_context(self, i: int, context: str) -&gt; None:
    """
    Set self.input_tokens[i].context, but only if it does not already exist!

    See the docstring for pre_scan for details.
    """

    trace = False  # Do not delete the trace below.

    valid_contexts = (
        'annotation', 'arg', 'complex-slice', 'simple-slice',
        'dict', 'import', 'initializer',
    )
    if context not in valid_contexts:
        self.oops(f"Unexpected context! {context!r}")  # pragma: no cover

    token = self.input_tokens[i]

    if trace:  # pragma: no cover
        token_s = f"&lt;{token.kind}: {token.show_val(12)}&gt;"
        ignore_s = 'Ignore' if token.context else ' ' * 6
        print(f"{i:3} {ignore_s} token: {token_s} context: {context}")

    if not token.context:
        token.context = context
</t>
<t tx="ekr.20240106220724.1">def dump_token_range(self, i1: int, i2: int, tag: Optional[str] = None) -&gt; None:  # pragma: no cover
    """Dump the given range of input tokens."""
    if tag:
        print(tag)
    for token in self.input_tokens[i1 : i2 + 1]:
        print(token.dump())
</t>
<t tx="ekr.20240107141830.1">def gen_word_op(self, s: str) -&gt; None:
    """Add a word-op request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word-op', s)
    self.gen_blank()
</t>
<t tx="ekr.20240109035004.1">def gen_dot_op(self) -&gt; None:
    """Handle the '.' input token."""
    context = self.input_token.context

    # Get the previous significant **input** token.
    # This is the only call to next(i) anywhere!
    next_i = self._next(self.index)
    next = 'None' if next_i is None else self.input_tokens[next_i]
    import_is_next = next and next.kind == 'name' and next.value == 'import'

    if context == 'import':
        if (
            self.prev_output_kind == 'word'
            and self.prev_output_value in ('from', 'import')
        ):
            self.gen_blank()
            op = 'op' if import_is_next else 'op-no-blanks'
            self.gen_token(op, '.')
        elif import_is_next:
            self.gen_token('op', '.')
            self.gen_blank()
        else:
            self.pending_ws = ''
            self.gen_token('op-no-blanks', '.')
    else:
        self.pending_ws = ''
        self.gen_token('op-no-blanks', '.')
</t>
<t tx="ekr.20240109082712.1">def is_unary_op(self, i: int, val: str) -&gt; bool:

    if val == '~':
        return True
    if val not in '+-':  # pragma: no cover
        return False

    # Get the previous significant **input** token.
    # This is the only call to _prev(i) anywhere!
    prev_i = self._prev(i)
    prev_token = None if prev_i is None else self.input_tokens[prev_i]
    kind = prev_token.kind if prev_token else ''
    value = prev_token.value if prev_token else ''

    if kind in ('number', 'string'):
        return_val = False
    elif kind == 'op' and value in ')]':
        return_val = False
    elif kind == 'op' and value in '{([:':
        return_val = True
    elif kind != 'name':
        return_val = True
    else:
        # The hard case: prev_token is a 'name' token.
        # Any Python keyword indicates a unary operator.
        return_val = keyword.iskeyword(value) or keyword.issoftkeyword(value)
    return return_val
</t>
<t tx="ekr.20240110205127.1"># The parser calls scanner methods to move through the list of input tokens.
</t>
<t tx="ekr.20240111035404.1">__slots__ = [
    # Command-line arguments.
    'all', 'beautified', 'diff', 'report', 'write',

    # Global data.
    'contents', 'filename', 'input_tokens', 'output_list', 'tab_width',
    'insignificant_tokens',  # New.

    # Token-related data for visitors.
    'index', 'input_token', 'line_number',
    'pending_lws', 'pending_ws', 'prev_output_kind', 'prev_output_value',  # New.

    # Parsing state for visitors.
    'decorator_seen', 'in_arg_list', 'in_doc_part', 'state_stack', 'verbatim',

    # Whitespace state. Don't even *think* about changing these!
    'curly_brackets_level', 'indent_level', 'lws', 'paren_level', 'square_brackets_stack',

    # Regular expressions.
    'at_others_pat', 'beautify_pat', 'comment_pat', 'end_doc_pat',
    'nobeautify_pat', 'node_pat', 'start_doc_pat',
]
</t>
<t tx="ekr.20240111051726.1">def do_dedent(self) -&gt; None:
    """Handle dedent token."""
    # Note: other methods use self.indent_level.
    self.indent_level -= 1
    self.lws = self.indent_level * self.tab_width * ' '
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'dedent'
</t>
<t tx="ekr.20240112023403.1"># Debugging vars...
self.contents = contents
self.filename = filename
self.line_number: Optional[int] = None

# The input and output lists...
self.output_list: list[str] = []
self.input_tokens = input_tokens  # The list of input tokens.

# State vars for whitespace.
self.curly_brackets_level = 0  # Number of unmatched '{' tokens.
self.paren_level = 0  # Number of unmatched '(' tokens.
self.square_brackets_stack: list[bool] = []  # A stack of bools, for self.gen_word().
self.indent_level = 0  # Set only by do_indent and do_dedent.

# Parse state.
self.decorator_seen = False  # Set by do_name for do_op.
self.in_arg_list = 0  # &gt; 0 if in an arg list of a def.
self.in_doc_part = False
self.state_stack: list[ParseState] = []  # Stack of ParseState objects.

# Leo-related state.
self.verbatim = False  # True: don't beautify.

# Ivars describing the present input token...
self.index = 0  # The index within the tokens array of the token being scanned.
self.lws = ''  # Leading whitespace. Required!
</t>
<t tx="ekr.20240112082350.1">def internal_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about an error in the beautifier itself."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        # '\n\n'
        'Error in token-based beautifier!\n'
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
        "Please report this message to Leo's developers"
    )
</t>
<t tx="ekr.20240115233050.1">def _prev(self, i: int) -&gt; Optional[int]:
    """
    Return the previous *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.
    """
    i -= 1
    while i &gt;= 0:
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            return i
        i -= 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20240116040458.1">insignificant_kinds = (
    'comment', 'dedent', 'encoding', 'endmarker', 'indent', 'newline', 'nl', 'ws',
)

# 'name' tokens that may appear in expressions.
operator_keywords = (
    'await',  # Debatable.
    'and', 'in', 'not', 'not in', 'or',  # Operators.
    'True', 'False', 'None',  # Values.
)
</t>
<t tx="ekr.20240117053310.1">def oops(self, message: str) -&gt; None:  # pragma: no cover
    """Raise InternalBeautifierError."""
    raise InternalBeautifierError(self.internal_error_message(message))
</t>
<t tx="ekr.20240119062227.1">@language rest
@wrap

"""
Leo's token-based beautifier, three times faster than the beautifier in leoAst.py.

**Design**

The *pre_scan* method is the heart of the algorithm. It sets context
for the `:`, `=`, `**` and `.` tokens *without* using the parse tree.
*pre_scan* calls three *finishers*.

Each finisher uses a list of *relevant earlier tokens* to set the
context for one kind of (input) token. Finishers look behind (in the
stream of input tokens) with essentially no cost.

After the pre-scan, *tbo.beautify* (the main loop) calls *visitors*
for each separate type of *input* token.

Visitors call *code generators* to generate strings in the output
list, using *lazy evaluation* to generate whitespace.
"""
</t>
<t tx="ekr.20240126012433.1"></t>
<t tx="ekr.20240128114622.1">def pre_scan(self) -&gt; None:
    """
    Scan the entire file in one iterative pass, adding context to a few
    kinds of tokens as follows:

    Token   Possible Contexts (or None)
    =====   ===========================
    ':'     'annotation', 'dict', 'complex-slice', 'simple-slice'
    '='     'annotation', 'initializer'
    '*'     'arg'
    '**'    'arg'
    '.'     'import'
    """

    # The main loop.
    in_import = False
    scan_stack: list[ScanState] = []
    prev_token: Optional[InputToken] = None
    for i, token in enumerate(self.input_tokens):
        kind, value = token.kind, token.value
        if kind in 'newline':
            &lt;&lt; pre-scan 'newline' tokens &gt;&gt;
        elif kind == 'op':
            &lt;&lt; pre-scan 'op' tokens &gt;&gt;
        elif kind == 'name':
            &lt;&lt; pre-scan 'name' tokens &gt;&gt;
        # Remember the previous significant token.
        if kind not in self.insignificant_kinds:
            prev_token = token
    # Sanity check.
    if scan_stack:  # pragma: no cover
        print('pre_scan: non-empty scan_stack')
        print(scan_stack)
</t>
<t tx="ekr.20240128114842.1">class ScanState:  # leoTokens.py.
    """
    A class representing tbo.pre_scan's scanning state.

    Valid (kind, value) pairs:

       kind  Value
       ====  =====
      'args' None
      'from' None
    'import' None
     'slice' list of colon indices
      'dict' list of colon indices

    """

    __slots__ = ('kind', 'token', 'value')

    def __init__(self, kind: str, token: InputToken) -&gt; None:
        self.kind = kind
        self.token = token
        self.value: list[int] = []  # Not always used.

    def __repr__(self) -&gt; str:  # pragma: no cover
        return f"ScanState: i: {self.token.index:&lt;4} kind: {self.kind} value: {self.value}"

    def __str__(self) -&gt; str:  # pragma: no cover
        return f"ScanState: i: {self.token.index:&lt;4} kind: {self.kind} value: {self.value}"
</t>
<t tx="ekr.20240128123117.1">top_state: Optional[ScanState] = scan_stack[-1] if scan_stack else None

# Handle '[' and ']'.
if value == '[':
    scan_stack.append(ScanState('slice', token))
elif value == ']':
    assert top_state and top_state.kind == 'slice'
    self.finish_slice(i, top_state)
    scan_stack.pop()

# Handle '{' and '}'.
if value == '{':
    scan_stack.append(ScanState('dict', token))
elif value == '}':
    assert top_state and top_state.kind == 'dict'
    self.finish_dict(i, top_state)
    scan_stack.pop()

# Handle '(' and ')'
elif value == '(':
    if self.is_python_keyword(prev_token) or prev_token and prev_token.kind != 'name':
        state_kind = '('
    else:
        state_kind = 'arg'
    scan_stack.append(ScanState(state_kind, token))
elif value == ')':
    assert top_state and top_state.kind in ('(', 'arg'), repr(top_state)
    if top_state.kind == 'arg':
        self.finish_arg(i, top_state)
    scan_stack.pop()

# Handle interior tokens in 'arg' and 'slice' states.
if top_state:
    if top_state.kind in ('dict', 'slice') and value == ':':
        top_state.value.append(i)
    if top_state.kind == 'arg' and value in '**=:,':
        top_state.value.append(i)

# Handle '.' and '(' tokens inside 'import' and 'from' statements.
if in_import and value in '(.':
    self.set_context(i, 'import')
</t>
<t tx="ekr.20240128230812.1"># 'import' and 'from x import' statements may span lines.
# 'ws' tokens represent continued lines like this:   ws: ' \\\n    '
if in_import and not scan_stack:
    in_import = False
</t>
<t tx="ekr.20240128231119.1">prev_is_yield = prev_token and prev_token.kind == 'name' and prev_token.value == 'yield'
if value in ('from', 'import') and not prev_is_yield:
    # 'import' and 'from x import' statements should be at the outer level.
    assert not scan_stack, scan_stack
    in_import = True
</t>
<t tx="ekr.20240128233406.1">def finish_slice(self, end: int, state: ScanState) -&gt; None:
    """Set context for all ':' when scanning from '[' to ']'."""

    # Sanity checks.
    assert state.kind == 'slice', repr(state)
    token = state.token
    assert token.value == '[', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Do nothing if there are no ':' tokens in the slice.
    if not colons:
        return

    # Compute final context by scanning the tokens.
    final_context = 'simple-slice'
    inter_colon_tokens = 0
    prev = token
    for i in range(i1 + 1, end - 1):
        token = self.input_tokens[i]
        kind, value = token.kind, token.value
        if kind not in self.insignificant_kinds:
            if kind == 'op':
                if value == '.':
                    # Ignore '.' tokens and any preceding 'name' token.
                    if prev and prev.kind == 'name':  # pragma: no cover
                        inter_colon_tokens -= 1
                elif value == ':':
                    inter_colon_tokens = 0
                elif value in '-+':
                    # Ignore unary '-' or '+' tokens.
                    if not self.is_unary_op_with_prev(prev, token):
                        inter_colon_tokens += 1
                        if inter_colon_tokens &gt; 1:
                            final_context = 'complex-slice'
                            break
                elif value == '~':
                    # '~' is always a unary op.
                    pass
                else:
                    # All other ops contribute.
                    inter_colon_tokens += 1
                    if inter_colon_tokens &gt; 1:
                        final_context = 'complex-slice'
                        break
            else:
                inter_colon_tokens += 1
                if inter_colon_tokens &gt; 1:
                    final_context = 'complex-slice'
                    break
            prev = token

    # Set the context of all outer-level ':' tokens.
    for i in colons:
        self.set_context(i, final_context)
</t>
<t tx="ekr.20240129034209.1">def is_unary_op_with_prev(self, prev: Optional[InputToken], token: InputToken) -&gt; bool:
    """
    Return True if token is a unary op in the context of prev, the previous
    significant token.
    """
    if token.value == '~':  # pragma: no cover
        return True
    if prev is None:
        return True  # pragma: no cover
    assert token.value in '**-+', repr(token.value)
    if prev.kind in ('number', 'string'):
        return_val = False
    elif prev.kind == 'op' and prev.value in ')]':
         # An unnecessary test?
        return_val = False  # pragma: no cover
    elif prev.kind == 'op' and prev.value in '{([:,':
        return_val = True
    elif prev.kind != 'name':
        # An unnecessary test?
        return_val = True  # pragma: no cover
    else:
        # prev is a'name' token.
        return self.is_python_keyword(token)
    return return_val
</t>
<t tx="ekr.20240129035336.1">def is_python_keyword(self, token: Optional[InputToken]) -&gt; bool:
    """Return True if token is a 'name' token referring to a Python keyword."""
    if not token or token.kind != 'name':
        return False
    return keyword.iskeyword(token.value) or keyword.issoftkeyword(token.value)
</t>
<t tx="ekr.20240129040347.1">def finish_dict(self, end: int, state: Optional[ScanState]) -&gt; None:
    """
    Set context for all ':' when scanning from '{' to '}'

    Strictly speaking, setting this context is unnecessary because
    tbo.gen_colon generates the same code regardless of this context.

    In other words, this method can be a do-nothing!
    """

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'dict', repr(state)
    token = state.token
    assert token.value == '{', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Set the context for all ':' tokens.
    for i in colons:
        self.set_context(i, 'dict')
</t>
<t tx="ekr.20240129041304.1">def finish_arg(self, end: int, state: Optional[ScanState]) -&gt; None:
    """Set context for all ':' when scanning from '(' to ')'."""

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'arg', repr(state)
    token = state.token
    assert token.value == '(', repr(token)
    values = state.value
    assert isinstance(values, list), repr(values)
    i1 = token.index
    assert i1 &lt; end, (i1, end)
    if not values:
        return

    # Compute the context for each *separate* '=' token.
    equal_context = 'initializer'
    for i in values:
        token = self.input_tokens[i]
        assert token.kind == 'op', repr(token)
        if token.value == ',':
            equal_context = 'initializer'
        elif token.value == ':':
            equal_context = 'annotation'
        elif token.value == '=':
            self.set_context(i, equal_context)
            equal_context = 'initializer'

    # Set the context of all outer-level ':', '*', and '**' tokens.
    prev: Optional[InputToken] = None
    for i in range(i1, end):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_kinds:
            if token.kind == 'op':
                if token.value in ('*', '**'):
                    if self.is_unary_op_with_prev(prev, token):
                        self.set_context(i, 'arg')
                elif token.value == '=':
                    # The code above has set the context.
                    assert token.context in ('initializer', 'annotation'), (i, repr(token.context))
                elif token.value == ':':
                    self.set_context(i, 'annotation')
            prev = token
</t>
<t tx="ekr.20240214065940.1"></t>
<t tx="ekr.20240226131015.1">def user_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about a user error."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
    )
</t>
<t tx="ekr.20240418043826.1">def do_newline(self) -&gt; None:
    """
    do_newline: Handle a regular newline.

    From https://docs.python.org/3/library/token.html

    NEWLINE tokens end *logical* lines of Python code.
    """

    self.output_list.append('\n')
    self.pending_lws = ''  # Set only by 'dedent', 'indent' or 'ws' tokens.
    self.pending_ws = ''
    self.prev_output_kind = 'newline'
    self.prev_output_value = '\n'
</t>
<t tx="ekr.20240418043827.1">def do_nl(self) -&gt; None:
    """
    do_nl: Handle a continuation line.

    From https://docs.python.org/3/library/token.html

    NL tokens end *physical* lines. They appear when when a logical line of
    code spans multiple physical lines.
    """
    return self.do_newline()
</t>
<t tx="ekr.20240418045924.1">def do_op(self) -&gt; None:
    """Handle an op token."""
    val = self.input_token.value

    if val == '.':
        self.gen_dot_op()
    elif val == '@':
        self.gen_token('op-no-blanks', val)
        self.push_state('decorator')
    elif val == ':':
        # Treat slices differently.
        self.gen_colon()
    elif val in ',;':
        # Pep 8: Avoid extraneous whitespace immediately before
        # comma, semicolon, or colon.
        self.pending_ws = ''
        self.gen_token('op', val)
        self.gen_blank()
    elif val in '([{':
        # Pep 8: Avoid extraneous whitespace immediately inside
        # parentheses, brackets or braces.
        self.gen_lt()
    elif val in ')]}':
        # Ditto.
        self.gen_rt()
    elif val == '=':
        self.gen_equal_op()
    elif val in '~+-':
        self.gen_possible_unary_op()
    elif val == '*':
        self.gen_star_op()
    elif val == '**':
        self.gen_star_star_op()
    else:
        # Pep 8: always surround binary operators with a single space.
        # '==','+=','-=','*=','**=','/=','//=','%=','!=','&lt;=','&gt;=','&lt;','&gt;',
        # '^','~','*','**','&amp;','|','/','//',
        # Pep 8: If operators with different priorities are used, consider
        # adding whitespace around the operators with the lowest priorities.
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240418050017.1">def do_name(self) -&gt; None:
    """Handle a name token."""
    name = self.input_token.value
    if name in self.operator_keywords:
        self.gen_word_op(name)
    else:
        self.gen_word(name)
</t>
<t tx="ekr.20240420034216.1"># Leo-specific code...
if self.node_pat.match(val):
    # Clear per-node state.
    self.in_doc_part = False
    self.verbatim = False
    self.decorator_seen = False
    # Do *not* clear other state, which may persist across @others.
        # self.curly_brackets_level = 0
        # self.in_arg_list = 0
        # self.indent_level = 0
        # self.lws = ''
        # self.paren_level = 0
        # self.square_brackets_stack = []
        # self.state_stack = []
else:
    # Keep track of verbatim mode.
    if self.beautify_pat.match(val):
        self.verbatim = False
    elif self.nobeautify_pat.match(val):
        self.verbatim = True
    # Keep trace of @doc parts, to honor the convention for splitting lines.
    if self.start_doc_pat.match(val):
        self.in_doc_part = True
    if self.end_doc_pat.match(val):
        self.in_doc_part = False
</t>
<t tx="ekr.20240926050431.1">def beautify_file(filename: str) -&gt; bool:
    """
    Beautify the given file, writing it if has changed.
    """
    settings: dict[str, Any] = {
        'all': False,  # Don't beautify all files.
        'beautified': True,  # Report changed files.
        'diff': False,  # Don't show diffs.
        'report': True,  # Report changed files.
        'write': True,  # Write changed files.
    }
    tbo = TokenBasedOrange(settings)
    return tbo.beautify_file(filename)
</t>
<t tx="ekr.20240927151701.1"></t>
<t tx="ekr.20240927151701.100"></t>
<t tx="ekr.20240927151701.101"></t>
<t tx="ekr.20240927151701.102">vertical (v) or horizontal (h)

myLeoSettings.leo: vertical</t>
<t tx="ekr.20240927151701.103"></t>
<t tx="ekr.20240927151701.104"></t>
<t tx="ekr.20240927151701.105">@language rest
@wrap

See #3456.

</t>
<t tx="ekr.20240927151701.106"></t>
<t tx="ekr.20240927151701.107"></t>
<t tx="ekr.20240927151701.108"># leonine</t>
<t tx="ekr.20240927151701.109"></t>
<t tx="ekr.20240927151701.110"></t>
<t tx="ekr.20240927151701.111"></t>
<t tx="ekr.20240927151701.112"></t>
<t tx="ekr.20240927151701.113">True: (Recommended) Make a "Recovered Nodes" node whenever
Leo reads a file that has been changed outside of Leo.
</t>
<t tx="ekr.20240927151701.114"></t>
<t tx="ekr.20240927151701.115"></t>
<t tx="ekr.20240927151701.116"></t>
<t tx="ekr.20240927151701.117"></t>
<t tx="ekr.20240927151701.118"></t>
<t tx="ekr.20240927151701.119"></t>
<t tx="ekr.20240927151701.120"></t>
<t tx="ekr.20240927151701.121"></t>
<t tx="ekr.20240927151701.122"></t>
<t tx="ekr.20240927151701.123"></t>
<t tx="ekr.20240927151701.124"></t>
<t tx="ekr.20240927151701.126">Set to True to enable node appearance modifications
See tree-declutter-patterns
</t>
<t tx="ekr.20240927151701.144"></t>
<t tx="ekr.20240927151701.145">Only supported with the mod_tempfname.py plugin.

True: The plugin will store temporary files utilizing cleaner
file names (no unique number is appended to the node's headline text).
Unique temporary directory paths are used to insure unique files are
created by creating temporary directories reflecting each node's ancestor
nodes in the Leo outline. Note: Do not have multiple sibling nodes (nodes
having the same parent node) in Leo with the same headline text. There will
be a conflict if both are opened in an external editor at the same time.

False: The plugin will store temporary files with an appended
unique number to insure unique temporary filenames.
</t>
<t tx="ekr.20240927151701.146">True: check all @&lt;file&gt; nodes in the outline for changes in corresponding external files.</t>
<t tx="ekr.20240927151701.147"></t>
<t tx="ekr.20240927151701.148"></t>
<t tx="ekr.20240927151701.149"></t>
<t tx="ekr.20240927151701.150"></t>
<t tx="ekr.20240927151701.151"></t>
<t tx="ekr.20240927151701.152">It is *strange* to set this to True!</t>
<t tx="ekr.20240927151701.153">@language rest

To test #2041 &amp; #2094

The @bool use-find-dialog and @bool minibuffer-find-mode settings comprise
a tri-state setting, as shown in this table:
    
minibuffer-find-mode    use-find-dialog     mode: Ctrl-F puts focus in
--------------------    ---------------     --------------------------
    True                    Ignored         minibuffer
    False                   True            dialog
    False                   False           Find tab in the log pane

*All modes*

- Start the search with Ctrl-F (start-search).
- Enter the find pattern.
- (Optional) Use &lt;Tab&gt; to enter the search pattern.
- Use &lt;Enter&gt; to start the search.

*dialog and find tab modes*

- Non-functional "buttons" remind you of key bindings.

*minibuffer mode*

- Use Ctrl-G as always to leave the minibuffer.
- The Find tab is not made visible, but the status area shows the settings.</t>
<t tx="ekr.20240927151701.154">@language rest

The @bool use-find-dialog and @bool minibuffer-find-mode settings comprise
a tri-state setting, as shown in this table:
    
minibuffer-find-mode    use-find-dialog     mode: Ctrl-F puts focus in
--------------------    ---------------     --------------------------
    True                    Ignored         minibuffer
    False                   True            dialog
    False                   False           Find tab in the log pane

*All modes*

- Start the seas with Ctrl-F (start-search).
- Enter the find pattern.
- (Optional) Use &lt;Tab&gt; to enter the search pattern.
- Use &lt;Enter&gt; to start the search.

*dialog and find tab modes*

- Non-functional "buttons" remind you of key bindings.

*minibuffer mode*

- Use Ctrl-G as always to leave the minibuffer.
- The Find tab is not made visible, but the status area shows the settings.</t>
<t tx="ekr.20240927151701.155">Added on-popover to import-html-tags (for leovue)</t>
<t tx="ekr.20240927151701.156"># lowercase html tags, one per line.
# *** Add ons-popover tag for LeoVue.

a
abbr
acronym
address
applet
area
b
base
basefont
bdo
big
blockquote
body
br
button
caption
center
cite
code
col
colgroup
dd
del
dfn
dir
div
dl
dt
em
fieldset
font
form
frame
frameset
head
h1
h2
h3
h4
h5
h6
hr
html
i
iframe
img
input
ins
kbd
label
legend
li
link
map
menu
meta
noframes
noscript
object
ol
ons-popover
optgroup
option
p
param
pre
q
s
samp
script
select
small
span
strike
strong
style
sub
sup
table
tbody
td
textarea
tfoot
th
thead
title
tr
tt
u
ul
var</t>
<t tx="ekr.20240927151701.157"># lowercase xml tags, one per line.

html
body
head
div
table
</t>
<t tx="ekr.20240927151701.170" annotate="7d71002858080000007072696f7269747971014d0f27580a000000707269736574646174657102580a000000323032312d30332d33307103752e"># Recommended plugins, from leoSettings.leo:

plugins_menu.py
mod_scripting.py
nav_qt.py
viewrendered.py


# contextmenu.py      # Required by the vim.py and xemacs.py plugins.
</t>
<t tx="ekr.20240927151701.176"></t>
<t tx="ekr.20240927151701.177"># True: show vr pane when opening a file.</t>
<t tx="ekr.20240927151701.178"># True: hide the vr pane for text-only renderings.</t>
<t tx="ekr.20240927151701.179"></t>
<t tx="ekr.20240927151701.184"></t>
<t tx="ekr.20240927151701.190">Only difference from myLeoSettings.leo

Note: EKRWinowsDark.leo defines comment1_font

All three @color settings work.
The @font setting does not work.
</t>
<t tx="ekr.20240927151701.191">Bold</t>
<t tx="ekr.20240927151701.192">Italics</t>
<t tx="ekr.20240927151701.193"></t>
<t tx="ekr.20240927151701.194"># bold keywords defined in forth-bold-words</t>
<t tx="ekr.20240927151701.195"></t>
<t tx="ekr.20240927151701.196"># Note: the default font size is 12.
rest_comment1_family = None
rest_comment1_size = 12pt
rest_comment1_slant = italic
rest_comment1_weight = None
</t>
<t tx="ekr.20240927151701.202"># Note: Use jj instead of escape to end insert mode.</t>
<t tx="ekr.20240927151701.203" __bookmarks="7d7100580700000069735f6475706571014930300a732e"></t>
<t tx="ekr.20240927151701.206">@language python

"""
Back up this .leo file.

os.environ['LEO_BACKUP'] must be the path to an existing (writable) directory.
"""
c.backup_helper(sub_dir='ekr-tbo-in-rust')
</t>
<t tx="ekr.20240927151701.207">@language python

print(p.gnx)</t>
<t tx="ekr.20240927151701.229">@language python
"""Recursively import all python files in a directory and clean the result."""
@tabwidth -4 # For a better match.
g.cls()
&lt;&lt; rust dir_list &gt;&gt;

dir_ = r'C:\Python\Python3.12\Lib\site-packages\coverage'
dir_ = r'C:\Python\Python3.12\Lib\site-packages\mypyc'
dir_ = r'C:\Python\Python3.12\Lib\site-packages\findimports.py'
dir_ = r'C:\Repos\ruff\crates'

c.recursiveImport(
    dir_=dir_,
    kind = '@clean', # '@auto', '@clean', '@nosent','@file',
    recursive = True,
    safe_at_file = True,
    # '.html', '.js', '.json', '.py', '.rs', '.svg', '.ts', '.tsx']
    # '.codon', '.cpp', '.cc', '.el', '.scm',
    theTypes = ['.py', 'rs'],
    verbose = False,
)
if 1:
    last = c.lastTopLevel()
    last.expand()
    if last.hasChildren():
        last.firstChild().expand()
    c.redraw(last)
print('Done')</t>
<t tx="ekr.20240927151701.230">dir_list = (
    r'C:\Repos\RustPython\common\src',
    r'C:\Repos\RustPython\compiler\codegen\src',
    r'C:\Repos\RustPython\compiler\core\src',
    r'C:\Repos\RustPython\compiler\src',
    r'C:\Repos\RustPython\compiler\codegen\src',  # compile.rs: AST to bytecode.
    r'C:\Repos\RustPython\compiler\core\src', # bytecode.rs: implements bytecodes.
    
    r'C:\Repos\RustPython\derive\src',
    r'C:\Repos\RustPython\derive-impl\src',
    r'C:\Repos\RustPython\pylib\src',
    r'C:\Repos\RustPython\src',
    r'C:\Repos\RustPython\stdlib\src',
    r'C:\Repos\RustPython\vm\src', # compiler.rs.
    r'C:\Repos\RustPython\vm\src\stdlib', # *****ast.rs  Also, many .rs versions of stdlib.
    r'C:\Repos\RustPython\vm\src\vm',  # compile.rs.
    r'C:\Repos\RustPython\vm\src\stdlib\ast', # gen.rs automatically generated by ast/asdl_rs.py.
)
</t>
<t tx="ekr.20240927151701.44"></t>
<t tx="ekr.20240927151701.45"></t>
<t tx="ekr.20240927151701.46">
</t>
<t tx="ekr.20240927151701.47"></t>
<t tx="ekr.20240927151701.48"># This node contains the commands needed to execute a program in a particular language.

# Format: language-name: command

# Create a temporary file if c.p is not any kind of @&lt;file&gt; node.

# Compute the final command as follows:

# 1. If command contains &lt;FILE&gt;, replace &lt;FILE&gt; with the full path to the external file.
# 2. If command contains &lt;NO-FILE&gt;, just remove &lt;NO-FILE&gt;.
# 3. Otherwise, append the full path to the external file to the command.

go: go run . &lt;NO-FILE&gt;
python: python
rust: rustc
</t>
<t tx="ekr.20240927151701.49"># This node contains the regex pattern to determine the line number in error messages.
# Format: language-name: regex pattern
#
# Patterns must define two groups, in either order:
# One group, containing only digits, defines the line number.
# The other group defines the file name.

go: ^\s*(.*):([0-9]+):([0-9]+):.+$
python: ^\s*File "(.+)", line ([0-9]+), in .+$
rust: ^\s*--&gt; (.+):([0-9]+):([0-9]+)\s*$</t>
<t tx="ekr.20240927151701.50">cargo-run
backup
</t>
<t tx="ekr.20240927151701.51"># legacy: (default) Leo's legacy layout
# big-tree: replaces @bool big-outline-pane</t>
<t tx="ekr.20240927151701.63"></t>
<t tx="ekr.20240927151701.64"></t>
<t tx="ekr.20240927151701.65"># The headline must be: @outline-data tree-abbreviations

# A list tree abbreviation names.

# For each abbreviation name, there should be corresponding child node,
# the **abbreviation node** whose headline matches the abbreviation name.

# When a tree abbreviation fires, Leo pastes all the descendants of
# the abbreviation node as the last children of the presently selected node.

importer;;
per-commander-plugin;;
demo;;
</t>
<t tx="ekr.20240927151701.66"></t>
<t tx="ekr.20240927151701.67"></t>
<t tx="ekr.20240927151701.68"></t>
<t tx="ekr.20240927151701.69">'''
A template for demonstrations based on plugins/demo.py.
The demo;; abbreviation will create this tree.
'''
&lt;&lt; imports &gt;&gt;
@others
# Use the *same* command/key binding for demo-start and demo.next.
try:
    if getattr(g.app, 'demo', None):
        g.app.demo.next()
    else:
        g.cls()
        print('starting demo')
        demo = MyDemo(c, trace=False)
        demo.bind('callout', callout)
        demo.bind('title', title)
        demo.start(script_string=script_string)
except Exception:
    g.app.demo = None
    raise
</t>
<t tx="ekr.20240927151701.70">if c.isChanged(): c.save()
import imp
from leo.core.leoQt import QtGui
import leo.plugins.demo as demo_module
imp.reload(demo_module)</t>
<t tx="ekr.20240927151701.71"># A short example. Change as needed.
script_string = '''\
callout('Callout 1 centered')
title('This is title 1')
###
callout('Callout 2 (700, 200)', position=[700, 200])
title('This is title 2')
demo.next()
'''
</t>
<t tx="ekr.20240927151701.72">class MyDemo (demo_module.Demo):
    
    def setup_script(self):
        '''Delete all previously shown widgets.'''
        self.delete_widgets()</t>
<t tx="ekr.20240927151701.73">def callout(text, **keys):
    w = demo_module.Callout(text, **keys)
    
def title(text, **keys):
    w = demo_module.Title(text, **keys)
</t>
<t tx="ekr.20240927151701.74"></t>
<t tx="ekr.20240927151701.75">&lt;&lt; docstring &gt;&gt;
### From leoSettings.leo
# Created 2017/05/30
@language python
@tabwidth -4
__version__ = '0.0'
&lt;&lt; version history &gt;&gt;
&lt;&lt; imports &gt;&gt;
@others</t>
<t tx="ekr.20240927151701.76">'''
&lt;|docstring|&gt;
'''
</t>
<t tx="ekr.20240927151701.77">@
Put notes about each version here.
&lt;|Initial version notes|&gt;</t>
<t tx="ekr.20240927151701.78">import leo.core.leoGlobals as g

&lt;|imports|&gt;</t>
<t tx="ekr.20240927151701.79">def init ():
        
    ok = g.app.gui.guiName() in ('qt','qttabs')
    if ok:
        if 1: # Create the commander class *before* the frame is created.
            g.registerHandler('before-create-leo-frame',onCreate)
        else: # Create the commander class *after* the frame is created.
            g.registerHandler('after-create-leo-frame',onCreate)
        g.plugin_signon(__name__)   
    return ok
</t>
<t tx="ekr.20240927151701.80">def onCreate (tag, keys):
    
    c = keys.get('c')
    if c:
        thePluginController = pluginController(c)
</t>
<t tx="ekr.20240927151701.81">class &lt;|Controller Class Name|&gt;:
    
    @others</t>
<t tx="ekr.20240927151701.82">def __init__ (self,c):
    
    self.c = c
    # Warning: hook handlers must use keywords.get('c'), NOT self.c.
    &lt;|ivars|&gt;</t>
<t tx="ekr.20240927151701.83"></t>
<t tx="ekr.20240927151701.84">'''
The @auto importer for the {|{x=get_language()}|} language.

Created {|{x=time.strftime("%Y/%m/%d")}|} by the `importer;;` abbreviation.
'''
import leo.plugins.importers.linescanner as linescanner
Importer = linescanner.Importer
@others
importer_dict = {
    'class': {|{x=cap_name}|}_Importer,
    'extensions': [&lt;|comma-separated lists of extensions|&gt;],
        # Example: ['.c', '.cc', '.c++', '.cpp', '.cxx', '.h', '.h++']
}
@language python
@tabwidth -4


</t>
<t tx="ekr.20240927151701.85">class {|{x=cap_name}|}_Importer(Importer):
    '''The importer for the {|{x=name}|} language.'''

    def __init__(self, importCommands):
        '''{|{x=cap_name}|}_Importer.__init__'''
        # Init the base class.
        Importer.__init__(self,
            importCommands,
            language = '{|{x=name}|}',
            state_class = {|{x=cap_name}|}_ScanState,
            strict = &lt;|True leading whitespace is significant. Otherwise False|&gt;,
        )
        
    @others
</t>
<t tx="ekr.20240927151701.86"># These can be overridden in subclasses.
</t>
<t tx="ekr.20240927151701.87">### define an override if desired...

if 0: # The base class
    def clean_headline(self, s):
        '''Return a cleaned up headline s.'''
        return s.strip()
        
# A more complex example, for the C language.

# def clean_headline(self, s):
    # '''Return a cleaned up headline s.'''
    # import re
    # type1 = r'(static|extern)*'
    # type2 = r'(void|int|float|double|char)*'
    # class_pattern = r'\s*(%s)\s*class\s+(\w+)' % (type1)
    # pattern = r'\s*(%s)\s*(%s)\s*(\w+)' % (type1, type2)
    # m = re.match(class_pattern, s)
    # if m:
        # prefix1 = '%s ' % (m.group(1)) if m.group(1) else ''
        # return '%sclass %s' % (prefix1, m.group(2))
    # m = re.match(pattern, s)
    # if m:
        # prefix1 = '%s ' % (m.group(1)) if m.group(1) else ''
        # prefix2 = '%s ' % (m.group(2)) if m.group(2) else ''
        # h = m.group(3) or '&lt;no c function name&gt;'
        # return '%s%s%s' % (prefix1, prefix2, h)
    # else:
        # return s
</t>
<t tx="ekr.20240927151701.88">def clean_nodes(self, parent):
    '''
    Clean all nodes in parent's tree.
    Subclasses override this as desired.
    See perl_i.clean_nodes for an example.
    '''
    pass
</t>
<t tx="ekr.20240927151701.89">class {|{x=cap_name}|}_ScanState:
    '''A class representing the state of the {|{x=name}|} line-oriented scan.'''
    
    def __init__(self, d=None):
        '''{|{x=cap_name}|}_ScanState.__init__'''
        if d:
            prev = d.get('prev')
            self.context = prev.context
            ### Adjust these by hand.
            self.curlies = prev.curlies
        else:
            self.context = ''
            ### Adjust these by hand.
            self.curlies = 0

    def __repr__(self):
        '''{|{x=cap_name}|}_ScanState.__repr__'''
        ### Adjust these by hand.
        return "{|{x=cap_name}|}_ScanState context: %r curlies: %s" % (
            self.context, self.curlies)

    __str__ = __repr__

    @others

</t>
<t tx="ekr.20240927151701.9" __bookmarks="7d7100580700000069735f6475706571014930300a732e">@language rest
@wrap

The @settings tree contains all active settings. 

Settings outside this tree have no effect.</t>
<t tx="ekr.20240927151701.90">def level(self):
    '''{|{x=cap_name}|}_ScanState.level.'''
    return &lt;|self.curlies|&gt;
        ### Examples:
        # self.indent # for python, coffeescript.
        # self.curlies
        # (self, curlies, self.parens)
</t>
<t tx="ekr.20240927151701.91">def update(self, data):
    '''
    {|{x=cap_name}|}_ScanState.update

    Update the state using the 6-tuple returned by v2_scan_line.
    Return i = data[1]
    '''
    context, i, delta_c, delta_p, delta_s, bs_nl = data
    # All ScanState classes must have a context ivar.
    self.context = context
    self.curlies += delta_c  
    ### Update {|{x=cap_name}|}_ScanState ivars
    # self.bs_nl = bs_nl
    # self.parens += delta_p
    # self.squares += delta_s
    return i
</t>
<t tx="ekr.20240927151701.92"></t>
<t tx="ekr.20240927151701.93"></t>
<t tx="ekr.20240927151701.94">True: same as recent_files_group, except that even files (basenames) which are unique
have their containing path listed in the submenu - so visual clutter is reduced
but you can still see where things come from before you load them.

False: don't use submenus for multiple path entries, unless recent_files_group
is true (and recent_files_omit_directories is False)
</t>
<t tx="ekr.20240927151701.95"></t>
<t tx="ekr.20240927151701.96">True: show user tips on startup.</t>
<t tx="ekr.20240927151701.97"></t>
<t tx="ekr.20240927151701.98"></t>
<t tx="ekr.20240927151701.99"></t>
<t tx="ekr.20240927152759.1">@language python
g.cls()
import os
import subprocess

if c.changed:
    c.save()
command = 'cargo run'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240927153018.1"></t>
<t tx="ekr.20240927154009.1"></t>
<t tx="ekr.20240927154016.1">@language rest
@nowrap

code:

ekr-tbo-in-rust: https://github.com/edreamleo/ekr-tbo-in-rust
ruff_python_parser: https://github.com/astral-sh/ruff/tree/main/crates/ruff_python_parser/src
lexer.rs: https://github.com/astral-sh/ruff/blob/main/crates/ruff_python_parser/src/lexer.rs

docs:
Rust Book: https://doc.rust-lang.org/stable/book/title-page.html

Aha: Refactoring is harder in Rust.
</t>
<t tx="ekr.20240927154323.1">@language rest
@wrap

- Transliterate python tbo.beautify (main token loop)

- Create write_token! macro?</t>
<t tx="ekr.20240927163405.1">#[derive(Debug)]
pub struct Beautifier {
    args: Vec&lt;String&gt;,
    files_list: Vec&lt;String&gt;,
    input_list: Vec&lt;InputTok&gt;,
    output_list: Vec&lt;OutputTok&gt;,
}

///// Temporary.
#[allow(dead_code)]
#[allow(non_snake_case)]
impl Beautifier {
    @others
}
</t>
<t tx="ekr.20240927163405.10">fn do_Complex(&amp;mut self, tok_value: &amp;str) {
    self.add_output_token("Complex", tok_value);
}
</t>
<t tx="ekr.20240927163405.100">fn do_StartModule(&amp;mut self) {
    // self.add_output_token("StartModule", "");
    println!("do_StartModule");
}
</t>
<t tx="ekr.20240927163405.101">fn do_Tilde(&amp;mut self) {
    self.add_output_token("Tilde", "~");
}
</t>
<t tx="ekr.20240927163405.102">fn do_True(&amp;mut self) {
    self.add_output_token("True", "True");
}
</t>
<t tx="ekr.20240927163405.103">fn do_Try(&amp;mut self) {
    self.add_output_token("Try", "try");
}
</t>
<t tx="ekr.20240927163405.104">fn do_Type(&amp;mut self) {
    self.add_output_token("Type", "type");
}
</t>
<t tx="ekr.20240927163405.105">fn do_Vbar(&amp;mut self) {
    self.add_output_token("Vbar", "|");
}
</t>
<t tx="ekr.20240927163405.106">fn do_VbarEqual(&amp;mut self) {
    self.add_output_token("VbarEqual", "|=");
}
</t>
<t tx="ekr.20240927163405.107">fn do_While(&amp;mut self) {
    self.add_output_token("While", "while");
}
</t>
<t tx="ekr.20240927163405.108">fn do_With(&amp;mut self) {
    self.add_output_token("With", "with");
}
</t>
<t tx="ekr.20240927163405.109">fn do_Yield(&amp;mut self) {
    self.add_output_token("Yield", "yield");
}
</t>
<t tx="ekr.20240927163405.11">fn do_Float(&amp;mut self, tok_value: &amp;str) {
    self.add_output_token("Float", tok_value);
}
</t>
<t tx="ekr.20240927163405.110">fn enabled(&amp;self, arg: &amp;str) -&gt; bool {
    //! Beautifier::enabled: return true if the given command-line argument is enabled.
    //! Example:  x.enabled("--report");
    return self.args.contains(&amp;arg.to_string());

}
</t>
<t tx="ekr.20240927163405.111">fn get_args(&amp;mut self) {
    //! Beautifier::get_args: Set the args and files_list ivars.
    let args: Vec&lt;String&gt; = env::args().collect();
    let valid_args = vec![
        "--all", 
        "--beautified",
        "--diff",
        "-h", "--help",
        "--report",
        "--write",
    ];
    for (i, arg) in args.iter().enumerate() {
        if i &gt; 0 {
            if valid_args.contains(&amp;arg.as_str()) {
                self.args.push(arg.to_string())
            }
            else if 
                arg.as_str().starts_with("--") ||
                arg.as_str().starts_with("--")
            {
                println!("Ignoring invalid arg: {arg}");
            }
            else {
                println!("File: {arg}");
                self.files_list.push(arg.to_string());
            }
        }
    }
}
</t>
<t tx="ekr.20240927163405.112">fn make_input_list(&amp;mut self, contents: &amp;str) -&gt; u32 {

    let mut count: u32 = 0;
    let results = lex(&amp;contents, Mode::Module);  // An iterator yielding Option(Tok).
    for result in results {
        use Tok::*;
        count += 1;
        let token = result.ok().unwrap();
        let (ref tok_class, tok_range) = token;
        let tok_value = &amp;contents[tok_range];

        // Variants names are necessary, but otherwise not used.
        #[allow(unused_variables)]
        let class_name = match tok_class {
            // Tokens with values...
            // Use tok_value for *all* values.
            Comment(value) =&gt; "Comment",  // No idea why parens are needed here.
            Complex { real, imag } =&gt; "Complex",
            Float { value } =&gt; "Float",
            Int { value } =&gt; "Int",
            Name { name } =&gt; "Name",
            Tok::String { value, kind, triple_quoted } =&gt; "String",
            
            // Common tokens...
            Class =&gt; "Class",
            Dedent =&gt; "Dedent",
            Def =&gt; "Def",
            Indent =&gt; "Indent",
            Newline =&gt; "Newline",
            NonLogicalNewline =&gt; "NonLogicalNewline",

            // All other tokens...
            Amper =&gt; "Amper",
            AmperEqual =&gt; "AmperEqual",
            And =&gt; "And",
            As =&gt; "As",
            Assert =&gt; "Assert",
            Async =&gt; "Async",
            At =&gt; "At",
            AtEqual =&gt; "AtEqual",
            Await =&gt; "Await",
            Break =&gt; "Break",
            Case =&gt; "Case",
            CircumFlex =&gt; "CircumFlex",
            CircumflexEqual =&gt; "CircumflexEqual",
            Colon =&gt; "Colon",
            ColonEqual =&gt; "ColonEqual",
            Comma =&gt; "Comma",
            Continue =&gt; "Continue",
            Del =&gt; "Del",
            Dot =&gt; "Dot",
            DoubleSlash =&gt; "DoubleSlash",
            DoubleSlashEqual =&gt; "DoubleSlashEqual",
            DoubleStar =&gt; "DoubleStar",
            DoubleStarEqual =&gt; "DoubleStarEqual",
            Elif =&gt; "Elif",
            Ellipsis =&gt; "Ellipsis",
            Else =&gt; "Else",
            EndOfFile =&gt; "EndOfFile",
            EqEqual =&gt; "EqEqual",
            Equal =&gt; "Equal",
            Except =&gt; "Except",
            False =&gt; "False",
            Finally =&gt; "Finally",
            For =&gt; "For",
            From =&gt; "From",
            Global =&gt; "Global",
            Greater =&gt; "Greater",
            GreaterEqual =&gt; "GreaterEqual",
            If =&gt; "If",
            Import =&gt; "Import",
            In =&gt; "In",
            Is =&gt; "Is",
            Lambda =&gt; "Lambda",
            Lbrace =&gt; "Lbrace",
            LeftShift =&gt; "LeftShift",
            LeftShiftEqual =&gt; "LeftShiftEqual",
            Less =&gt; "Less",
            LessEqual =&gt; "LessEqual",
            Lpar =&gt; "Lpar",
            Lsqb =&gt; "Lsqb",
            Match =&gt; "Match",
            Minus =&gt; "Minus",
            MinusEqual =&gt; "MinusEqual",
            None =&gt; "None",
            Nonlocal =&gt; "Nonlocal",
            Not =&gt; "Not",
            NotEqual =&gt; "NotEqual",
            Or =&gt; "Or",
            Pass =&gt; "Pass",
            Percent =&gt; "Percent",
            PercentEqual =&gt; "PercentEqual",
            Plus =&gt; "Plus",
            PlusEqual =&gt; "PlusEqual",
            Raise =&gt; "Raise",
            Rarrow =&gt; "Rarrow",
            Rbrace =&gt; "Rbrace",
            Return =&gt; "Return",
            RightShift =&gt; "RightShift",
            RightShiftEqual =&gt; "RightShiftEqual",
            Rpar =&gt; "Rpar",
            Rsqb =&gt; "Rsqb",
            Semi =&gt; "Semi",
            Slash =&gt; "Slash",
            SlashEqual =&gt; "SlashEqual",
            Star =&gt; "Star",
            StarEqual =&gt; "StarEqual",
            StartExpression =&gt; "StartExpression",
            StartInteractive =&gt; "StartInteractive",
            StartModule =&gt; "StartModule",
            Tilde =&gt; "Tilde",
            True =&gt; "True",
            Try =&gt; "Try",
            Type =&gt; "Type",
            Vbar =&gt; "Vbar",
            VbarEqual =&gt; "VbarEqual",
            While =&gt; "While",
            With =&gt; "With",
            Yield =&gt; "Yield",
        };
        self.add_input_token(class_name, tok_value);
    }
    return count;
}
</t>
<t tx="ekr.20240927163405.113">fn make_output_list(&amp;mut self) {

    //// let mut lws = String::new();

    // for input_token in self.input_list.iter() {
    for input_token in &amp;self.input_list.clone() {
        // println!("{:?}", input_token);
        self.make_output_token(input_token);
    }
}
</t>
<t tx="ekr.20240927163405.114">fn make_output_token(&amp;mut self, input_token: &amp;InputTok) {
    self.add_output_token (input_token.kind.as_str(), input_token.value.as_str());
}
</t>
<t tx="ekr.20240927163405.115">pub fn new() -&gt; Beautifier {
    let mut x = Beautifier {
        args: Vec::new(),
        files_list: Vec::new(),
        input_list: Vec::new(),
        output_list: Vec::new(),
    };
    x.get_args();
    return x;
}
</t>
<t tx="ekr.20240927163405.116">fn show_args (&amp;self) {
    println!("Command-line arguments...");
    for (i, arg) in self.args.iter().enumerate() {
        if i &gt; 0 {
            println!("  {arg}");
        }
    }
    for file_arg in self.files_list.iter() {
        println!("  {file_arg}");
    }
}
</t>
<t tx="ekr.20240927163405.117">fn show_help (&amp;self) {
    //! Beautifier::show_help: print the help messages.
    println!("{}", textwrap::dedent("
        Beautify or diff files.

        -h --help:      Print this help message and exit.
        --all:          Beautify all files, even unchanged files.
        --beautified:   Report beautified files individually, even if not written.
        --diff:         Show diffs instead of changing files.
        --report:       Print summary report.
        --write:        Write beautifed files (dry-run mode otherwise).
    "));
}
</t>
<t tx="ekr.20240927163405.118">fn show_output_list (&amp;self) {
    println!("\nOutput tokens...");
    for (i, arg) in self.output_list.iter().enumerate() {
        if i &gt; 0 {
            print!("{:?}", arg);
        }
    }
}
</t>
<t tx="ekr.20240927163405.119">fn tokenize_contents(&amp;mut self, contents: String ) -&gt; u32 {

    let count = self.make_input_list(&amp;contents);
    self.make_output_list();
    return count;
}
</t>
<t tx="ekr.20240927163405.12">fn do_Int(&amp;mut self, tok_value: &amp;str) {
    self.add_output_token("Int", tok_value);
}
</t>
<t tx="ekr.20240927163405.13">fn do_Name(&amp;mut self, tok_value: &amp;str) {
    self.add_output_token("Name", tok_value);
}
</t>
<t tx="ekr.20240927163405.14">fn do_String(&amp;mut self, tok_value: &amp;str) {
    // correct.
    // print!("{tok_value}");
    
    // incorrect.
        // let quote = if *triple_quoted {"'''"} else {"'"};
        // print!("{:?}:{quote}{value}{quote}", kind);

    self.add_output_token("String", tok_value);
}
</t>
<t tx="ekr.20240927163405.15"></t>
<t tx="ekr.20240927163405.16">fn do_Dedent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_token("Dedent", tok_value);
}
</t>
<t tx="ekr.20240927163405.17">fn do_Indent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_token("Indent", tok_value);
}
</t>
<t tx="ekr.20240927163405.18">fn do_Newline(&amp;mut self) {
    self.add_output_token("Indent", "\n");
}
</t>
<t tx="ekr.20240927163405.19">fn do_NonLogicalNewline(&amp;mut self) {
    self.add_output_token("Indent", "\n");
}
</t>
<t tx="ekr.20240927163405.2">// #[allow(dead_code)]
fn add_input_token (&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    // println!("{:?}", kind);
    self.input_list.push(InputTok {
        kind: kind.to_string(),
        value: value.to_string(),
    });
}
</t>
<t tx="ekr.20240927163405.20"></t>
<t tx="ekr.20240927163405.21">fn do_Amper(&amp;mut self) {
    self.add_output_token("Amper", "&amp;");
}
</t>
<t tx="ekr.20240927163405.22">fn do_AmperEqual(&amp;mut self) {
    self.add_output_token("AmperEqual", "&amp;=");
}
</t>
<t tx="ekr.20240927163405.23">fn do_And(&amp;mut self) {
    self.add_output_token("And", "and");
}
</t>
<t tx="ekr.20240927163405.24">fn do_As(&amp;mut self) {
    self.add_output_token("As", "as");
}
</t>
<t tx="ekr.20240927163405.25">fn do_Assert(&amp;mut self) {
    self.add_output_token("Assert", "assert");
}
</t>
<t tx="ekr.20240927163405.26">fn do_Async(&amp;mut self) {
    self.add_output_token("Async", "async");
}
</t>
<t tx="ekr.20240927163405.27">fn do_At(&amp;mut self) {
    self.add_output_token("At", "@");
}
</t>
<t tx="ekr.20240927163405.28">fn do_AtEqual(&amp;mut self) {
    self.add_output_token("AtEqual", "@=");
}
</t>
<t tx="ekr.20240927163405.29">fn do_Await(&amp;mut self) {
    self.add_output_token("Await", "await");
}
</t>
<t tx="ekr.20240927163405.3">fn add_output_token (&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    self.output_list.push(OutputTok {
        kind: kind.to_string(),
        value: value.to_string(),
    });
}
</t>
<t tx="ekr.20240927163405.30">fn do_Break(&amp;mut self) {
    self.add_output_token("Break", "break");
}
</t>
<t tx="ekr.20240927163405.31">fn do_Case(&amp;mut self) {
    self.add_output_token("Case", "case");
}
</t>
<t tx="ekr.20240927163405.32">fn do_CircumFlex(&amp;mut self) {
    self.add_output_token("CircumFlex", "^");
}
</t>
<t tx="ekr.20240927163405.33">fn do_CircumflexEqual(&amp;mut self) {
    self.add_output_token("CircumflexEqual", "^=");
}
</t>
<t tx="ekr.20240927163405.34">fn do_Class(&amp;mut self) {
    self.add_output_token("Class", "class");
}
</t>
<t tx="ekr.20240927163405.35">fn do_Colon(&amp;mut self) {
    self.add_output_token("Colon", ":");
}
</t>
<t tx="ekr.20240927163405.36">fn do_ColonEqual(&amp;mut self) {
    self.add_output_token("ColonEqual", ":=");
}
</t>
<t tx="ekr.20240927163405.37">fn do_Comma(&amp;mut self) {
    self.add_output_token("Comma", ",");
}
</t>
<t tx="ekr.20240927163405.38">fn do_Continue(&amp;mut self) {
    self.add_output_token("Continue", "continue");
}
</t>
<t tx="ekr.20240927163405.39">fn do_Def(&amp;mut self) {
    self.add_output_token("Def", "def");
}
</t>
<t tx="ekr.20240927163405.4">pub fn beautify_all_files(&amp;mut self) {
    println!("beautify_all_files");
    for file_name in self.files_list.clone() {
        self.beautify_one_file(file_name);
    }
}

</t>
<t tx="ekr.20240927163405.40">fn do_Del(&amp;mut self) {
    self.add_output_token("Del", "del");
}
</t>
<t tx="ekr.20240927163405.41">fn do_Dot(&amp;mut self) {
    self.add_output_token("Dot", ".");
}
</t>
<t tx="ekr.20240927163405.42">fn do_DoubleSlash(&amp;mut self) {
    self.add_output_token("DoubleSlash", "//");
}
</t>
<t tx="ekr.20240927163405.43">fn do_DoubleSlashEqual(&amp;mut self) {
    self.add_output_token("DoubleSlashEqual", "//=");
}
</t>
<t tx="ekr.20240927163405.44">fn do_DoubleStar(&amp;mut self) {
    self.add_output_token("DoubleStar", "**");
}
</t>
<t tx="ekr.20240927163405.45">fn do_DoubleStarEqual(&amp;mut self) {
    self.add_output_token("DoubleStarEqual", "**=");
}
</t>
<t tx="ekr.20240927163405.46">fn do_Elif(&amp;mut self) {
    self.add_output_token("Elif", "elif");
}
</t>
<t tx="ekr.20240927163405.47">fn do_Ellipsis(&amp;mut self) {
    self.add_output_token("Ellipsis", "...");
}
</t>
<t tx="ekr.20240927163405.48">fn do_Else(&amp;mut self) {
    self.add_output_token("Else", "else");
}
</t>
<t tx="ekr.20240927163405.49">fn do_EndOfFile(&amp;mut self) {
    self.add_output_token("EndOfFile", "EOF");
}
</t>
<t tx="ekr.20240927163405.5">fn beautify_one_file(&amp;mut self, file_name: String) {
    // println!("beautifiy_one_file: {file_name}");
    self.output_list = Vec::new();
    // Read the file into contents (a String).
    let t1 = std::time::Instant::now();
    let contents = fs::read_to_string(file_name.clone())
        .expect("Error reading{file_name}");
    // print_type(&amp;contents, "contents");
    let t2 = t1.elapsed();
    // Tokenize.
    let t3 = std::time::Instant::now();
    let n_tokens = self.tokenize_contents(contents);
    let t4 = t3.elapsed();
    // Report
    if self.enabled("--report") {
        println!(" file name: {file_name}");
        println!("      read: {:.2?}", t2);
        println!("  tokenize: {:.2?}", t4);
        println!("    tokens: {n_tokens}");
    }
    // Show tokens.
    &lt;&lt; show output_list &gt;&gt;
}
</t>
<t tx="ekr.20240927163405.50">fn do_EqEqual(&amp;mut self) {
    self.add_output_token("EqEqual", "==");
}
</t>
<t tx="ekr.20240927163405.51">fn do_Equal(&amp;mut self) {
    self.add_output_token("Equal", "=");
}
</t>
<t tx="ekr.20240927163405.52">fn do_Except(&amp;mut self) {
    self.add_output_token("Except", "except");
}
</t>
<t tx="ekr.20240927163405.53">fn do_False(&amp;mut self) {
    self.add_output_token("False", "False");
}
</t>
<t tx="ekr.20240927163405.54">fn do_Finally(&amp;mut self) {
    self.add_output_token("Finally", "finally");
}
</t>
<t tx="ekr.20240927163405.55">fn do_For(&amp;mut self) {
    self.add_output_token("For", "for");
}
</t>
<t tx="ekr.20240927163405.56">fn do_From(&amp;mut self) {
    self.add_output_token("From", "from");
}
</t>
<t tx="ekr.20240927163405.57">fn do_Global(&amp;mut self) {
    self.add_output_token("Global", "global");
}
</t>
<t tx="ekr.20240927163405.58">fn do_Greater(&amp;mut self) {
    self.add_output_token("Greater", "&gt;");
}
</t>
<t tx="ekr.20240927163405.59">fn do_GreaterEqual(&amp;mut self) {
    self.add_output_token("GreaterEqual", "&gt;-");
}
</t>
<t tx="ekr.20240927163405.6">if false {  // --show-output
    self.show_output_list()
}
</t>
<t tx="ekr.20240927163405.60">fn do_If(&amp;mut self) {
    self.add_output_token("If", "if");
}
</t>
<t tx="ekr.20240927163405.61">fn do_Import(&amp;mut self) {
    self.add_output_token("Import", "import");
}
</t>
<t tx="ekr.20240927163405.62">fn do_In(&amp;mut self) {
    self.add_output_token("In", "in");
}
</t>
<t tx="ekr.20240927163405.63">fn do_Is(&amp;mut self) {
    self.add_output_token("Is", "is");
}
</t>
<t tx="ekr.20240927163405.64">fn do_Lambda(&amp;mut self) {
    self.add_output_token("Lambda", "lambda");
}
</t>
<t tx="ekr.20240927163405.65">fn do_Lbrace(&amp;mut self) {
    self.add_output_token("Lbrace", "[");
}
</t>
<t tx="ekr.20240927163405.66">fn do_LeftShift(&amp;mut self) {
    self.add_output_token("LeftShift", "&lt;&lt;");
}
</t>
<t tx="ekr.20240927163405.67">fn do_LeftShiftEqual(&amp;mut self) {
    self.add_output_token("LeftShiftEqual", "&lt;&lt;=");
}
</t>
<t tx="ekr.20240927163405.68">fn do_Less(&amp;mut self) {
    self.add_output_token("Less", "&lt;");
}
</t>
<t tx="ekr.20240927163405.69">fn do_LessEqual(&amp;mut self) {
    self.add_output_token("LessEqual", "&lt;=");
}
</t>
<t tx="ekr.20240927163405.7"></t>
<t tx="ekr.20240927163405.70">fn do_Lpar(&amp;mut self) {
    self.add_output_token("Lpar", "(");
}
</t>
<t tx="ekr.20240927163405.71">fn do_Lsqb(&amp;mut self) {
    self.add_output_token("Lsqb", "[");
}
</t>
<t tx="ekr.20240927163405.72">fn do_Match(&amp;mut self) {
    self.add_output_token("Match", "match");
}
</t>
<t tx="ekr.20240927163405.73">fn do_Minus(&amp;mut self) {
    self.add_output_token("Minus", "-");
}
</t>
<t tx="ekr.20240927163405.74">fn do_MinusEqual(&amp;mut self) {
    self.add_output_token("MinusEqual", "-=");
}
</t>
<t tx="ekr.20240927163405.75">fn do_None(&amp;mut self) {
    self.add_output_token("None", "None");
}
</t>
<t tx="ekr.20240927163405.76">fn do_Nonlocal(&amp;mut self) {
    self.add_output_token("Nonlocal", "nonlocal");
}
</t>
<t tx="ekr.20240927163405.77">fn do_Not(&amp;mut self) {
    self.add_output_token("Not", "not");
}
</t>
<t tx="ekr.20240927163405.78">fn do_NotEqual(&amp;mut self) {
    self.add_output_token("NotEqual", "!=");
}
</t>
<t tx="ekr.20240927163405.79">fn do_Or(&amp;mut self) {
    self.add_output_token("Or", "or");
}
</t>
<t tx="ekr.20240927163405.8"></t>
<t tx="ekr.20240927163405.80">fn do_Pass(&amp;mut self) {
    self.add_output_token("Pass", "pass");
}
</t>
<t tx="ekr.20240927163405.81">fn do_Percent(&amp;mut self) {
    self.add_output_token("Percent", "%");
}
</t>
<t tx="ekr.20240927163405.82">fn do_PercentEqual(&amp;mut self) {
    self.add_output_token("PercentEqual", "%=");
}
</t>
<t tx="ekr.20240927163405.83">fn do_Plus(&amp;mut self) {
    self.add_output_token("Plus", "+");
}
</t>
<t tx="ekr.20240927163405.84">fn do_PlusEqual(&amp;mut self) {
    self.add_output_token("PlusEqual", "+=");
}
</t>
<t tx="ekr.20240927163405.85">fn do_Raise(&amp;mut self) {
    self.add_output_token("Raise", "raise");
}
</t>
<t tx="ekr.20240927163405.86">fn do_Rarrow(&amp;mut self) {
    self.add_output_token("Rarrow", "-&gt;");
}
</t>
<t tx="ekr.20240927163405.87">fn do_Rbrace(&amp;mut self) {
    self.add_output_token("Rbrace", "]");
}
</t>
<t tx="ekr.20240927163405.88">fn do_Return(&amp;mut self) {
    self.add_output_token("Return", "return");
}
</t>
<t tx="ekr.20240927163405.89">fn do_RightShift(&amp;mut self) {
    self.add_output_token("RightShift", "&gt;&gt;");
}
</t>
<t tx="ekr.20240927163405.9">fn do_Comment(&amp;mut self, tok_value: &amp;str) {
    // print!("{tok_value}");  // Correct.
    // print!("{value} ");  // Wrong!
    self.add_output_token("Comment", tok_value);
}
</t>
<t tx="ekr.20240927163405.90">fn do_RightShiftEqual(&amp;mut self) {
    self.add_output_token("RightShiftEqual", "&gt;&gt;=");
}
</t>
<t tx="ekr.20240927163405.91">fn do_Rpar(&amp;mut self) {
    self.add_output_token("Rpar", ")");
}
</t>
<t tx="ekr.20240927163405.92">fn do_Rsqb(&amp;mut self) {
    self.add_output_token("Rsqb", "]");
}
</t>
<t tx="ekr.20240927163405.93">fn do_Semi(&amp;mut self) {
    self.add_output_token("Semi", ";");
}
</t>
<t tx="ekr.20240927163405.94">fn do_Slash(&amp;mut self) {
    self.add_output_token("Slash", "/");
}
</t>
<t tx="ekr.20240927163405.95">fn do_SlashEqual(&amp;mut self) {
    self.add_output_token("SlashEqual", "/=");
}
</t>
<t tx="ekr.20240927163405.96">fn do_Star(&amp;mut self) {
    self.add_output_token("Star", "*");
}
</t>
<t tx="ekr.20240927163405.97">fn do_StarEqual(&amp;mut self) {
    self.add_output_token("StarEqual", "*=");
}
</t>
<t tx="ekr.20240927163405.98">fn do_StartExpression(&amp;mut self) {
    // self.add_output_token("StartExpression", "");
}
</t>
<t tx="ekr.20240927163405.99">fn do_StartInteractive(&amp;mut self) {
    // self.add_output_token("StartModule", "");
}
</t>
<t tx="ekr.20240927163812.1"></t>
<t tx="ekr.20240927163855.1">@language python

g.cls()
print('@command test')
import os
if c.isChanged():
    c.save()
root = r'C:\Repos\ekr-private\rust_beautifier'
os.chdir(root)
for command in (
    'cargo run -- --all --report --write c:/Repos/leo-editor/leo/core/leoTokens.py',
):
    print(command)
    os.system(command)
</t>
<t tx="ekr.20240927163907.1">@language rest
@wrap

The Rust version of Leo's beautifier *must* generate pseudo 'ws' tokens, just as in leoTokens.py because 'ws' tokens are needed to support the @nobeautify directive.

Stats:

 file name: c:/Repos/leo-editor/leo/core/leoTokens.py
      read: 484.20s
  tokenize: 11.9ms to 12.4ms
    tokens: 9739</t>
<t tx="ekr.20240927163942.1">//! leoTokens.rs: A beautifier for Python that uses *only* tokens.

&lt;&lt; leoTokens.rs: global suppressions &gt;&gt;
&lt;&lt; leoTokens.rs: use statements &gt;&gt;

@others

@language rust
@tabwidth -4
@pagewidth 70
</t>
<t tx="ekr.20240927163942.10">if false {  // --show-output
    self.show_output_list()
}
</t>
<t tx="ekr.20240927163942.100">fn do_Star(&amp;mut self) {
    self.add_output_string("Star", "*");
}
</t>
<t tx="ekr.20240927163942.101">fn do_StarEqual(&amp;mut self) {
    self.add_output_string("StarEqual", "*=");
}
</t>
<t tx="ekr.20240927163942.102">fn do_StartExpression(&amp;mut self) {
    // self.add_output_string("StartExpression", "");
}
</t>
<t tx="ekr.20240927163942.103">fn do_StartInteractive(&amp;mut self) {
    // self.add_output_string("StartModule", "");
}
</t>
<t tx="ekr.20240927163942.104">fn do_StartModule(&amp;mut self) {
    // self.add_output_string("StartModule", "");
    println!("do_StartModule");
}
</t>
<t tx="ekr.20240927163942.105">fn do_Tilde(&amp;mut self) {
    self.add_output_string("Tilde", "~");
}
</t>
<t tx="ekr.20240927163942.106">fn do_True(&amp;mut self) {
    self.add_output_string("True", "True");
}
</t>
<t tx="ekr.20240927163942.107">fn do_Try(&amp;mut self) {
    self.add_output_string("Try", "try");
}
</t>
<t tx="ekr.20240927163942.108">fn do_Type(&amp;mut self) {
    self.add_output_string("Type", "type");
}
</t>
<t tx="ekr.20240927163942.109">fn do_Vbar(&amp;mut self) {
    self.add_output_string("Vbar", "|");
}
</t>
<t tx="ekr.20240927163942.11"></t>
<t tx="ekr.20240927163942.110">fn do_VbarEqual(&amp;mut self) {
    self.add_output_string("VbarEqual", "|=");
}
</t>
<t tx="ekr.20240927163942.111">fn do_While(&amp;mut self) {
    self.add_output_string("While", "while");
}
</t>
<t tx="ekr.20240927163942.112">fn do_With(&amp;mut self) {
    self.add_output_string("With", "with");
}
</t>
<t tx="ekr.20240927163942.113">fn do_Yield(&amp;mut self) {
    self.add_output_string("Yield", "yield");
}
</t>
<t tx="ekr.20240927163942.114">fn enabled(&amp;self, arg: &amp;str) -&gt; bool {
    //! Beautifier::enabled: return true if the given command-line argument is enabled.
    //! Example:  x.enabled("--report");
    return self.args.contains(&amp;arg.to_string());

}
</t>
<t tx="ekr.20240927163942.115">fn get_args(&amp;mut self) {
    //! Beautifier::get_args: Set the args and files_list ivars.
    let args: Vec&lt;String&gt; = env::args().collect();
    let valid_args = vec![
        "--all", 
        "--beautified",
        "--diff",
        "-h", "--help",
        "--report",
        "--write",
    ];
    for (i, arg) in args.iter().enumerate() {
        if i &gt; 0 {
            if valid_args.contains(&amp;arg.as_str()) {
                self.args.push(arg.to_string())
            }
            else if 
                arg.as_str().starts_with("--") ||
                arg.as_str().starts_with("--")
            {
                println!("Ignoring invalid arg: {arg}");
            }
            else {
                println!("File: {arg}");
                self.files_list.push(arg.to_string());
            }
        }
    }
}
</t>
<t tx="ekr.20240927163942.116">fn make_input_list(&amp;mut self, contents: &amp;str) -&gt; u32 {

    let mut count: u32 = 0;
    let results = lex(&amp;contents, Mode::Module);  // An iterator yielding Option(Tok).
    for result in results {
        use Tok::*;
        count += 1;
        let token = result.ok().unwrap();
        let (ref tok_class, tok_range) = token;
        let tok_value = &amp;contents[tok_range];

        // Variants names are necessary, but otherwise not used.
        #[allow(unused_variables)]
        let class_name = match tok_class {
            // Tokens with values...
            // Use tok_value for *all* values.
            Comment(value) =&gt; "Comment",  // No idea why parens are needed here.
            Complex { real, imag } =&gt; "Complex",
            Float { value } =&gt; "Float",
            Int { value } =&gt; "Int",
            Name { name } =&gt; "Name",
            Tok::String { value, kind, triple_quoted } =&gt; "String",
            
            // Common tokens...
            Class =&gt; "Class",
            Dedent =&gt; "Dedent",
            Def =&gt; "Def",
            Indent =&gt; "Indent",
            Newline =&gt; "Newline",
            NonLogicalNewline =&gt; "NonLogicalNewline",

            // All other tokens...
            Amper =&gt; "Amper",
            AmperEqual =&gt; "AmperEqual",
            And =&gt; "And",
            As =&gt; "As",
            Assert =&gt; "Assert",
            Async =&gt; "Async",
            At =&gt; "At",
            AtEqual =&gt; "AtEqual",
            Await =&gt; "Await",
            Break =&gt; "Break",
            Case =&gt; "Case",
            CircumFlex =&gt; "CircumFlex",
            CircumflexEqual =&gt; "CircumflexEqual",
            Colon =&gt; "Colon",
            ColonEqual =&gt; "ColonEqual",
            Comma =&gt; "Comma",
            Continue =&gt; "Continue",
            Del =&gt; "Del",
            Dot =&gt; "Dot",
            DoubleSlash =&gt; "DoubleSlash",
            DoubleSlashEqual =&gt; "DoubleSlashEqual",
            DoubleStar =&gt; "DoubleStar",
            DoubleStarEqual =&gt; "DoubleStarEqual",
            Elif =&gt; "Elif",
            Ellipsis =&gt; "Ellipsis",
            Else =&gt; "Else",
            EndOfFile =&gt; "EndOfFile",
            EqEqual =&gt; "EqEqual",
            Equal =&gt; "Equal",
            Except =&gt; "Except",
            False =&gt; "False",
            Finally =&gt; "Finally",
            For =&gt; "For",
            From =&gt; "From",
            Global =&gt; "Global",
            Greater =&gt; "Greater",
            GreaterEqual =&gt; "GreaterEqual",
            If =&gt; "If",
            Import =&gt; "Import",
            In =&gt; "In",
            Is =&gt; "Is",
            Lambda =&gt; "Lambda",
            Lbrace =&gt; "Lbrace",
            LeftShift =&gt; "LeftShift",
            LeftShiftEqual =&gt; "LeftShiftEqual",
            Less =&gt; "Less",
            LessEqual =&gt; "LessEqual",
            Lpar =&gt; "Lpar",
            Lsqb =&gt; "Lsqb",
            Match =&gt; "Match",
            Minus =&gt; "Minus",
            MinusEqual =&gt; "MinusEqual",
            None =&gt; "None",
            Nonlocal =&gt; "Nonlocal",
            Not =&gt; "Not",
            NotEqual =&gt; "NotEqual",
            Or =&gt; "Or",
            Pass =&gt; "Pass",
            Percent =&gt; "Percent",
            PercentEqual =&gt; "PercentEqual",
            Plus =&gt; "Plus",
            PlusEqual =&gt; "PlusEqual",
            Raise =&gt; "Raise",
            Rarrow =&gt; "Rarrow",
            Rbrace =&gt; "Rbrace",
            Return =&gt; "Return",
            RightShift =&gt; "RightShift",
            RightShiftEqual =&gt; "RightShiftEqual",
            Rpar =&gt; "Rpar",
            Rsqb =&gt; "Rsqb",
            Semi =&gt; "Semi",
            Slash =&gt; "Slash",
            SlashEqual =&gt; "SlashEqual",
            Star =&gt; "Star",
            StarEqual =&gt; "StarEqual",
            StartExpression =&gt; "StartExpression",
            StartInteractive =&gt; "StartInteractive",
            StartModule =&gt; "StartModule",
            Tilde =&gt; "Tilde",
            True =&gt; "True",
            Try =&gt; "Try",
            Type =&gt; "Type",
            Vbar =&gt; "Vbar",
            VbarEqual =&gt; "VbarEqual",
            While =&gt; "While",
            With =&gt; "With",
            Yield =&gt; "Yield",
        };
        self.add_input_token(class_name, tok_value);
    }
    return count;
}
</t>
<t tx="ekr.20240927163942.117">fn make_output_list(&amp;mut self) {

    //// Prototype only.
    for input_token in &amp;self.input_list.clone() {
        // println!("{:?}", input_token);
        self.add_output_string(input_token.kind.as_str(), input_token.value.as_str());
    }
}
</t>
<t tx="ekr.20240927163942.118">pub fn new() -&gt; Beautifier {
    let mut x = Beautifier {
        args: Vec::new(),
        files_list: Vec::new(),
        input_list: Vec::new(),
        output_list: Vec::new(),
    };
    x.get_args();
    return x;
}
</t>
<t tx="ekr.20240927163942.119">fn show_args (&amp;self) {
    println!("Command-line arguments...");
    for (i, arg) in self.args.iter().enumerate() {
        if i &gt; 0 {
            println!("  {arg}");
        }
    }
    for file_arg in self.files_list.iter() {
        println!("  {file_arg}");
    }
}
</t>
<t tx="ekr.20240927163942.12"></t>
<t tx="ekr.20240927163942.120">fn show_help (&amp;self) {
    //! Beautifier::show_help: print the help messages.
    println!("{}", textwrap::dedent("
        Beautify or diff files.

        -h --help:      Print this help message and exit.
        --all:          Beautify all files, even unchanged files.
        --beautified:   Report beautified files individually, even if not written.
        --diff:         Show diffs instead of changing files.
        --report:       Print summary report.
        --write:        Write beautifed files (dry-run mode otherwise).
    "));
}
</t>
<t tx="ekr.20240927163942.121">fn show_output_list (&amp;self) {
    println!("\nOutput list...");
    for (i, arg) in self.output_list.iter().enumerate() {
        if i &gt; 0 {
            print!("{:?}", arg);
        }
    }
}
</t>
<t tx="ekr.20240927163942.122">fn tokenize_contents(&amp;mut self, contents: String ) -&gt; u32 {

    let count = self.make_input_list(&amp;contents);
    
    // Simulate iterating the input list twice.
    for _z in &amp;self.input_list.clone() {
    }
    for _z in &amp;self.input_list.clone() {
    }

    // Simulate writing strings to the output list.
    // self.make_output_list();
    for _z in &amp;self.input_list.clone() {
    }
    return count;
}
</t>
<t tx="ekr.20240927163942.123">// Only Clone is valid for String.
#[derive(Clone)]
struct InputTok {
    kind: String,
    value: String,
}

impl fmt::Debug for InputTok {
    fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result {
        let kind_s = format!("{:?}", self.kind);
        let mut value = self.value.to_string();
        if true {
            return write!(f, "{value} ");
        }
        else {  // Debug format.
            value.truncate(60);
            // repr format is not useful.
            // let value_s = format!("{:?}", value);
            let value_s = format!("{}", value);
            return write!(f, "InputTok: {kind_s:&gt;10}: {value_s}");
        }
    }
}
</t>
<t tx="ekr.20240927163942.124">pub fn entry() {

    if false {
        tokenize();
        return;
    }
    // Main line of beautifier.
    let mut x = Beautifier::new();
    if x.enabled("--help") || x.enabled("-h") {
        x.show_help();
        return;
    }
    x.show_args();
    x.beautify_all_files();
}
</t>
<t tx="ekr.20240927163942.125">fn tokenize() {
    &lt;&lt; tokenize: define contents &gt;&gt;
    println!("fn tokenize");
    println!("\nSource:\n{contents}");

    for debug in [true, false].iter() {

        println!("{}", if *debug {"Tokens..."} else {"\nBeautified:"});

        let results = lex(contents, Mode::Module);  // An iterator yielding Option(Tok).
        let mut count = 0;
        let mut lws = String::new();
        for (i, result) in results.enumerate() {
            use Tok::*;
            let token = result.ok().unwrap();
            let (ref tok_class, tok_range) = token;
            let tok_value = &amp;contents[tok_range];

            if *debug {
                let s = format!("{tok_class}");
                print!("\nToken: {s:20} {:?}", tok_value);
            }
            else {
                // Comment(value), Name(name)
                #[allow(unused_variables)]
                match tok_class {
                    Comment(value) =&gt; {
                        // print!("{value} ");  // Wrong!
                        print!("{tok_value}");
                    },
                    Dedent =&gt; {
                        lws.pop();
                        lws.pop();
                        print!("{lws}");
                    },
                    Def =&gt; {
                        print!("{tok_value} ");
                    },
                    Indent =&gt; {
                        lws.push_str("    ");
                        print!("{lws}");
                    },
                    Name {name} =&gt; {
                        print!("{tok_value} ");
                    },
                    Newline =&gt; {
                        print!("{tok_value}");
                        print!("{lws}");
                        if false {  // old
                            println!("");
                            print!("{lws}");
                        }
                    },
                    NonLogicalNewline =&gt; {
                        println!("");
                        print!("{lws}");
                    },
                    Return =&gt; {
                        print!("{tok_value} ");
                    },
                    Tok::String {value, kind, triple_quoted} =&gt; {
                        // correct.
                        print!("{tok_value}");
                        if false {  // incorrect.
                            let quote = if *triple_quoted {"'''"} else {"'"};
                            print!("{:?}:{quote}{value}{quote}", kind);
                        }
                    },
                    _ =&gt; {
                        print!("{tok_value}");
                        if false {
                            // to_string quotes values!
                            let s = tok_class.to_string().replace("'", "");
                            print!("{s}");
                        }
                    },
                }
            }
            count = i
        }
        if *debug {
            println!("\n{count} tokens")
        }
    }
}
</t>
<t tx="ekr.20240927163942.126">let contents = r#"
def test():
# Comment 1.
print('abc')
# Comment 2.
"#;

// print("xyz")
// print(rf'pdb')
// print(fr'pdb2')
// return bool(i &amp; 1)
</t>
<t tx="ekr.20240927163942.127">#[allow(dead_code)]
fn print_type&lt;T&gt;(_: &amp;T, tag: &amp;str) {
    println!("{tag} type: {}", std::any::type_name::&lt;T&gt;())
}
</t>
<t tx="ekr.20240927163942.13">fn do_Comment(&amp;mut self, tok_value: &amp;str) {
    // print!("{tok_value}");  // Correct.
    // print!("{value} ");  // Wrong!
    self.add_output_string("Comment", tok_value);
}
</t>
<t tx="ekr.20240927163942.14">fn do_Complex(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Complex", tok_value);
}
</t>
<t tx="ekr.20240927163942.15">fn do_Float(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Float", tok_value);
}
</t>
<t tx="ekr.20240927163942.16">fn do_Int(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Int", tok_value);
}
</t>
<t tx="ekr.20240927163942.17">fn do_Name(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Name", tok_value);
}
</t>
<t tx="ekr.20240927163942.18">fn do_String(&amp;mut self, tok_value: &amp;str) {
    // correct.
    // print!("{tok_value}");
    
    // incorrect.
        // let quote = if *triple_quoted {"'''"} else {"'"};
        // print!("{:?}:{quote}{value}{quote}", kind);

    self.add_output_string("String", tok_value);
}
</t>
<t tx="ekr.20240927163942.19"></t>
<t tx="ekr.20240927163942.2">// All suppressions are local to a class or statement.

// #![allow(non_snake_case)]
// #![allow(unused_imports)]
// #![allow(unreachable_code)]
// #![allow(unused_mut)]
// #![allow(unused_variables)]
</t>
<t tx="ekr.20240927163942.20">fn do_Dedent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Dedent", tok_value);
}
</t>
<t tx="ekr.20240927163942.21">fn do_Indent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Indent", tok_value);
}
</t>
<t tx="ekr.20240927163942.22">fn do_Newline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240927163942.23">fn do_NonLogicalNewline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240927163942.24"></t>
<t tx="ekr.20240927163942.25">fn do_Amper(&amp;mut self) {
    self.add_output_string("Amper", "&amp;");
}
</t>
<t tx="ekr.20240927163942.26">fn do_AmperEqual(&amp;mut self) {
    self.add_output_string("AmperEqual", "&amp;=");
}
</t>
<t tx="ekr.20240927163942.27">fn do_And(&amp;mut self) {
    self.add_output_string("And", "and");
}
</t>
<t tx="ekr.20240927163942.28">fn do_As(&amp;mut self) {
    self.add_output_string("As", "as");
}
</t>
<t tx="ekr.20240927163942.29">fn do_Assert(&amp;mut self) {
    self.add_output_string("Assert", "assert");
}
</t>
<t tx="ekr.20240927163942.3">use rustpython_parser::{lexer::lex, Mode, Tok};

use std::env;
use std::fs;
use std::fmt;
// use std::time;

use textwrap;
</t>
<t tx="ekr.20240927163942.30">fn do_Async(&amp;mut self) {
    self.add_output_string("Async", "async");
}
</t>
<t tx="ekr.20240927163942.31">fn do_At(&amp;mut self) {
    self.add_output_string("At", "@");
}
</t>
<t tx="ekr.20240927163942.32">fn do_AtEqual(&amp;mut self) {
    self.add_output_string("AtEqual", "@=");
}
</t>
<t tx="ekr.20240927163942.33">fn do_Await(&amp;mut self) {
    self.add_output_string("Await", "await");
}
</t>
<t tx="ekr.20240927163942.34">fn do_Break(&amp;mut self) {
    self.add_output_string("Break", "break");
}
</t>
<t tx="ekr.20240927163942.35">fn do_Case(&amp;mut self) {
    self.add_output_string("Case", "case");
}
</t>
<t tx="ekr.20240927163942.36">fn do_CircumFlex(&amp;mut self) {
    self.add_output_string("CircumFlex", "^");
}
</t>
<t tx="ekr.20240927163942.37">fn do_CircumflexEqual(&amp;mut self) {
    self.add_output_string("CircumflexEqual", "^=");
}
</t>
<t tx="ekr.20240927163942.38">fn do_Class(&amp;mut self) {
    self.add_output_string("Class", "class");
}
</t>
<t tx="ekr.20240927163942.39">fn do_Colon(&amp;mut self) {
    self.add_output_string("Colon", ":");
}
</t>
<t tx="ekr.20240927163942.4"></t>
<t tx="ekr.20240927163942.40">fn do_ColonEqual(&amp;mut self) {
    self.add_output_string("ColonEqual", ":=");
}
</t>
<t tx="ekr.20240927163942.41">fn do_Comma(&amp;mut self) {
    self.add_output_string("Comma", ",");
}
</t>
<t tx="ekr.20240927163942.42">fn do_Continue(&amp;mut self) {
    self.add_output_string("Continue", "continue");
}
</t>
<t tx="ekr.20240927163942.43">fn do_Def(&amp;mut self) {
    self.add_output_string("Def", "def");
}
</t>
<t tx="ekr.20240927163942.44">fn do_Del(&amp;mut self) {
    self.add_output_string("Del", "del");
}
</t>
<t tx="ekr.20240927163942.45">fn do_Dot(&amp;mut self) {
    self.add_output_string("Dot", ".");
}
</t>
<t tx="ekr.20240927163942.46">fn do_DoubleSlash(&amp;mut self) {
    self.add_output_string("DoubleSlash", "//");
}
</t>
<t tx="ekr.20240927163942.47">fn do_DoubleSlashEqual(&amp;mut self) {
    self.add_output_string("DoubleSlashEqual", "//=");
}
</t>
<t tx="ekr.20240927163942.48">fn do_DoubleStar(&amp;mut self) {
    self.add_output_string("DoubleStar", "**");
}
</t>
<t tx="ekr.20240927163942.49">fn do_DoubleStarEqual(&amp;mut self) {
    self.add_output_string("DoubleStarEqual", "**=");
}
</t>
<t tx="ekr.20240927163942.5">#[derive(Debug)]
pub struct Beautifier {
    args: Vec&lt;String&gt;,
    files_list: Vec&lt;String&gt;,
    input_list: Vec&lt;InputTok&gt;,
    output_list: Vec&lt;String&gt;,
}

///// Temporary.
#[allow(dead_code)]
#[allow(non_snake_case)]
impl Beautifier {
    @others
}
</t>
<t tx="ekr.20240927163942.50">fn do_Elif(&amp;mut self) {
    self.add_output_string("Elif", "elif");
}
</t>
<t tx="ekr.20240927163942.51">fn do_Ellipsis(&amp;mut self) {
    self.add_output_string("Ellipsis", "...");
}
</t>
<t tx="ekr.20240927163942.52">fn do_Else(&amp;mut self) {
    self.add_output_string("Else", "else");
}
</t>
<t tx="ekr.20240927163942.53">fn do_EndOfFile(&amp;mut self) {
    self.add_output_string("EndOfFile", "EOF");
}
</t>
<t tx="ekr.20240927163942.54">fn do_EqEqual(&amp;mut self) {
    self.add_output_string("EqEqual", "==");
}
</t>
<t tx="ekr.20240927163942.55">fn do_Equal(&amp;mut self) {
    self.add_output_string("Equal", "=");
}
</t>
<t tx="ekr.20240927163942.56">fn do_Except(&amp;mut self) {
    self.add_output_string("Except", "except");
}
</t>
<t tx="ekr.20240927163942.57">fn do_False(&amp;mut self) {
    self.add_output_string("False", "False");
}
</t>
<t tx="ekr.20240927163942.58">fn do_Finally(&amp;mut self) {
    self.add_output_string("Finally", "finally");
}
</t>
<t tx="ekr.20240927163942.59">fn do_For(&amp;mut self) {
    self.add_output_string("For", "for");
}
</t>
<t tx="ekr.20240927163942.6">#[allow(unused_variables)]
fn add_output_string (&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add one string to the output list.
    self.output_list.push(value.to_string())
}
</t>
<t tx="ekr.20240927163942.60">fn do_From(&amp;mut self) {
    self.add_output_string("From", "from");
}
</t>
<t tx="ekr.20240927163942.61">fn do_Global(&amp;mut self) {
    self.add_output_string("Global", "global");
}
</t>
<t tx="ekr.20240927163942.62">fn do_Greater(&amp;mut self) {
    self.add_output_string("Greater", "&gt;");
}
</t>
<t tx="ekr.20240927163942.63">fn do_GreaterEqual(&amp;mut self) {
    self.add_output_string("GreaterEqual", "&gt;-");
}
</t>
<t tx="ekr.20240927163942.64">fn do_If(&amp;mut self) {
    self.add_output_string("If", "if");
}
</t>
<t tx="ekr.20240927163942.65">fn do_Import(&amp;mut self) {
    self.add_output_string("Import", "import");
}
</t>
<t tx="ekr.20240927163942.66">fn do_In(&amp;mut self) {
    self.add_output_string("In", "in");
}
</t>
<t tx="ekr.20240927163942.67">fn do_Is(&amp;mut self) {
    self.add_output_string("Is", "is");
}
</t>
<t tx="ekr.20240927163942.68">fn do_Lambda(&amp;mut self) {
    self.add_output_string("Lambda", "lambda");
}
</t>
<t tx="ekr.20240927163942.69">fn do_Lbrace(&amp;mut self) {
    self.add_output_string("Lbrace", "[");
}
</t>
<t tx="ekr.20240927163942.7">// #[allow(dead_code)]
fn add_input_token (&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    // println!("{:?}", kind);
    self.input_list.push(InputTok {
        kind: kind.to_string(),
        value: value.to_string(),
    });
}
</t>
<t tx="ekr.20240927163942.70">fn do_LeftShift(&amp;mut self) {
    self.add_output_string("LeftShift", "&lt;&lt;");
}
</t>
<t tx="ekr.20240927163942.71">fn do_LeftShiftEqual(&amp;mut self) {
    self.add_output_string("LeftShiftEqual", "&lt;&lt;=");
}
</t>
<t tx="ekr.20240927163942.72">fn do_Less(&amp;mut self) {
    self.add_output_string("Less", "&lt;");
}
</t>
<t tx="ekr.20240927163942.73">fn do_LessEqual(&amp;mut self) {
    self.add_output_string("LessEqual", "&lt;=");
}
</t>
<t tx="ekr.20240927163942.74">fn do_Lpar(&amp;mut self) {
    self.add_output_string("Lpar", "(");
}
</t>
<t tx="ekr.20240927163942.75">fn do_Lsqb(&amp;mut self) {
    self.add_output_string("Lsqb", "[");
}
</t>
<t tx="ekr.20240927163942.76">fn do_Match(&amp;mut self) {
    self.add_output_string("Match", "match");
}
</t>
<t tx="ekr.20240927163942.77">fn do_Minus(&amp;mut self) {
    self.add_output_string("Minus", "-");
}
</t>
<t tx="ekr.20240927163942.78">fn do_MinusEqual(&amp;mut self) {
    self.add_output_string("MinusEqual", "-=");
}
</t>
<t tx="ekr.20240927163942.79">fn do_None(&amp;mut self) {
    self.add_output_string("None", "None");
}
</t>
<t tx="ekr.20240927163942.8">pub fn beautify_all_files(&amp;mut self) {
    for file_name in self.files_list.clone() {
        self.beautify_one_file(file_name);
    }
}

</t>
<t tx="ekr.20240927163942.80">fn do_Nonlocal(&amp;mut self) {
    self.add_output_string("Nonlocal", "nonlocal");
}
</t>
<t tx="ekr.20240927163942.81">fn do_Not(&amp;mut self) {
    self.add_output_string("Not", "not");
}
</t>
<t tx="ekr.20240927163942.82">fn do_NotEqual(&amp;mut self) {
    self.add_output_string("NotEqual", "!=");
}
</t>
<t tx="ekr.20240927163942.83">fn do_Or(&amp;mut self) {
    self.add_output_string("Or", "or");
}
</t>
<t tx="ekr.20240927163942.84">fn do_Pass(&amp;mut self) {
    self.add_output_string("Pass", "pass");
}
</t>
<t tx="ekr.20240927163942.85">fn do_Percent(&amp;mut self) {
    self.add_output_string("Percent", "%");
}
</t>
<t tx="ekr.20240927163942.86">fn do_PercentEqual(&amp;mut self) {
    self.add_output_string("PercentEqual", "%=");
}
</t>
<t tx="ekr.20240927163942.87">fn do_Plus(&amp;mut self) {
    self.add_output_string("Plus", "+");
}
</t>
<t tx="ekr.20240927163942.88">fn do_PlusEqual(&amp;mut self) {
    self.add_output_string("PlusEqual", "+=");
}
</t>
<t tx="ekr.20240927163942.89">fn do_Raise(&amp;mut self) {
    self.add_output_string("Raise", "raise");
}
</t>
<t tx="ekr.20240927163942.9">fn beautify_one_file(&amp;mut self, file_name: String) {
    // println!("beautifiy_one_file: {file_name}");
    self.output_list = Vec::new();
    // Read the file into contents (a String).
    let t1 = std::time::Instant::now();
    let contents = fs::read_to_string(file_name.clone())
        .expect("Error reading{file_name}");
    // print_type(&amp;contents, "contents");
    let t2 = t1.elapsed();
    // Tokenize.
    let t3 = std::time::Instant::now();
    let n_tokens = self.tokenize_contents(contents);
    let t4 = t3.elapsed();
    // Report
    if self.enabled("--report") {
        println!(" file name: {file_name}");
        println!("      read: {:.2?}", t2);
        println!("  tokenize: {:.2?}", t4);
        println!("    tokens: {n_tokens}");
    }
    // Show tokens.
    &lt;&lt; show output_list &gt;&gt;
}
</t>
<t tx="ekr.20240927163942.90">fn do_Rarrow(&amp;mut self) {
    self.add_output_string("Rarrow", "-&gt;");
}
</t>
<t tx="ekr.20240927163942.91">fn do_Rbrace(&amp;mut self) {
    self.add_output_string("Rbrace", "]");
}
</t>
<t tx="ekr.20240927163942.92">fn do_Return(&amp;mut self) {
    self.add_output_string("Return", "return");
}
</t>
<t tx="ekr.20240927163942.93">fn do_RightShift(&amp;mut self) {
    self.add_output_string("RightShift", "&gt;&gt;");
}
</t>
<t tx="ekr.20240927163942.94">fn do_RightShiftEqual(&amp;mut self) {
    self.add_output_string("RightShiftEqual", "&gt;&gt;=");
}
</t>
<t tx="ekr.20240927163942.95">fn do_Rpar(&amp;mut self) {
    self.add_output_string("Rpar", ")");
}
</t>
<t tx="ekr.20240927163942.96">fn do_Rsqb(&amp;mut self) {
    self.add_output_string("Rsqb", "]");
}
</t>
<t tx="ekr.20240927163942.97">fn do_Semi(&amp;mut self) {
    self.add_output_string("Semi", ";");
}
</t>
<t tx="ekr.20240927163942.98">fn do_Slash(&amp;mut self) {
    self.add_output_string("Slash", "/");
}
</t>
<t tx="ekr.20240927163942.99">fn do_SlashEqual(&amp;mut self) {
    self.add_output_string("SlashEqual", "/=");
}
</t>
<t tx="ekr.20240927164013.1"></t>
<t tx="ekr.20240928073118.1">@language python
g.cls()
import os
import subprocess

if c.changed:
    c.save()
command = 'cargo fmt'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240928185643.1">@language rest
@wrap

Python (with extra tracing code in tbo.init_tokens_from_file:

&gt; python -c "import leo.core.leoTokens" --all --report leo\core\leoFrame.py
tbo: 0.03 sec. dirty: 0   checked: 1   beautified: 0   in leo\core\leoFrame.py

       read:   0.28 ms
make_tokens:  29.45 ms
      total:  29.73 ms
      
Rust, with nanosecond resolution.

 leoFrame.py

     files: 1, tokens: 14619, ws tokens: 5156
       read:    0.458 ms  Slower than python, but maybe not significant.
make_tokens:   14.196 ms
      write:    0.000 ms
      total:   14.654 ms
      
Thats about 0.1 sec / file.

So tbo-in-rust twice as fast, but without any formatting!
</t>
<t tx="ekr.20240929024648.113">fn make_input_list(
    contents: &amp;str,
    input_list: &amp;mut Vec&lt;InputTok&gt;,
) -&gt; (usize, usize) {
    // Add InputToks to the input_list for every token given by the RustPython lex.
    // The gem: Generate "ws" pseudo-tokens for all whitespace.
    let mut tokens_n: usize = 0;
    let mut ws_tokens_n: usize = 0;
    let mut prev_start: usize = 0;
    for token_tuple in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        use Tok::*;
        tokens_n += 1;
        let (token, range) = token_tuple;
        let tok_value = &amp;contents[range];
        let start_i: usize = usize::from(range.start());
        let end_i: usize = usize::from(range.end());
        
        // The gem: create a whitespace pseudo-tokens.
        if start_i &gt; prev_start {
            let ws = &amp;contents[prev_start..start_i];
            add_input_token(input_list, "ws", ws);
            ws_tokens_n += 1
        }
        prev_start = end_i;

        // Variants names are necessary, but otherwise not used.
        #[allow(unused_variables)]
        
        let class_name = match token {
            // Tokens with values...
            // Use tok_value for *all* values.
            Comment(value) =&gt; "Comment",  // No idea why parens are needed here.
            Complex { real, imag } =&gt; "Complex",
            Float { value } =&gt; "Float",
            Int { value } =&gt; "Int",
            Name { name } =&gt; "Name",
            Tok::String { value, kind, triple_quoted } =&gt; "String",
            
            // Common tokens...
            Class =&gt; "Class",
            Dedent =&gt; "Dedent",
            Def =&gt; "Def",
            Indent =&gt; "Indent",
            Newline =&gt; "Newline",
            NonLogicalNewline =&gt; "NonLogicalNewline",

            // All other tokens...
            Amper =&gt; "Amper",
            AmperEqual =&gt; "AmperEqual",
            And =&gt; "And",
            As =&gt; "As",
            Assert =&gt; "Assert",
            Async =&gt; "Async",
            At =&gt; "At",
            AtEqual =&gt; "AtEqual",
            Await =&gt; "Await",
            Break =&gt; "Break",
            Case =&gt; "Case",
            CircumFlex =&gt; "CircumFlex",
            CircumflexEqual =&gt; "CircumflexEqual",
            Colon =&gt; "Colon",
            ColonEqual =&gt; "ColonEqual",
            Comma =&gt; "Comma",
            Continue =&gt; "Continue",
            Del =&gt; "Del",
            Dot =&gt; "Dot",
            DoubleSlash =&gt; "DoubleSlash",
            DoubleSlashEqual =&gt; "DoubleSlashEqual",
            DoubleStar =&gt; "DoubleStar",
            DoubleStarEqual =&gt; "DoubleStarEqual",
            Elif =&gt; "Elif",
            Ellipsis =&gt; "Ellipsis",
            Else =&gt; "Else",
            EndOfFile =&gt; "EndOfFile",
            EqEqual =&gt; "EqEqual",
            Equal =&gt; "Equal",
            Except =&gt; "Except",
            False =&gt; "False",
            Finally =&gt; "Finally",
            For =&gt; "For",
            From =&gt; "From",
            Global =&gt; "Global",
            Greater =&gt; "Greater",
            GreaterEqual =&gt; "GreaterEqual",
            If =&gt; "If",
            Import =&gt; "Import",
            In =&gt; "In",
            Is =&gt; "Is",
            Lambda =&gt; "Lambda",
            Lbrace =&gt; "Lbrace",
            LeftShift =&gt; "LeftShift",
            LeftShiftEqual =&gt; "LeftShiftEqual",
            Less =&gt; "Less",
            LessEqual =&gt; "LessEqual",
            Lpar =&gt; "Lpar",
            Lsqb =&gt; "Lsqb",
            Match =&gt; "Match",
            Minus =&gt; "Minus",
            MinusEqual =&gt; "MinusEqual",
            None =&gt; "None",
            Nonlocal =&gt; "Nonlocal",
            Not =&gt; "Not",
            NotEqual =&gt; "NotEqual",
            Or =&gt; "Or",
            Pass =&gt; "Pass",
            Percent =&gt; "Percent",
            PercentEqual =&gt; "PercentEqual",
            Plus =&gt; "Plus",
            PlusEqual =&gt; "PlusEqual",
            Raise =&gt; "Raise",
            Rarrow =&gt; "Rarrow",
            Rbrace =&gt; "Rbrace",
            Return =&gt; "Return",
            RightShift =&gt; "RightShift",
            RightShiftEqual =&gt; "RightShiftEqual",
            Rpar =&gt; "Rpar",
            Rsqb =&gt; "Rsqb",
            Semi =&gt; "Semi",
            Slash =&gt; "Slash",
            SlashEqual =&gt; "SlashEqual",
            Star =&gt; "Star",
            StarEqual =&gt; "StarEqual",
            StartExpression =&gt; "StartExpression",
            StartInteractive =&gt; "StartInteractive",
            StartModule =&gt; "StartModule",
            Tilde =&gt; "Tilde",
            True =&gt; "True",
            Try =&gt; "Try",
            Type =&gt; "Type",
            Vbar =&gt; "Vbar",
            VbarEqual =&gt; "VbarEqual",
            While =&gt; "While",
            With =&gt; "With",
            Yield =&gt; "Yield",
        };
        // add_input_token(&amp;mut input_list, class_name, tok_value);
        add_input_token(input_list, class_name, tok_value);
    }
    return (tokens_n, ws_tokens_n);
}
</t>
<t tx="ekr.20240929024648.120">// Only Clone is valid for String.
#[derive(Clone)]
struct InputTok {
    kind: String,
    value: String,
}

impl fmt::Debug for InputTok {
    fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result {
        let kind_s = format!("{:?}", self.kind);
        let mut value = self.value.to_string();
        if true {
            return write!(f, "{value} ");
        }
        else {  // Debug format.
            value.truncate(60);
            // repr format is not useful.
            // let value_s = format!("{:?}", value);
            let value_s = format!("{}", value);
            return write!(f, "InputTok: {kind_s:&gt;10}: {value_s}");
        }
    }
}
</t>
<t tx="ekr.20240929031635.1">fn scan_input_list(contents: String, tokens: Vec&lt;(Tok, TextRange)&gt;) -&gt; usize {

    let mut count: usize = 0;
    for (token, range) in tokens {
        // Range is a TextRange.
        count += 1;
        // To do: Find gaps in the ranges.
        let start_i = usize::from(range.start());
        let end_i = usize::from(range.end());
        if false {
            if count &lt; 20 {
                println!("{start_i:&gt;3}..{end_i:3} token: {token:?}");
            }
        }
    }
    return count;
}
</t>
<t tx="ekr.20240929032636.1">pub fn entry() {
    // leoFrame.py is a typical size
    let file_path = "C:\\Repos\\leo-editor\\leo\\core\\leoFrame.py";
    let short_file_name = "leoFrame.py";
    &lt;&lt; 1: read &gt;&gt;
    &lt;&lt; 2: Make input_list &gt;&gt;
    &lt;&lt; 3: print stats &gt;&gt;
}
</t>
<t tx="ekr.20240929032710.1">fn fmt_ms(t: u128) -&gt; String {
    //! Convert microseconds to fractional milliseconds.
    let ms = t / 1000;
    let micro = (t % 1000) / 10;
    return f!("{ms}.{micro:02}");  // Two-digits for fraction.
}

</t>
<t tx="ekr.20240929033044.1">fn add_input_token (input_list: &amp;mut Vec&lt;InputTok&gt;, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    let new_tok = InputTok {
        kind: kind.to_string(),
        value: value.to_string()
    };
    input_list.push(new_tok);
}
</t>
<t tx="ekr.20240929074037.1">#[derive(Debug)]
pub struct Beautifier {
    // Set in LB:beautify_one_file...
    args: Vec&lt;String&gt;,
    files_list: Vec&lt;String&gt;,
    input_list: Vec&lt;InputTok&gt;,
    output_list: Vec&lt;String&gt;,
    stats: Stats,
    // Set in LB:beautify...
    // Debugging
    line_number: i32,  // Use -1 instead of None?
    // State vars for whitespace.
    curly_brackets_level: i32,
    indent_level: i32,
    paren_level: i32,
    square_brackets_stack: Vec&lt;bool&gt;,
    // Parse state.
    decorator_seen: bool,  // Set by do_name for do_op.
    in_arg_list: i32, // &gt; 0 if in an arg list of a def.
    in_doc_part: bool,
    // To do
    // state_stack = Vec&lt;ParseState&gt;,  // list[ParseState] = []  # Stack of ParseState objects.
    // Leo-related state.
    verbatim: bool,
    // Ivars describing the present input token.
    index: u32,
    lws: String,
}

///// Temporary.
#[allow(dead_code)]
#[allow(non_snake_case)]
impl Beautifier {
    @others
}
</t>
<t tx="ekr.20240929074037.10">fn do_Complex(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Complex", tok_value);
}
</t>
<t tx="ekr.20240929074037.100">fn do_StartModule(&amp;mut self) {
    // self.add_output_string("StartModule", "");
    println!("do_StartModule");
}
</t>
<t tx="ekr.20240929074037.101">fn do_Tilde(&amp;mut self) {
    self.add_output_string("Tilde", "~");
}
</t>
<t tx="ekr.20240929074037.102">fn do_True(&amp;mut self) {
    self.add_output_string("True", "True");
}
</t>
<t tx="ekr.20240929074037.103">fn do_Try(&amp;mut self) {
    self.add_output_string("Try", "try");
}
</t>
<t tx="ekr.20240929074037.104">fn do_Type(&amp;mut self) {
    self.add_output_string("Type", "type");
}
</t>
<t tx="ekr.20240929074037.105">fn do_Vbar(&amp;mut self) {
    self.add_output_string("Vbar", "|");
}
</t>
<t tx="ekr.20240929074037.106">fn do_VbarEqual(&amp;mut self) {
    self.add_output_string("VbarEqual", "|=");
}
</t>
<t tx="ekr.20240929074037.107">fn do_While(&amp;mut self) {
    self.add_output_string("While", "while");
}
</t>
<t tx="ekr.20240929074037.108">fn do_With(&amp;mut self) {
    self.add_output_string("With", "with");
}
</t>
<t tx="ekr.20240929074037.109">fn do_Yield(&amp;mut self) {
    self.add_output_string("Yield", "yield");
}
</t>
<t tx="ekr.20240929074037.11">fn do_Float(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Float", tok_value);
}
</t>
<t tx="ekr.20240929074037.110">fn enabled(&amp;self, arg: &amp;str) -&gt; bool {
    //! Beautifier::enabled: return true if the given command-line argument is enabled.
    //! Example:  x.enabled("--report");
    return self.args.contains(&amp;arg.to_string());
}
</t>
<t tx="ekr.20240929074037.111">fn get_args(&amp;mut self) {
    //! Beautifier::get_args: Set the args and files_list ivars.
    let args: Vec&lt;String&gt; = env::args().collect();
    let valid_args = vec![
        "--all", 
        "--beautified",
        "--diff",
        "-h", "--help",
        "--report",
        "--write",
    ];
    for (i, arg) in args.iter().enumerate() {
        if i &gt; 0 {
            if valid_args.contains(&amp;arg.as_str()) {
                self.args.push(arg.to_string())
            }
            else if 
                arg.as_str().starts_with("--") ||
                arg.as_str().starts_with("--")
            {
                println!("Ignoring invalid arg: {arg}");
            }
            else {
                println!("File: {arg}");
                self.files_list.push(arg.to_string());
            }
        }
    }
}
</t>
<t tx="ekr.20240929074037.112">fn make_input_list(&amp;mut self, contents: &amp;str) {
    // Add InputToks to the input_list for every token given by the RustPython lex.
    let mut n_tokens: u64 = 0;
    let mut n_ws_tokens: u64 = 0;
    let mut prev_start: usize = 0;
    for token_tuple in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        use Tok::*;
        n_tokens += 1;
        let (token, range) = token_tuple;
        let tok_value = &amp;contents[range];
        let start_i = usize::from(range.start());
        let end_i = usize::from(range.end());
        
        // The gem: create a whitespace pseudo-tokens.
        if start_i &gt; prev_start {
            let ws = &amp;contents[prev_start..start_i];
            self.add_input_token("ws", ws);
            n_ws_tokens += 1
        }
        prev_start = end_i;

        // Variants names are necessary, but otherwise not used.
        #[allow(unused_variables)]
        let class_name = match token {
            // Tokens with values...
            // Use tok_value for *all* values.
            Comment(value) =&gt; "Comment",  // No idea why parens are needed here.
            Complex { real, imag } =&gt; "Complex",
            Float { value } =&gt; "Float",
            Int { value } =&gt; "Int",
            Name { name } =&gt; "Name",
            Tok::String { value, kind, triple_quoted } =&gt; "String",
            
            // Common tokens...
            Class =&gt; "Class",
            Dedent =&gt; "Dedent",
            Def =&gt; "Def",
            Indent =&gt; "Indent",
            Newline =&gt; "Newline",
            NonLogicalNewline =&gt; "NonLogicalNewline",

            // All other tokens...
            Amper =&gt; "Amper",
            AmperEqual =&gt; "AmperEqual",
            And =&gt; "And",
            As =&gt; "As",
            Assert =&gt; "Assert",
            Async =&gt; "Async",
            At =&gt; "At",
            AtEqual =&gt; "AtEqual",
            Await =&gt; "Await",
            Break =&gt; "Break",
            Case =&gt; "Case",
            CircumFlex =&gt; "CircumFlex",
            CircumflexEqual =&gt; "CircumflexEqual",
            Colon =&gt; "Colon",
            ColonEqual =&gt; "ColonEqual",
            Comma =&gt; "Comma",
            Continue =&gt; "Continue",
            Del =&gt; "Del",
            Dot =&gt; "Dot",
            DoubleSlash =&gt; "DoubleSlash",
            DoubleSlashEqual =&gt; "DoubleSlashEqual",
            DoubleStar =&gt; "DoubleStar",
            DoubleStarEqual =&gt; "DoubleStarEqual",
            Elif =&gt; "Elif",
            Ellipsis =&gt; "Ellipsis",
            Else =&gt; "Else",
            EndOfFile =&gt; "EndOfFile",
            EqEqual =&gt; "EqEqual",
            Equal =&gt; "Equal",
            Except =&gt; "Except",
            False =&gt; "False",
            Finally =&gt; "Finally",
            For =&gt; "For",
            From =&gt; "From",
            Global =&gt; "Global",
            Greater =&gt; "Greater",
            GreaterEqual =&gt; "GreaterEqual",
            If =&gt; "If",
            Import =&gt; "Import",
            In =&gt; "In",
            Is =&gt; "Is",
            Lambda =&gt; "Lambda",
            Lbrace =&gt; "Lbrace",
            LeftShift =&gt; "LeftShift",
            LeftShiftEqual =&gt; "LeftShiftEqual",
            Less =&gt; "Less",
            LessEqual =&gt; "LessEqual",
            Lpar =&gt; "Lpar",
            Lsqb =&gt; "Lsqb",
            Match =&gt; "Match",
            Minus =&gt; "Minus",
            MinusEqual =&gt; "MinusEqual",
            None =&gt; "None",
            Nonlocal =&gt; "Nonlocal",
            Not =&gt; "Not",
            NotEqual =&gt; "NotEqual",
            Or =&gt; "Or",
            Pass =&gt; "Pass",
            Percent =&gt; "Percent",
            PercentEqual =&gt; "PercentEqual",
            Plus =&gt; "Plus",
            PlusEqual =&gt; "PlusEqual",
            Raise =&gt; "Raise",
            Rarrow =&gt; "Rarrow",
            Rbrace =&gt; "Rbrace",
            Return =&gt; "Return",
            RightShift =&gt; "RightShift",
            RightShiftEqual =&gt; "RightShiftEqual",
            Rpar =&gt; "Rpar",
            Rsqb =&gt; "Rsqb",
            Semi =&gt; "Semi",
            Slash =&gt; "Slash",
            SlashEqual =&gt; "SlashEqual",
            Star =&gt; "Star",
            StarEqual =&gt; "StarEqual",
            StartExpression =&gt; "StartExpression",
            StartInteractive =&gt; "StartInteractive",
            StartModule =&gt; "StartModule",
            Tilde =&gt; "Tilde",
            True =&gt; "True",
            Try =&gt; "Try",
            Type =&gt; "Type",
            Vbar =&gt; "Vbar",
            VbarEqual =&gt; "VbarEqual",
            While =&gt; "While",
            With =&gt; "With",
            Yield =&gt; "Yield",
        };
        self.add_input_token(class_name, tok_value);
    }
    // Update counts.
    self.stats.n_tokens += n_tokens;
    self.stats.n_ws_tokens += n_ws_tokens;
}
</t>
<t tx="ekr.20240929074037.113">fn beautify(&amp;mut self) {
    //! Beautify the input_tokens, creating the output_list.
    &lt;&lt; LB::beautify: init ivars &gt;&gt;
    for input_token in &amp;self.input_list {
        // println!("{:?}", input_token);
        // let value = input_token.value.as_str();
        if true {  // All these work.
            self.output_list.push(input_token.value.to_string());  // Converts str to String
            self.output_list.push("value".to_string());  // Converts str to String
        } else {  // All these FAIL.
            // self.add_output_string(&amp;"Test", &amp;"Value");  // mutable borrow occurs here.
            // self.output_list.push(&amp;input_token.value); //  expected `String`, found `&amp;String`
            // self.output_list.push("Value");  // expected `String`, found `&amp;str`
            // self.output_list.push(&amp;"Value");  // expected `String`, found `&amp;&amp;str`
        }
    }
}
</t>
<t tx="ekr.20240929074037.114">pub fn new() -&gt; Beautifier {
    let mut x = Beautifier {
        // Set in beautify_one_file
        args: Vec::new(),
        files_list: Vec::new(),
        input_list: Vec::new(),
        output_list: Vec::new(),
        stats: Stats::new(),
        // Set in LB::beautify.
        // state_stack = Vec&lt;ParseState&gt;,  // list[ParseState] = []  # Stack of ParseState objects.
        curly_brackets_level: 0,
        decorator_seen: false,
        in_arg_list: 0,
        in_doc_part: false,
        indent_level: 0,
        index: 0,
        line_number: 0,
        lws: String::new(),
        paren_level: 0,
        square_brackets_stack: Vec::new(),
        verbatim: false,
    };
    x.get_args();
    return x;
}
</t>
<t tx="ekr.20240929074037.115">fn show_args (&amp;self) {
    println!("Command-line arguments...");
    for (i, arg) in self.args.iter().enumerate() {
        if i &gt; 0 {
            println!("  {arg}");
        }
    }
    for file_arg in self.files_list.iter() {
        println!("  {file_arg}");
    }
}
</t>
<t tx="ekr.20240929074037.116">fn show_help (&amp;self) {
    //! Beautifier::show_help: print the help messages.
    println!("{}", textwrap::dedent("
        Beautify or diff files.

        -h --help:      Print this help message and exit.
        --all:          Beautify all files, even unchanged files.
        --beautified:   Report beautified files individually, even if not written.
        --diff:         Show diffs instead of changing files.
        --report:       Print summary report.
        --write:        Write beautifed files (dry-run mode otherwise).
    "));
}
</t>
<t tx="ekr.20240929074037.117">fn show_output_list (&amp;self) {
    println!("\nOutput list...");
    for (i, arg) in self.output_list.iter().enumerate() {
        if i &gt; 0 {
            print!("{:?}", arg);
        }
    }
}
</t>
<t tx="ekr.20240929074037.12">fn do_Int(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Int", tok_value);
}
</t>
<t tx="ekr.20240929074037.13">fn do_Name(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Name", tok_value);
}
</t>
<t tx="ekr.20240929074037.14">fn do_String(&amp;mut self, tok_value: &amp;str) {
    // correct.
    // print!("{tok_value}");
    
    // incorrect.
        // let quote = if *triple_quoted {"'''"} else {"'"};
        // print!("{:?}:{quote}{value}{quote}", kind);

    self.add_output_string("String", tok_value);
}
</t>
<t tx="ekr.20240929074037.15"></t>
<t tx="ekr.20240929074037.16">fn do_Dedent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Dedent", tok_value);
}
</t>
<t tx="ekr.20240929074037.17">fn do_Indent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Indent", tok_value);
}
</t>
<t tx="ekr.20240929074037.18">fn do_Newline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240929074037.19">fn do_NonLogicalNewline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240929074037.2">#[allow(unused_variables)]
fn add_output_string (&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add one string to the output list.
    if !value.is_empty() {
        self.output_list.push(value.to_string())
    }
}
</t>
<t tx="ekr.20240929074037.20"></t>
<t tx="ekr.20240929074037.21">fn do_Amper(&amp;mut self) {
    self.add_output_string("Amper", "&amp;");
}
</t>
<t tx="ekr.20240929074037.22">fn do_AmperEqual(&amp;mut self) {
    self.add_output_string("AmperEqual", "&amp;=");
}
</t>
<t tx="ekr.20240929074037.23">fn do_And(&amp;mut self) {
    self.add_output_string("And", "and");
}
</t>
<t tx="ekr.20240929074037.24">fn do_As(&amp;mut self) {
    self.add_output_string("As", "as");
}
</t>
<t tx="ekr.20240929074037.25">fn do_Assert(&amp;mut self) {
    self.add_output_string("Assert", "assert");
}
</t>
<t tx="ekr.20240929074037.26">fn do_Async(&amp;mut self) {
    self.add_output_string("Async", "async");
}
</t>
<t tx="ekr.20240929074037.27">fn do_At(&amp;mut self) {
    self.add_output_string("At", "@");
}
</t>
<t tx="ekr.20240929074037.28">fn do_AtEqual(&amp;mut self) {
    self.add_output_string("AtEqual", "@=");
}
</t>
<t tx="ekr.20240929074037.29">fn do_Await(&amp;mut self) {
    self.add_output_string("Await", "await");
}
</t>
<t tx="ekr.20240929074037.3">// #[allow(dead_code)]
fn add_input_token (&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    self.input_list.push(InputTok {
        kind: kind.to_string(),
        value: value.to_string(),
    });
}
</t>
<t tx="ekr.20240929074037.30">fn do_Break(&amp;mut self) {
    self.add_output_string("Break", "break");
}
</t>
<t tx="ekr.20240929074037.31">fn do_Case(&amp;mut self) {
    self.add_output_string("Case", "case");
}
</t>
<t tx="ekr.20240929074037.32">fn do_CircumFlex(&amp;mut self) {
    self.add_output_string("CircumFlex", "^");
}
</t>
<t tx="ekr.20240929074037.33">fn do_CircumflexEqual(&amp;mut self) {
    self.add_output_string("CircumflexEqual", "^=");
}
</t>
<t tx="ekr.20240929074037.34">fn do_Class(&amp;mut self) {
    self.add_output_string("Class", "class");
}
</t>
<t tx="ekr.20240929074037.35">fn do_Colon(&amp;mut self) {
    self.add_output_string("Colon", ":");
}
</t>
<t tx="ekr.20240929074037.36">fn do_ColonEqual(&amp;mut self) {
    self.add_output_string("ColonEqual", ":=");
}
</t>
<t tx="ekr.20240929074037.37">fn do_Comma(&amp;mut self) {
    self.add_output_string("Comma", ",");
}
</t>
<t tx="ekr.20240929074037.38">fn do_Continue(&amp;mut self) {
    self.add_output_string("Continue", "continue");
}
</t>
<t tx="ekr.20240929074037.39">fn do_Def(&amp;mut self) {
    self.add_output_string("Def", "def");
}
</t>
<t tx="ekr.20240929074037.4">pub fn beautify_all_files(&amp;mut self) {
    // for file_name in self.files_list.clone() {
    for file_name in self.files_list.clone() {
        self.beautify_one_file(&amp;file_name);
    }
}

</t>
<t tx="ekr.20240929074037.40">fn do_Del(&amp;mut self) {
    self.add_output_string("Del", "del");
}
</t>
<t tx="ekr.20240929074037.41">fn do_Dot(&amp;mut self) {
    self.add_output_string("Dot", ".");
}
</t>
<t tx="ekr.20240929074037.42">fn do_DoubleSlash(&amp;mut self) {
    self.add_output_string("DoubleSlash", "//");
}
</t>
<t tx="ekr.20240929074037.43">fn do_DoubleSlashEqual(&amp;mut self) {
    self.add_output_string("DoubleSlashEqual", "//=");
}
</t>
<t tx="ekr.20240929074037.44">fn do_DoubleStar(&amp;mut self) {
    self.add_output_string("DoubleStar", "**");
}
</t>
<t tx="ekr.20240929074037.45">fn do_DoubleStarEqual(&amp;mut self) {
    self.add_output_string("DoubleStarEqual", "**=");
}
</t>
<t tx="ekr.20240929074037.46">fn do_Elif(&amp;mut self) {
    self.add_output_string("Elif", "elif");
}
</t>
<t tx="ekr.20240929074037.47">fn do_Ellipsis(&amp;mut self) {
    self.add_output_string("Ellipsis", "...");
}
</t>
<t tx="ekr.20240929074037.48">fn do_Else(&amp;mut self) {
    self.add_output_string("Else", "else");
}
</t>
<t tx="ekr.20240929074037.49">fn do_EndOfFile(&amp;mut self) {
    self.add_output_string("EndOfFile", "EOF");
}
</t>
<t tx="ekr.20240929074037.5">fn beautify_one_file(&amp;mut self, file_name: &amp;str) {
    // Compute short_file_name from file_name.
    if true {  // Testing only.
        let file_path = path::Path::new(file_name);
        let os_str = file_path.file_name().unwrap(); // &amp;OsStr
        let short_file_name = os_str.to_str().unwrap();
        println!("{short_file_name}");
    }
    // Read the file into contents (a String).
    self.output_list = Vec::new();
    let t1 = std::time::Instant::now();
    let contents = fs::read_to_string(file_name)
        .expect("Error reading{file_name}");
    // print_type(&amp;contents, "contents");
    let read_time = t1.elapsed().as_nanos();
    // Make the list of input tokens
    let t3 = std::time::Instant::now();
    self.make_input_list(&amp;contents);
    let make_tokens_time = t3.elapsed().as_nanos();
    // Beautify.
    let t4 = std::time::Instant::now();
    self.beautify();
    let beautify_time = t4.elapsed().as_nanos();
    // Update stats.
    self.stats.n_files += 1;
    let write_time = 0;
    self.stats.update_times(beautify_time, make_tokens_time, read_time, write_time);
}
</t>
<t tx="ekr.20240929074037.50">fn do_EqEqual(&amp;mut self) {
    self.add_output_string("EqEqual", "==");
}
</t>
<t tx="ekr.20240929074037.51">fn do_Equal(&amp;mut self) {
    self.add_output_string("Equal", "=");
}
</t>
<t tx="ekr.20240929074037.52">fn do_Except(&amp;mut self) {
    self.add_output_string("Except", "except");
}
</t>
<t tx="ekr.20240929074037.53">fn do_False(&amp;mut self) {
    self.add_output_string("False", "False");
}
</t>
<t tx="ekr.20240929074037.54">fn do_Finally(&amp;mut self) {
    self.add_output_string("Finally", "finally");
}
</t>
<t tx="ekr.20240929074037.55">fn do_For(&amp;mut self) {
    self.add_output_string("For", "for");
}
</t>
<t tx="ekr.20240929074037.56">fn do_From(&amp;mut self) {
    self.add_output_string("From", "from");
}
</t>
<t tx="ekr.20240929074037.57">fn do_Global(&amp;mut self) {
    self.add_output_string("Global", "global");
}
</t>
<t tx="ekr.20240929074037.58">fn do_Greater(&amp;mut self) {
    self.add_output_string("Greater", "&gt;");
}
</t>
<t tx="ekr.20240929074037.59">fn do_GreaterEqual(&amp;mut self) {
    self.add_output_string("GreaterEqual", "&gt;-");
}
</t>
<t tx="ekr.20240929074037.60">fn do_If(&amp;mut self) {
    self.add_output_string("If", "if");
}
</t>
<t tx="ekr.20240929074037.61">fn do_Import(&amp;mut self) {
    self.add_output_string("Import", "import");
}
</t>
<t tx="ekr.20240929074037.62">fn do_In(&amp;mut self) {
    self.add_output_string("In", "in");
}
</t>
<t tx="ekr.20240929074037.63">fn do_Is(&amp;mut self) {
    self.add_output_string("Is", "is");
}
</t>
<t tx="ekr.20240929074037.64">fn do_Lambda(&amp;mut self) {
    self.add_output_string("Lambda", "lambda");
}
</t>
<t tx="ekr.20240929074037.65">fn do_Lbrace(&amp;mut self) {
    self.add_output_string("Lbrace", "[");
}
</t>
<t tx="ekr.20240929074037.66">fn do_LeftShift(&amp;mut self) {
    self.add_output_string("LeftShift", "&lt;&lt;");
}
</t>
<t tx="ekr.20240929074037.67">fn do_LeftShiftEqual(&amp;mut self) {
    self.add_output_string("LeftShiftEqual", "&lt;&lt;=");
}
</t>
<t tx="ekr.20240929074037.68">fn do_Less(&amp;mut self) {
    self.add_output_string("Less", "&lt;");
}
</t>
<t tx="ekr.20240929074037.69">fn do_LessEqual(&amp;mut self) {
    self.add_output_string("LessEqual", "&lt;=");
}
</t>
<t tx="ekr.20240929074037.7"></t>
<t tx="ekr.20240929074037.70">fn do_Lpar(&amp;mut self) {
    self.add_output_string("Lpar", "(");
}
</t>
<t tx="ekr.20240929074037.71">fn do_Lsqb(&amp;mut self) {
    self.add_output_string("Lsqb", "[");
}
</t>
<t tx="ekr.20240929074037.72">fn do_Match(&amp;mut self) {
    self.add_output_string("Match", "match");
}
</t>
<t tx="ekr.20240929074037.73">fn do_Minus(&amp;mut self) {
    self.add_output_string("Minus", "-");
}
</t>
<t tx="ekr.20240929074037.74">fn do_MinusEqual(&amp;mut self) {
    self.add_output_string("MinusEqual", "-=");
}
</t>
<t tx="ekr.20240929074037.75">fn do_None(&amp;mut self) {
    self.add_output_string("None", "None");
}
</t>
<t tx="ekr.20240929074037.76">fn do_Nonlocal(&amp;mut self) {
    self.add_output_string("Nonlocal", "nonlocal");
}
</t>
<t tx="ekr.20240929074037.77">fn do_Not(&amp;mut self) {
    self.add_output_string("Not", "not");
}
</t>
<t tx="ekr.20240929074037.78">fn do_NotEqual(&amp;mut self) {
    self.add_output_string("NotEqual", "!=");
}
</t>
<t tx="ekr.20240929074037.79">fn do_Or(&amp;mut self) {
    self.add_output_string("Or", "or");
}
</t>
<t tx="ekr.20240929074037.8"></t>
<t tx="ekr.20240929074037.80">fn do_Pass(&amp;mut self) {
    self.add_output_string("Pass", "pass");
}
</t>
<t tx="ekr.20240929074037.81">fn do_Percent(&amp;mut self) {
    self.add_output_string("Percent", "%");
}
</t>
<t tx="ekr.20240929074037.82">fn do_PercentEqual(&amp;mut self) {
    self.add_output_string("PercentEqual", "%=");
}
</t>
<t tx="ekr.20240929074037.83">fn do_Plus(&amp;mut self) {
    self.add_output_string("Plus", "+");
}
</t>
<t tx="ekr.20240929074037.84">fn do_PlusEqual(&amp;mut self) {
    self.add_output_string("PlusEqual", "+=");
}
</t>
<t tx="ekr.20240929074037.85">fn do_Raise(&amp;mut self) {
    self.add_output_string("Raise", "raise");
}
</t>
<t tx="ekr.20240929074037.86">fn do_Rarrow(&amp;mut self) {
    self.add_output_string("Rarrow", "-&gt;");
}
</t>
<t tx="ekr.20240929074037.87">fn do_Rbrace(&amp;mut self) {
    self.add_output_string("Rbrace", "]");
}
</t>
<t tx="ekr.20240929074037.88">fn do_Return(&amp;mut self) {
    self.add_output_string("Return", "return");
}
</t>
<t tx="ekr.20240929074037.89">fn do_RightShift(&amp;mut self) {
    self.add_output_string("RightShift", "&gt;&gt;");
}
</t>
<t tx="ekr.20240929074037.9">fn do_Comment(&amp;mut self, tok_value: &amp;str) {
    // print!("{tok_value}");  // Correct.
    // print!("{value} ");  // Wrong!
    self.add_output_string("Comment", tok_value);
}
</t>
<t tx="ekr.20240929074037.90">fn do_RightShiftEqual(&amp;mut self) {
    self.add_output_string("RightShiftEqual", "&gt;&gt;=");
}
</t>
<t tx="ekr.20240929074037.91">fn do_Rpar(&amp;mut self) {
    self.add_output_string("Rpar", ")");
}
</t>
<t tx="ekr.20240929074037.92">fn do_Rsqb(&amp;mut self) {
    self.add_output_string("Rsqb", "]");
}
</t>
<t tx="ekr.20240929074037.93">fn do_Semi(&amp;mut self) {
    self.add_output_string("Semi", ";");
}
</t>
<t tx="ekr.20240929074037.94">fn do_Slash(&amp;mut self) {
    self.add_output_string("Slash", "/");
}
</t>
<t tx="ekr.20240929074037.95">fn do_SlashEqual(&amp;mut self) {
    self.add_output_string("SlashEqual", "/=");
}
</t>
<t tx="ekr.20240929074037.96">fn do_Star(&amp;mut self) {
    self.add_output_string("Star", "*");
}
</t>
<t tx="ekr.20240929074037.97">fn do_StarEqual(&amp;mut self) {
    self.add_output_string("StarEqual", "*=");
}
</t>
<t tx="ekr.20240929074037.98">fn do_StartExpression(&amp;mut self) {
    // self.add_output_string("StartExpression", "");
}
</t>
<t tx="ekr.20240929074037.99">fn do_StartInteractive(&amp;mut self) {
    // self.add_output_string("StartModule", "");
}
</t>
<t tx="ekr.20240929074547.1">#[derive(Debug)]
pub struct Stats {
    // Cumulative statistics for all files.
    n_files: u64, // Number of files.
    n_tokens: u64, // Number of tokens.
    n_ws_tokens: u64, // Number of pseudo-ws tokens.

    // Timing stat, in microseconds...
    beautify_time: u128,
    make_tokens_time: u128,
    read_time: u128,
    write_time: u128,
}

// #[allow(dead_code)]
// #[allow(non_snake_case)]
impl Stats {
    @others
}
</t>
<t tx="ekr.20240929074941.1">fn update_times (&amp;mut self,
    beautify: u128,
    make_tokens: u128,
    read_time: u128,
    write_time: u128
) {
    // Update cumulative timing stats.
    self.beautify_time += beautify;
    self.make_tokens_time += make_tokens;
    self.read_time += read_time;
    self.write_time += write_time;
}
</t>
<t tx="ekr.20240929075236.1">fn report (&amp;mut self) {
    // Cumulative counts.
    let n_files = self.n_files;
    let n_tokens = self.n_tokens;
    let n_ws_tokens = self.n_ws_tokens;
    // Print cumulative timing stats, in ms.
    let read_time = self.fmt_ns(self.read_time);
    let make_tokens_time = self.fmt_ns(self.make_tokens_time);
    let beautify_time = self.fmt_ns(self.beautify_time);
    let write_time = self.fmt_ns(self.write_time);
    let total_time = self.fmt_ns(self.make_tokens_time + self.read_time + self.beautify_time + self.write_time);
    println!("");
    println!("     files: {n_files}, tokens: {n_tokens}, ws tokens: {n_ws_tokens}");
    println!("       read: {read_time:&gt;7} ms");
    println!("make_tokens: {make_tokens_time:&gt;7} ms");
    println!("   beautify: {beautify_time:&gt;7} ms");
    println!("      write: {write_time:&gt;7} ms");
    println!("      total: {total_time:&gt;7} ms");
}
</t>
<t tx="ekr.20240929080242.1">fn fmt_ns(&amp;mut self, t: u128) -&gt; String {
    //! Convert nanoseconds to fractional milliseconds.
    let ms = t / 1000000;
    let micro = (t % 1000000) / 10000;  // 2-places only.
    // println!("t: {t:8} ms: {ms:03} micro: {micro:02}");
    return f!("{ms:4}.{micro:02}");
}

</t>
<t tx="ekr.20240929084852.1">@language rest
@wrap

Whether a value is on the stack or the heap affects how the language behaves.

Rules:
- Each value in Rust has an *owner*.
- There can only be one owner at a time.
- When the owner goes out of scope, the value will be dropped.

String literals can't be mutated.  String objects are on the heap and can be mutated.
The *drop* function releases heap objects.

Move is a shallow copy:
@language rust
    let s1 = String::from("hello");
    let s2 = s1;
    println!("{s1}, world!");  // wrong.
@language rest

*clone* makes a deep copy.

Stack-only vars have *copy trait*.
Can't add `copy` trait if object implements `drop`.

Ownership and functions:

Passing a variable to a function will move or copy, just as assignment does. 

Return Values and Scope:

Returning values can also transfer ownership.

References and borrowing:

'&amp;' represents a reference.
References are immutable by defaault.
If you have a mutable reference to a value, you can have no other references to that value. 

Rules of reference:

- At any time, you can have either one mutable reference or any number of immutable references.
- References must always be valid.
</t>
<t tx="ekr.20240930063514.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git commit'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930063546.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git status'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930063740.1">@language python
import os
import subprocess

if c.changed:
    c.save()
for command in ('git add *.rs', 'git status'):
    subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930064435.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git reset'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930064622.1">@language python
import os
import subprocess
if c.changed:
    c.save()
command = 'git push'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930084648.1">fn read(file_path: &amp;str) -&gt; String {
    let error_s = f!("Can not read {file_path}");
    return fs::read_to_string(file_path).expect(&amp;error_s);
}
</t>
<t tx="ekr.20240930085546.1">fn lex_contents(contents: &amp;str) -&gt; Vec&lt;(Tok, TextRange)&gt; {
    return lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;();
}
</t>
<t tx="ekr.20240930100553.1">// Compute cumulative stats.
let total_time = fmt_ms(t1.elapsed().as_micros());
let tokens_n = input_list.len();
println!("");
println!("tbo: {short_file_name}");
println!("{n_tokens} lex tokens, {ws_tokens_n} ws_tokens, len(input_list): {tokens_n}");
println!("");
println!("       read: {read_time:&gt;5} ms");
// println!("        lex: {lex_time:&gt;5} ms");
println!("make_tokens: {loop_time:&gt;5} ms");
println!("      total: {total_time:&gt;5} ms");
</t>
<t tx="ekr.20240930100625.1">let t1 = Instant::now();
let contents = read(&amp;file_path);
let read_time = fmt_ms(t1.elapsed().as_micros());
</t>
<t tx="ekr.20240930100636.1">let t2 = Instant::now();
let tokens = lex_contents(&amp;contents);
let lex_time = fmt_ms(t2.elapsed().as_micros());
</t>
<t tx="ekr.20240930100707.1">let t4 = Instant::now();
let mut input_list: Vec&lt;InputTok&gt; = Vec::new();
let (n_tokens, ws_tokens_n) = make_input_list(&amp;contents, &amp;mut input_list); ////, tokens);
let loop_time = fmt_ms(t4.elapsed().as_micros());
</t>
<t tx="ekr.20240930101156.1">@language python
import os
import subprocess

if c.changed:
    c.save()
for command in ('git add *.leo', 'git status'):
    subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20241001055017.1">@language rust

// A trait defines the functionality a particular type has and can share with other types. 
// Traits are like interfaces.

pub trait Summary {
    fn summarize(&amp;self) -&gt; String;
}

impl Summary for NewsArticle {
    fn summarize(&amp;self) -&gt; String {
        format!("{}, by {} ({})",, self.author, self.location)
    }
}

// Traits can have default implementations for some or all functions.
pub trait Summary {
    fn summarize_author(&amp;self) -&gt; String;

    fn summarize(&amp;self) -&gt; String {
        format!("(Read more from {}...)", self.summarize_author())
    }
}

// ***&amp; Traits as parameters: var: &amp;impl TraitName.

pub fn notify(item1: &amp;impl Summary, item2: &amp;impl Summary) {...}

// *** Trait-bound syntax.  &lt;T: Trait&gt;(item: &amp;T).

pub fn notify&lt;T: Summary&gt;(item1: &amp;T, item2: &amp;T) {...}

// *** Multiple trait bounds Trail + Trait

pub fn notify(item: &amp;(impl Summary + Display)) {...}

pub fn notify&lt;T: Summary + Display&gt;(item: &amp;T) {...}

// *** Where syntax:

fn some_function&lt;T: Display + Clone, U: Clone + Debug&gt;(t: &amp;T, u: &amp;U) -&gt; i32 {...}

fn some_function&lt;T, U&gt;(t: &amp;T, u: &amp;U) -&gt; i32
where
    T: Display + Clone,
    U: Clone + Debug,
{...}

// *** Returning types that implement traits:

// The function can only return a single type.

fn returns_summarizable() -&gt; impl Summary {...}

// The ability to specify a return type only by the trait it implements is
// especially useful in the context of closures and iterators.

// *** We can call o.to_string method defined by the ToString trait on
//     any type that implements the Display trait. 
</t>
<t tx="ekr.20241001060848.1"></t>
<t tx="ekr.20241001071914.1">fn test_loop(contents: &amp;str) {
    let mut n_tokens = 0;
    for token in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        if n_tokens &lt; 10 {
            println!("token: {token:?}")
        }
        n_tokens += 1;
    }
    println!("tokens: {n_tokens}")
}
</t>
<t tx="ekr.20241001073040.1"></t>
<t tx="ekr.20241001093308.1">pub fn entry() {

    // Main line of beautifier.
    let mut x = Beautifier::new();
    if true {  // testing.
        println!("");
        for file_path in [
            "C:\\Repos\\leo-editor\\leo\\core\\leoFrame.py",
            // "C:\\Repos\\leo-editor\\leo\\core\\leoApp.py"
        ] {
            x.beautify_one_file(&amp;file_path);
        }
        x.stats.report();
    }
    else {
        if x.enabled("--help") || x.enabled("-h") {
            x.show_help();
            return;
        }
        x.show_args();
        x.beautify_all_files();
    }
}
</t>
<t tx="ekr.20241001093308.2">fn tokenize() {
    &lt;&lt; tokenize: define contents &gt;&gt;
    println!("fn tokenize");
    println!("\nSource:\n{contents}");

    for debug in [true, false].iter() {

        println!("{}", if *debug {"Tokens..."} else {"\nBeautified:"});

        let results = lex(contents, Mode::Module);  // An iterator yielding Option(Tok).
        let mut count = 0;
        let mut lws = String::new();
        for (i, result) in results.enumerate() {
            use Tok::*;
            let token = result.ok().unwrap();
            let (ref tok_class, tok_range) = token;
            let tok_value = &amp;contents[tok_range];

            if *debug {
                let s = format!("{tok_class}");
                print!("\nToken: {s:20} {:?}", tok_value);
            }
            else {
                // Comment(value), Name(name)
                #[allow(unused_variables)]
                match tok_class {
                    Comment(value) =&gt; {
                        // print!("{value} ");  // Wrong!
                        print!("{tok_value}");
                    },
                    Dedent =&gt; {
                        lws.pop();
                        lws.pop();
                        print!("{lws}");
                    },
                    Def =&gt; {
                        print!("{tok_value} ");
                    },
                    Indent =&gt; {
                        lws.push_str("    ");
                        print!("{lws}");
                    },
                    Name {name} =&gt; {
                        print!("{tok_value} ");
                    },
                    Newline =&gt; {
                        print!("{tok_value}");
                        print!("{lws}");
                        if false {  // old
                            println!("");
                            print!("{lws}");
                        }
                    },
                    NonLogicalNewline =&gt; {
                        println!("");
                        print!("{lws}");
                    },
                    Return =&gt; {
                        print!("{tok_value} ");
                    },
                    Tok::String {value, kind, triple_quoted} =&gt; {
                        // correct.
                        print!("{tok_value}");
                        if false {  // incorrect.
                            let quote = if *triple_quoted {"'''"} else {"'"};
                            print!("{:?}:{quote}{value}{quote}", kind);
                        }
                    },
                    _ =&gt; {
                        print!("{tok_value}");
                        if false {
                            // to_string quotes values!
                            let s = tok_class.to_string().replace("'", "");
                            print!("{s}");
                        }
                    },
                }
            }
            count = i
        }
        if *debug {
            println!("\n{count} tokens")
        }
    }
}
</t>
<t tx="ekr.20241001093308.3">let contents = r#"
def test():
# Comment 1.
print('abc')
# Comment 2.
"#;

// print("xyz")
// print(rf'pdb')
// print(fr'pdb2')
// return bool(i &amp; 1)
</t>
<t tx="ekr.20241001100954.1">pub fn new() -&gt;Stats {
    let x = Stats {
        // Cumulative counts.
        n_files: 0,  // Number of files.
        n_tokens: 0, // Number of tokens.
        n_ws_tokens: 0,  // Number of pseudo-ws tokens.

        // Timing stats, in nanoseconds...
        beautify_time: 0,
        make_tokens_time: 0,
        read_time: 0,
        write_time: 0,
    };
    return x;
}
</t>
<t tx="ekr.20241001104914.1"></t>
<t tx="ekr.20241001164547.1"></t>
<t tx="ekr.20241001213229.1"> def no_visitor(self) -&gt; None:  # pragma: no cover
        self.oops(f"Unknown kind: {self.input_token.kind!r}")

    &lt;&lt; LB::beautify: init ivars &gt;&gt;

    try:
        // Pre-scan the token list, setting context.s
        self.pre_scan();

        // Init ivars first.
        self.input_token = None;  // ???
        self.pending_lws = '';  // ???
        self.pending_ws = '';  // ???
        self.prev_output_kind = None;    // ???
        self.prev_output_value = None;  // ???

        // Init state.
        self.gen_token('file-start', '');
        self.push_state('file-start');

        // The main loop:
        prev_line_number: i32 = 0;
        for (self.index, self.input_token) in enumerate(input_tokens):
            // Set global for visitors.
            if prev_line_number != self.input_token.line_number {
                prev_line_number = self.input_token.line_number;
            }
            // Call the proper visitor.
            if self.verbatim {
                self.do_verbatim();
            } else {
                // Use match ???
                func = getattr(self, f"do_{self.input_token.kind}", self.no_visitor)
                func()
            }

        // Return the result. ???
        result = ''.join(self.output_list);
        return result;
</t>
<t tx="ekr.20241001213329.1">// Debugging vars...
self.line_number = 0;  // was None?

// State vars for whitespace.
self.curly_brackets_level = 0;  // Number of unmatched '{' tokens.
self.paren_level = 0;  // Number of unmatched '(' tokens.
self.square_brackets_stack = Vec::new();  // A stack of bools, for self.gen_word().
self.indent_level = 0;  // Set only by do_indent and do_dedent.

// Parse state.
self.decorator_seen = false; // Set by do_name for do_op.
self.in_arg_list = 0;        // &gt; 0 if in an arg list of a def.
self.in_doc_part = false;

// To do.
// self.state_stack = Vec::new();  // list[ParseState] = []  # Stack of ParseState objects.

// Leo-related state.
self.verbatim = false;  // True: don't beautify.

// Ivars describing the present input token...
self.index = 0;             // The index within the tokens array of the token being scanned.
self.lws = String::new();   // Leading whitespace. Required!
</t>
<t tx="ekr.20241001215023.1">class ParseState:
    """
    A class representing items in the parse state stack.

    The present states:

    'file-start': Ensures the stack stack is never empty.

    'decorator': The last '@' was a decorator.

        do_op():    push_state('decorator')
        do_name():  pops the stack if state.kind == 'decorator'.

    'indent': The indentation level for 'class' and 'def' names.

        do_name():      push_state('indent', self.level)
        do_dendent():   pops the stack once or
                        twice if state.value == self.level.
    """

    def __init__(self, kind: str, value: Union[int, str, None]) -&gt; None:
        self.kind = kind
        self.value = value

    def __repr__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover

    def __str__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover
</t>
<t tx="ekr.20241002054443.1"></t>
</tnodes>
</leo_file>
