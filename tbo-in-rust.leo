<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: https://leo-editor.github.io/leo-editor/leo_toc.html -->
<leo_file xmlns:leo="https://leo-editor.github.io/leo-editor/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20240927151701.1" descendentVnodeUnknownAttributes="7d7100285803000000302e3071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a73735805000000302e302e3771067d71075808000000616e6e6f7461746571087d71092858080000007072696f72697479710a4d0f27580a00000070726973657464617465710b580a000000323032312d30332d3330710c75735803000000302e31710d7d710e580b0000005f5f626f6f6b6d61726b73710f7d7110580700000069735f6475706571114930300a7373752e"><vh>Startup</vh>
<v t="ekr.20240927151701.9" descendentVnodeUnknownAttributes="7d71002858010000003071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a73735803000000302e3771067d71075808000000616e6e6f7461746571087d71092858080000007072696f72697479710a4d0f27580a00000070726973657464617465710b580a000000323032312d30332d3330710c7573752e"><vh>@settings</vh>
<v t="ekr.20240927151701.44"><vh>@bool allow-text-zoom = True</vh></v>
<v t="ekr.20240927151701.45"><vh>@bool check-python-code-on-write = False</vh></v>
<v t="ekr.20240927151701.46"><vh>@bool use-german-keyboard = False</vh></v>
<v t="ekr.20240927151701.47"><vh>@bool use-mouse-expand-gestures = False</vh></v>
<v t="ekr.20240927151701.48"><vh>@data exec-script-commands</vh></v>
<v t="ekr.20240927151701.49"><vh>@data exec-script-patterns</vh></v>
<v t="ekr.20240927151701.50"><vh>@data history-list</vh></v>
<v t="ekr.20240927151701.170" descendentVnodeUnknownAttributes="7d710058010000003071017d71025808000000616e6e6f7461746571037d71042858080000007072696f7269747971054d0f27580a000000707269736574646174657106580a000000323032312d30332d333071077573732e"><vh>@enabled-plugins</vh></v>
<v t="ekr.20240927151701.51"><vh>@string qt-layout-name = legacy</vh></v>
<v t="ekr.20240927151701.92"><vh>Appearance settings</vh>
<v t="ekr.20240927151701.93"><vh>@bool log-pane-wraps = False</vh></v>
<v t="ekr.20240927151701.94"><vh>@bool recent-files-group-always = True</vh></v>
<v t="ekr.20240927151701.95"><vh>@bool show-iconbar = True</vh></v>
<v t="ekr.20240927151701.96"><vh>@bool show-tips = False</vh></v>
<v t="ekr.20240927151701.97"><vh>@bool stayInTreeAfterSelect = True</vh></v>
<v t="ekr.20240927151701.98"><vh>@bool use-chapter-tabs = False</vh></v>
<v t="ekr.20240927151701.99"><vh>@bool use-chapters = False</vh></v>
<v t="ekr.20240927151701.100"><vh>@bool use-gutter = False</vh></v>
<v t="ekr.20240927151701.101"><vh>@int qweb-view-font-size = 30</vh></v>
<v t="ekr.20240927151701.102"><vh>@string initial-split-orientation = v</vh></v>
</v>
<v t="ekr.20240927151701.103"><vh>Coloring settings</vh>
<v t="ekr.20240927151701.104"><vh>@bool color-doc-parts-as-rest = True</vh></v>
<v t="ekr.20240927151701.105"><vh>@bool use-pygments = False</vh></v>
<v t="ekr.20240927151701.106"><vh>@bool use-pygments-styles = False</vh></v>
<v t="ekr.20240927151701.107"><vh>@color head-bg = @mistyrose2</vh></v>
<v t="ekr.20240927151701.108"><vh>@string pygments-style-name = leonine</vh></v>
<v t="ekr.20240927151701.109"><vh>@string target-language = rust</vh></v>
</v>
<v t="ekr.20240927151701.110"><vh>Command settings</vh>
<v t="ekr.20240927151701.111"><vh>@bool create-at-persistence-nodes-automatically = False</vh></v>
<v t="ekr.20240927151701.112"><vh>@bool enable-persistence = False</vh></v>
<v t="ekr.20240927151701.113"><vh>@bool make-node-conflicts-node = True</vh></v>
<v t="ekr.20240927151701.184"><vh>@bool qt-use-scintilla = False</vh></v>
<v t="ekr.20240927151701.114"><vh>@bool run-pyflakes-on-write = False</vh></v>
<v t="ekr.20240927151701.126"><vh>@bool tree-declutter = False</vh></v>
<v t="ekr.20240927151701.115"><vh>@bool use-jedi = False</vh></v>
<v t="ekr.20240927151701.116"><vh>@bool use-qcompleter = False</vh></v>
<v t="ekr.20240927151701.202"><vh>@bool vim-mode = False</vh></v>
<v t="ekr.20240927151701.117"><vh>@bool warn-about-redefined-shortcuts = True</vh></v>
<v t="ekr.20240927151701.118"><vh>@int auto-justify = 80</vh></v>
<v t="ekr.20240927151701.119"><vh>rst3 path options</vh>
<v t="ekr.20240927151701.120"><vh>@string rst3-write-intermediate-extension = .txt</vh></v>
<v t="ekr.20240927151701.121"><vh>@string rst3-default-path = None</vh></v>
<v t="ekr.20240927151701.122"><vh>@string rst3-stylesheet-name = default.css</vh></v>
<v t="ekr.20240927151701.123"><vh>@string rst3-stylesheet-path = None</vh></v>
<v t="ekr.20240927151701.124"><vh>@string rst3-publish-argv-for-missing-stylesheets = None</vh></v>
</v>
</v>
<v t="ekr.20240927151701.144"><vh>File settings</vh>
<v t="ekr.20240927151701.145"><vh>@bool open-with-clean-filenames = True</vh></v>
<v t="ekr.20240927151701.146"><vh>@bool check-for-changed-external-files = True</vh></v>
<v t="ekr.20240927151701.147"><vh>@bool open-with-save-on-update = False</vh></v>
<v t="ekr.20240927151701.148"><vh>@bool open-with-uses-derived-file-extensions = True</vh></v>
</v>
<v t="ekr.20240927151701.149"><vh>Find settings</vh>
<v t="ekr.20240927151701.150"><vh>@bool auto-scroll-find-tab = False</vh></v>
<v t="ekr.20240927151701.151"><vh>@bool close-find-dialog-after-search = False</vh></v>
<v t="ekr.20240927151701.152"><vh>@bool find-ignore-duplicates = False</vh></v>
<v t="ekr.20240927151701.153"><vh>@bool minibuffer-find-mode = True</vh></v>
<v t="ekr.20240927151701.154"><vh>@bool use-find-dialog = False</vh></v>
</v>
<v t="ekr.20240927151701.155"><vh>Importer settings</vh>
<v t="ekr.20240927151701.156"><vh>@data import-html-tags</vh></v>
<v t="ekr.20240927151701.157"><vh>@data import-xml-tags</vh></v>
</v>
<v t="ekr.20240927153018.1"><vh>Scripts</vh>
<v t="ekr.20240927151701.229"><vh> Recursive import script</vh>
<v t="ekr.20240927151701.230"><vh>&lt;&lt; rust dir_list &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20240927151701.190"><vh>Syntax coloring settings</vh>
<v t="ekr.20240927151701.191"><vh>@@color rest.keyword2 = red</vh></v>
<v t="ekr.20240927151701.192"><vh>@@color rest.keyword4 = blue</vh></v>
<v t="ekr.20240927151701.193"><vh>@@color rest.leokeyword = green</vh></v>
<v t="ekr.20240927151701.194"><vh>@color forth.keyword3 = black</vh></v>
<v t="ekr.20240927151701.195"><vh>@color python.name = @solarized-yellow</vh></v>
<v t="ekr.20240927151701.196"><vh>@font rest.comment1</vh></v>
</v>
<v t="ekr.20240927151701.176"><vh>VR settings</vh>
<v t="ekr.20240927151701.177"><vh>@bool view-rendered-auto-create = False</vh></v>
<v t="ekr.20240927151701.178"><vh>@bool view-rendered-auto-hide = False</vh></v>
<v t="ekr.20240927151701.179"><vh>@string view-rendered-default-kind = rst</vh></v>
</v>
</v>
<v t="ekr.20240927151701.203" descendentVnodeUnknownAttributes="7d710058010000003071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a7373732e"><vh>Buttons &amp; commands</vh>
<v t="ekr.20240927151701.206"><vh>@button backup</vh></v>
<v t="ekr.20240928073118.1"><vh>@button cargo-fmt</vh></v>
<v t="ekr.20240927152759.1"><vh>@button cargo-run</vh></v>
<v t="ekr.20240930063740.1"><vh>@@command ga</vh></v>
<v t="ekr.20240930101156.1"><vh>@@command ga-leo</vh></v>
<v t="ekr.20240930063514.1"><vh>@@command gc</vh></v>
<v t="ekr.20240930063546.1"><vh>@command gs</vh></v>
<v t="ekr.20240930064435.1"><vh>@@command git-reset</vh></v>
<v t="ekr.20240930064622.1"><vh>@@command push</vh></v>
<v t="ekr.20240927151701.207"><vh>@@button print-gnx</vh></v>
</v>
</v>
<v t="ekr.20240927154009.1"><vh>Files</vh>
<v t="ekr.20240927151245.1"><vh>@edit Cargo.toml</vh></v>
<v t="ekr.20241007005233.1"><vh>@clean test/test1.py</vh></v>
<v t="ekr.20241007130048.1"><vh>@clean readme.md</vh></v>
<v t="ekr.20240927151332.1"><vh>@file src/main.rs</vh></v>
<v t="ekr.20240928161210.1"><vh>@file src/tbo.rs</vh></v>
</v>
<v t="ekr.20241007063926.1"><vh>--- Unused</vh>
<v t="ekr.20241003094145.1"><vh> struct TestTok</vh></v>
<v t="ekr.20241001093308.1"><vh>fn test &amp; helpers</vh>
<v t="ekr.20241003094218.2"><vh>fn test_struct</vh></v>
<v t="ekr.20241003094218.1"><vh>fn test_vec &amp; push_vec</vh></v>
</v>
</v>
<v t="ekr.20241007184818.1"><vh>--- Copies: Python</vh>
<v t="ekr.20241007005513.1"><vh>COPY: tests from test_leoTokens.py</vh>
<v t="ekr.20241007005513.17"><vh>class TestTokens (BaseTest)</vh>
<v t="ekr.20241007005513.18"><vh>TT.show_example_dump</vh></v>
<v t="ekr.20241007005513.19"><vh>TT.test_bs_nl_tokens</vh></v>
<v t="ekr.20241007005513.20"><vh>TT.test_continuation_1</vh></v>
<v t="ekr.20241007005513.21"><vh>TT.test_continuation_2</vh></v>
<v t="ekr.20241007005513.22"><vh>TT.test_continuation_3</vh></v>
<v t="ekr.20241007005513.23"><vh>TT.test_string_concatenation_1</vh></v>
<v t="ekr.20241007005513.24"><vh>TT.test_string_concatenation_2</vh></v>
<v t="ekr.20241007005513.25"><vh>TT.test_string_concatenation_3</vh></v>
</v>
<v t="ekr.20241007005513.26"><vh>class TestTokenBasedOrange (BaseTest)</vh>
<v t="ekr.20241007005513.29"><vh>TestTBO.test_annotations</vh></v>
<v t="ekr.20241007005513.30"><vh>TestTBO.test_at_doc_part</vh></v>
<v t="ekr.20241007005513.31"><vh>TestTBO.test_backslash_newline</vh></v>
<v t="ekr.20241007005513.32"><vh>TestTBO.test_blank_lines_after_function</vh></v>
<v t="ekr.20241007005513.33"><vh>TestTBO.test_blank_lines_after_function_2</vh></v>
<v t="ekr.20241007005513.34"><vh>TestTBO.test_blank_lines_after_function_3</vh></v>
<v t="ekr.20241007005513.35"><vh>TestTBO.test_comment_indented</vh></v>
<v t="ekr.20241007005513.36"><vh>TestTBO.test_comment_space_after_delim</vh></v>
<v t="ekr.20241007005513.37"><vh>TestTBO.test_decorators</vh></v>
<v t="ekr.20241007005513.38"><vh>TestTBO.test_dont_delete_blank_lines</vh></v>
<v t="ekr.20241007005513.39"><vh>TestTBO.test_leo_sentinels</vh></v>
<v t="ekr.20241007005513.40"><vh>TestTBO.test_leo_sentinels_2</vh></v>
<v t="ekr.20241007005513.41"><vh>TestTBO.test_lines_before_class</vh></v>
<v t="ekr.20241007005513.42"><vh>TestTBO.test_multi_line_imports</vh></v>
<v t="ekr.20241007005513.43"><vh>TestTBO.test_multi_line_pet_peeves</vh></v>
<v t="ekr.20241007005513.44"><vh>TestTBO.test_multi_line_statement</vh></v>
<v t="ekr.20241007005513.45"><vh>TestTBO.test_one_line_pet_peeves</vh></v>
<v t="ekr.20241007005513.46"><vh>TestTBO.test_percent_op</vh></v>
<v t="ekr.20241007005513.47"><vh>TestTBO.test_relative_imports</vh></v>
<v t="ekr.20241007005513.48"><vh>TestTBO.test_slice</vh></v>
<v t="ekr.20241007005513.49"><vh>TestTBO.test_star_star_operator</vh></v>
<v t="ekr.20241007005513.50"><vh>TestTBO.test_unary_ops</vh></v>
<v t="ekr.20241007005513.51"><vh>TestTBO.test_verbatim</vh></v>
<v t="ekr.20241007005513.52"><vh>TestTBO.test_verbatim_with_pragma</vh></v>
<v t="ekr.20241007005513.53"><vh>TestTBO.verbatim2</vh></v>
</v>
</v>
<v t="ekr.20241007011856.1"><vh>COPY: leoTokens.py</vh>
<v t="ekr.20241007011856.4"><vh>top-level functions (leoTokens.py)</vh>
<v t="ekr.20241007011856.5"><vh>function: dump_contents</vh></v>
<v t="ekr.20241007011856.6"><vh>function: dump_lines</vh></v>
<v t="ekr.20241007011856.7"><vh>function: dump_results</vh></v>
<v t="ekr.20241007011856.8"><vh>function: dump_tokens</vh></v>
<v t="ekr.20241007011856.9"><vh>function: input_tokens_to_string</vh></v>
<v t="ekr.20241007011856.10"><vh>function: beautify_file (leoTokens.py) (new)</vh></v>
<v t="ekr.20241007011856.11"><vh>function: main (leoTokens.py)</vh></v>
<v t="ekr.20241007011856.12"><vh>function: orange_command (leoTokens.py)</vh></v>
<v t="ekr.20241007011856.13"><vh>function: scan_args (leoTokens.py)</vh></v>
</v>
<v t="ekr.20241007011856.14"><vh>Classes</vh>
<v t="ekr.20241007011856.15"><vh>class InternalBeautifierError(Exception)</vh></v>
<v t="ekr.20241007011856.16"><vh>class InputToken</vh>
<v t="ekr.20241007011856.17"><vh>itoken.brief_dump</vh></v>
<v t="ekr.20241007011856.18"><vh>itoken.dump</vh></v>
<v t="ekr.20241007011856.19"><vh>itoken.dump_header</vh></v>
<v t="ekr.20241007011856.20"><vh>itoken.error_dump</vh></v>
<v t="ekr.20241007011856.21"><vh>itoken.show_val</vh></v>
</v>
<v t="ekr.20241007011856.22"><vh>class Tokenizer</vh>
<v t="ekr.20241007011856.23"><vh>Tokenizer.add_token</vh></v>
<v t="ekr.20241007011856.24"><vh>Tokenizer.check_results</vh></v>
<v t="ekr.20241007011856.25"><vh>Tokenizer.check_round_trip</vh></v>
<v t="ekr.20241007011856.26"><vh>Tokenizer.create_input_tokens</vh></v>
<v t="ekr.20241007011856.27"><vh>Tokenizer.do_token (the gem)</vh></v>
<v t="ekr.20241007011856.28"><vh>Tokenizer.make_input_tokens (entry)</vh></v>
<v t="ekr.20241007011856.29"><vh>Tokenizer.tokens_to_string</vh></v>
</v>
<v t="ekr.20241007011856.30"><vh>class ParseState</vh></v>
<v t="ekr.20241007011856.31"><vh>class ScanState</vh></v>
<v t="ekr.20241007011856.32"><vh>class TokenBasedOrange</vh>
<v t="ekr.20241007011856.33"><vh>&lt;&lt; TokenBasedOrange: docstring &gt;&gt;</vh></v>
<v t="ekr.20241007011856.34"><vh>&lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;</vh></v>
<v t="ekr.20241007011856.35"><vh>&lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;</vh></v>
<v t="ekr.20241007011856.36"><vh>tbo.ctor</vh></v>
<v t="ekr.20241007011856.37"><vh>tbo: Checking &amp; dumping</vh>
<v t="ekr.20241007011856.38"><vh>tbo.dump_token_range</vh></v>
<v t="ekr.20241007011856.39"><vh>tbo.internal_error_message</vh></v>
<v t="ekr.20241007011856.40"><vh>tbo.user_error_message</vh></v>
<v t="ekr.20241007011856.41"><vh>tbo.oops</vh></v>
</v>
<v t="ekr.20241007011856.42"><vh>tbo: Entries &amp; helpers</vh>
<v t="ekr.20241007011856.43"><vh>tbo.beautify (main token loop)</vh>
<v t="ekr.20241007011856.44"><vh>&lt;&lt; tbo.beautify: init ivars &gt;&gt;</vh></v>
</v>
<v t="ekr.20241007011856.45"><vh>tbo.beautify_file (entry) (stats &amp; diffs)</vh></v>
<v t="ekr.20241007011856.46"><vh>tbo.init_tokens_from_file</vh></v>
<v t="ekr.20241007011856.47"><vh>tbo.regularize_newlines</vh></v>
<v t="ekr.20241007011856.48"><vh>tbo.write_file</vh></v>
<v t="ekr.20241007011856.49"><vh>tbo.show_diffs</vh></v>
</v>
<v t="ekr.20241007011856.50"><vh>tbo: Visitors &amp; generators</vh>
<v t="ekr.20241007011856.51"><vh>tbo.do_comment</vh>
<v t="ekr.20241007011856.52"><vh>&lt;&lt; do_comment: update comment-related state &gt;&gt;</vh></v>
</v>
<v t="ekr.20241007011856.53"><vh>tbo.do_dedent</vh></v>
<v t="ekr.20241007011856.54"><vh>tbo.do_encoding</vh></v>
<v t="ekr.20241007011856.55"><vh>tbo.do_endmarker</vh></v>
<v t="ekr.20241007011856.56"><vh>tbo.do_indent</vh></v>
<v t="ekr.20241007011856.57"><vh>tbo.do_name &amp; generators</vh>
<v t="ekr.20241007011856.58"><vh>tbo.do_name</vh></v>
<v t="ekr.20241007011856.59"><vh>tbo.gen_word</vh></v>
<v t="ekr.20241007011856.60"><vh>tbo.gen_word_op</vh></v>
</v>
<v t="ekr.20241007011856.61"><vh>tbo.do_newline, do_nl &amp; generators</vh>
<v t="ekr.20241007011856.62"><vh>tbo.do_newline</vh></v>
<v t="ekr.20241007011856.63"><vh>tbo.do_nl</vh></v>
</v>
<v t="ekr.20241007011856.64"><vh>tbo.do_number</vh></v>
<v t="ekr.20241007011856.65"><vh>tbo.do_op &amp; generators</vh>
<v t="ekr.20241007011856.66"><vh>tbo.do_op</vh></v>
<v t="ekr.20241007011856.67"><vh>tbo.gen_colon &amp; helper</vh></v>
<v t="ekr.20241007011856.68"><vh>tbo.gen_dot_op &amp; _next</vh>
<v t="ekr.20241007011856.69"><vh>tbo._next</vh></v>
</v>
<v t="ekr.20241007011856.70"><vh>tbo.gen_equal_op</vh></v>
<v t="ekr.20241007011856.71"><vh>tbo.gen_lt</vh></v>
<v t="ekr.20241007011856.72"><vh>tbo.gen_possible_unary_op &amp; helper</vh>
<v t="ekr.20241007011856.73"><vh>tbo.is_unary_op &amp; _prev</vh>
<v t="ekr.20241007011856.74"><vh>tbo._prev</vh></v>
</v>
</v>
<v t="ekr.20241007011856.75"><vh>tbo.gen_rt</vh></v>
<v t="ekr.20241007011856.76"><vh>tbo.gen_star_op</vh></v>
<v t="ekr.20241007011856.77"><vh>tbo.gen_star_star_op</vh></v>
<v t="ekr.20241007011856.78"><vh>tbo.push_state</vh></v>
</v>
<v t="ekr.20241007011856.79"><vh>tbo.do_string</vh></v>
<v t="ekr.20241007011856.80"><vh>tbo.do_verbatim</vh></v>
<v t="ekr.20241007011856.81"><vh>tbo.do_ws</vh></v>
<v t="ekr.20241007011856.82"><vh>tbo.gen_blank</vh></v>
<v t="ekr.20241007011856.83"><vh>tbo.gen_token</vh></v>
</v>
<v t="ekr.20241007011856.84"><vh>tbo: Scanning</vh>
<v t="ekr.20241007011856.85"><vh>tbo.pre_scan &amp; helpers</vh>
<v t="ekr.20241007011856.86"><vh>&lt;&lt; pre-scan 'newline' tokens &gt;&gt;</vh></v>
<v t="ekr.20241007011856.87"><vh>&lt;&lt; pre-scan 'op' tokens &gt;&gt;</vh></v>
<v t="ekr.20241007011856.88"><vh>&lt;&lt; pre-scan 'name' tokens &gt;&gt;</vh></v>
<v t="ekr.20241007011856.89"><vh>tbo.finish_arg</vh></v>
<v t="ekr.20241007011856.90"><vh>tbo.finish_slice</vh></v>
<v t="ekr.20241007011856.91"><vh>tbo.finish_dict</vh></v>
</v>
<v t="ekr.20241007011856.92"><vh>tbo.is_unary_op_with_prev</vh></v>
<v t="ekr.20241007011856.93"><vh>tbo.is_python_keyword</vh></v>
<v t="ekr.20241007011856.94"><vh>tbo.set_context</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20241007141942.1"><vh>COPY: tbo.pre_scan &amp; helpers</vh>
<v t="ekr.20241007141942.2"><vh>&lt;&lt; pre-scan 'newline' tokens &gt;&gt;</vh></v>
<v t="ekr.20241007141942.3"><vh>&lt;&lt; pre-scan 'op' tokens &gt;&gt;</vh></v>
<v t="ekr.20241007141942.4"><vh>&lt;&lt; pre-scan 'name' tokens &gt;&gt;</vh></v>
<v t="ekr.20241007141942.5"><vh>tbo.finish_arg</vh></v>
<v t="ekr.20241007141942.6"><vh>tbo.finish_slice</vh></v>
<v t="ekr.20241007141942.7"><vh>tbo.finish_dict</vh></v>
</v>
</v>
<v t="ekr.20240927154016.1"><vh>Notes</vh>
<v t="ekr.20240929084852.1"><vh>Ownership and mutation</vh></v>
<v t="ekr.20241001055017.1"><vh>Traits</vh></v>
<v t="ekr.20240928185643.1"><vh>Stats</vh></v>
</v>
<v t="ekr.20240927154323.1"><vh>** To do</vh></v>
<v t="ekr.20241001104914.1"><vh>--- classes</vh>
<v t="ekr.20241004095931.1"><vh>class AnnotatedInputTok</vh></v>
<v t="ekr.20241004110721.1"><vh>class Annotator</vh>
<v t="ekr.20241004095735.1"><vh>Annotator.annotate</vh></v>
<v t="ekr.20241005091217.1"><vh>Annotator.is_python_keyword (to do)</vh></v>
<v t="ekr.20241005092549.1"><vh>Annotator.is_unary_op_with_prev (to do)</vh></v>
<v t="ekr.20241004153742.1"><vh>Annotator.new</vh>
<v t="ekr.20241007085552.1"><vh>&lt;&lt; define Annotator::insignificant_tokens &gt;&gt;</vh></v>
<v t="ekr.20241007085705.1"><vh>&lt;&lt; define Annotator::op_kinds &gt;&gt;</vh></v>
</v>
<v t="ekr.20241004153802.1"><vh>Annotator.pre_scan &amp; helpers</vh>
<v t="ekr.20241004154345.2"><vh>&lt;&lt; pre-scan newline tokens &gt;&gt;</vh></v>
<v t="ekr.20241004154345.3"><vh>&lt;&lt; pre-scan op tokens &gt;&gt;</vh></v>
<v t="ekr.20241004154345.4"><vh>&lt;&lt; pre-scan name tokens &gt;&gt;</vh></v>
<v t="ekr.20241004154345.5"><vh>Annotator.finish_arg***</vh></v>
<v t="ekr.20241004154345.6"><vh>Annotator.finish_slice</vh></v>
<v t="ekr.20241004154345.7"><vh>Annotator.finish_dict</vh></v>
<v t="ekr.20241004163018.1"><vh>Annotator.set_context</vh></v>
</v>
</v>
<v t="ekr.20240929024648.120"><vh>class InputTok</vh></v>
<v t="ekr.20240929074037.1"><vh>class LeoBeautifier</vh>
<v t="ekr.20240929074037.114"><vh> LB.new</vh></v>
<v t="ekr.20240929074037.2"><vh>LB.add_output_string</vh></v>
<v t="ekr.20240929074037.113"><vh>LB.beautify</vh>
<v t="ekr.20241002062655.1"><vh>&lt;&lt; LB: beautify: dispatch on annotated_token.kind &gt;&gt;</vh></v>
</v>
<v t="ekr.20240929074037.4"><vh>LB.beautify_all_files</vh></v>
<v t="ekr.20240929074037.5"><vh>LB.beautify_one_file</vh></v>
<v t="ekr.20240929074037.7"><vh>LB.do_*</vh>
<v t="ekr.20241002071143.1"><vh>tbo.do_ws</vh></v>
<v t="ekr.20240929074037.8"><vh>LB:Handlers with values</vh>
<v t="ekr.20240929074037.9"><vh>LB.do_Comment</vh></v>
<v t="ekr.20240929074037.10"><vh>LB.do_Complex</vh></v>
<v t="ekr.20240929074037.11"><vh>LB.do_Float</vh></v>
<v t="ekr.20240929074037.12"><vh>LB.do_Int</vh></v>
<v t="ekr.20240929074037.13"><vh>LB.do_Name</vh></v>
<v t="ekr.20240929074037.14"><vh>LB.do_String</vh></v>
</v>
<v t="ekr.20240929074037.15"><vh>LB:Handlers using lws</vh>
<v t="ekr.20240929074037.16"><vh>LB.do_Dedent</vh></v>
<v t="ekr.20240929074037.17"><vh>LB.do_Indent</vh></v>
<v t="ekr.20240929074037.18"><vh>LB.do_Newline</vh></v>
<v t="ekr.20240929074037.19"><vh>LB.do_NonLogicalNewline</vh></v>
</v>
<v t="ekr.20240929074037.20"><vh>LB:Handlers w/o values</vh>
<v t="ekr.20240929074037.21"><vh>LB.do_Amper</vh></v>
<v t="ekr.20240929074037.22"><vh>LB.do_AmperEqual</vh></v>
<v t="ekr.20240929074037.23"><vh>LB.do_And</vh></v>
<v t="ekr.20240929074037.24"><vh>LB.do_As</vh></v>
<v t="ekr.20240929074037.25"><vh>LB.do_Assert</vh></v>
<v t="ekr.20240929074037.26"><vh>LB.do_Async</vh></v>
<v t="ekr.20240929074037.27"><vh>LB.do_At</vh></v>
<v t="ekr.20240929074037.28"><vh>LB.do_AtEqual</vh></v>
<v t="ekr.20240929074037.29"><vh>LB.do_Await</vh></v>
<v t="ekr.20240929074037.30"><vh>LB.do_Break</vh></v>
<v t="ekr.20240929074037.31"><vh>LB.do_Case</vh></v>
<v t="ekr.20240929074037.32"><vh>LB.do_CircumFlex</vh></v>
<v t="ekr.20240929074037.33"><vh>LB.do_CircumflexEqual</vh></v>
<v t="ekr.20240929074037.34"><vh>LB.do_Class</vh></v>
<v t="ekr.20240929074037.35"><vh>LB.do_Colon</vh></v>
<v t="ekr.20240929074037.36"><vh>LB.do_ColonEqual</vh></v>
<v t="ekr.20240929074037.37"><vh>LB.do_Comma</vh></v>
<v t="ekr.20240929074037.38"><vh>LB.do_Continue</vh></v>
<v t="ekr.20240929074037.39"><vh>LB.do_Def</vh></v>
<v t="ekr.20240929074037.40"><vh>LB.do_Del</vh></v>
<v t="ekr.20240929074037.41"><vh>LB.do_Dot</vh></v>
<v t="ekr.20240929074037.42"><vh>LB.do_DoubleSlash</vh></v>
<v t="ekr.20240929074037.43"><vh>LB.do_DoubleSlashEqual</vh></v>
<v t="ekr.20240929074037.44"><vh>LB.do_DoubleStar</vh></v>
<v t="ekr.20240929074037.45"><vh>LB.do_DoubleStarEqual</vh></v>
<v t="ekr.20240929074037.46"><vh>LB.do_Elif</vh></v>
<v t="ekr.20240929074037.47"><vh>LB.do_Ellipsis</vh></v>
<v t="ekr.20240929074037.48"><vh>LB.do_Else</vh></v>
<v t="ekr.20240929074037.49"><vh>LB.do_EndOfFile</vh></v>
<v t="ekr.20240929074037.50"><vh>LB.do_EqEqual</vh></v>
<v t="ekr.20240929074037.51"><vh>LB.do_Equal</vh></v>
<v t="ekr.20240929074037.52"><vh>LB.do_Except</vh></v>
<v t="ekr.20240929074037.53"><vh>LB.do_False</vh></v>
<v t="ekr.20240929074037.54"><vh>LB.do_Finally</vh></v>
<v t="ekr.20240929074037.55"><vh>LB.do_For</vh></v>
<v t="ekr.20240929074037.56"><vh>LB.do_From</vh></v>
<v t="ekr.20240929074037.57"><vh>LB.do_Global</vh></v>
<v t="ekr.20240929074037.58"><vh>LB.do_Greater</vh></v>
<v t="ekr.20240929074037.59"><vh>LB.do_GreaterEqual</vh></v>
<v t="ekr.20240929074037.60"><vh>LB.do_If</vh></v>
<v t="ekr.20240929074037.61"><vh>LB.do_Import</vh></v>
<v t="ekr.20240929074037.62"><vh>LB.do_In</vh></v>
<v t="ekr.20240929074037.63"><vh>LB.do_Is</vh></v>
<v t="ekr.20240929074037.64"><vh>LB.do_Lambda</vh></v>
<v t="ekr.20240929074037.65"><vh>LB.do_Lbrace</vh></v>
<v t="ekr.20240929074037.66"><vh>LB.do_LeftShift</vh></v>
<v t="ekr.20240929074037.67"><vh>LB.do_LeftShiftEqual</vh></v>
<v t="ekr.20240929074037.68"><vh>LB.do_Less</vh></v>
<v t="ekr.20240929074037.69"><vh>LB.do_LessEqual</vh></v>
<v t="ekr.20240929074037.70"><vh>LB.do_Lpar</vh></v>
<v t="ekr.20240929074037.71"><vh>LB.do_Lsqb</vh></v>
<v t="ekr.20240929074037.72"><vh>LB.do_Match</vh></v>
<v t="ekr.20240929074037.73"><vh>LB.do_Minus</vh></v>
<v t="ekr.20240929074037.74"><vh>LB.do_MinusEqual</vh></v>
<v t="ekr.20240929074037.75"><vh>LB.do_None</vh></v>
<v t="ekr.20240929074037.76"><vh>LB.do_Nonlocal</vh></v>
<v t="ekr.20240929074037.77"><vh>LB.do_Not</vh></v>
<v t="ekr.20240929074037.78"><vh>LB.do_NotEqual</vh></v>
<v t="ekr.20240929074037.79"><vh>LB.do_Or</vh></v>
<v t="ekr.20240929074037.80"><vh>LB.do_Pass</vh></v>
<v t="ekr.20240929074037.81"><vh>LB.do_Percent</vh></v>
<v t="ekr.20240929074037.82"><vh>LB.do_PercentEqual</vh></v>
<v t="ekr.20240929074037.83"><vh>LB.do_Plus</vh></v>
<v t="ekr.20240929074037.84"><vh>LB.do_PlusEqual</vh></v>
<v t="ekr.20240929074037.85"><vh>LB.do_Raise</vh></v>
<v t="ekr.20240929074037.86"><vh>LB.do_Rarrow</vh></v>
<v t="ekr.20240929074037.87"><vh>LB.do_Rbrace</vh></v>
<v t="ekr.20240929074037.88"><vh>LB.do_Return</vh></v>
<v t="ekr.20240929074037.89"><vh>LB.do_RightShift</vh></v>
<v t="ekr.20240929074037.90"><vh>LB.do_RightShiftEqual</vh></v>
<v t="ekr.20240929074037.91"><vh>LB.do_Rpar</vh></v>
<v t="ekr.20240929074037.92"><vh>LB.do_Rsqb</vh></v>
<v t="ekr.20240929074037.93"><vh>LB.do_Semi</vh></v>
<v t="ekr.20240929074037.94"><vh>LB.do_Slash</vh></v>
<v t="ekr.20240929074037.95"><vh>LB.do_SlashEqual</vh></v>
<v t="ekr.20240929074037.96"><vh>LB.do_Star</vh></v>
<v t="ekr.20240929074037.97"><vh>LB.do_StarEqual</vh></v>
<v t="ekr.20240929074037.98"><vh>LB.do_StartExpression</vh></v>
<v t="ekr.20240929074037.99"><vh>LB.do_StartInteractive</vh></v>
<v t="ekr.20240929074037.100"><vh>LB.do_StarModule</vh></v>
<v t="ekr.20240929074037.101"><vh>LB.do_Tilde</vh></v>
<v t="ekr.20240929074037.102"><vh>LB.do_True</vh></v>
<v t="ekr.20240929074037.103"><vh>LB.do_Try</vh></v>
<v t="ekr.20240929074037.104"><vh>LB.do_Type</vh></v>
<v t="ekr.20240929074037.105"><vh>LB.do_Vbar</vh></v>
<v t="ekr.20240929074037.106"><vh>LB.do_VbarEqual</vh></v>
<v t="ekr.20240929074037.107"><vh>LB.do_While</vh></v>
<v t="ekr.20240929074037.108"><vh>LB.do_With</vh></v>
<v t="ekr.20240929074037.109"><vh>LB.do_Yield</vh></v>
</v>
</v>
<v t="ekr.20240929074037.110"><vh>LB.enabled</vh></v>
<v t="ekr.20240929074037.111"><vh>LB.get_args</vh></v>
<v t="ekr.20240929074037.112"><vh>LB.make_input_list</vh>
<v t="ekr.20241002113506.1"><vh>&lt;&lt; Calculate class_name using match token &gt;&gt;</vh></v>
</v>
<v t="ekr.20240929074037.115"><vh>LB.show_args</vh></v>
<v t="ekr.20240929074037.116"><vh>LB.show_help</vh></v>
<v t="ekr.20241002163554.1"><vh>LB.string_to_static_str (not used)</vh></v>
</v>
<v t="ekr.20241004112826.1"><vh>class ParseState</vh>
<v t="ekr.20241004113118.1"><vh>&lt;&lt; docstring: ParseState &gt;&gt;</vh></v>
</v>
<v t="ekr.20240929074547.1"><vh>class Stats</vh>
<v t="ekr.20241001100954.1"><vh> Stats::new</vh></v>
<v t="ekr.20240929080242.1"><vh>Stats::fmt_ns</vh></v>
<v t="ekr.20240929075236.1"><vh>Stats::report</vh></v>
</v>
</v>
<v t="ekr.20241004073317.1"><vh>--- outer level</vh>
<v t="ekr.20241003093722.1"><vh>fn main</vh></v>
<v t="ekr.20240929074037.5"></v>
<v t="ekr.20240929074037.113"></v>
</v>
<v t="ekr.20241007144301.1"><vh>--- recent</vh>
<v t="ekr.20241007085552.1"></v>
<v t="ekr.20241007085705.1"></v>
<v t="ekr.20241004095735.1"></v>
<v t="ekr.20241005091217.1"></v>
<v t="ekr.20241005092549.1"></v>
<v t="ekr.20241004153802.1"></v>
<v t="ekr.20241004163018.1"></v>
</v>
</vnodes>
<tnodes>
<t tx="ekr.20240927151701.1"></t>
<t tx="ekr.20240927151701.100"></t>
<t tx="ekr.20240927151701.101"></t>
<t tx="ekr.20240927151701.102">vertical (v) or horizontal (h)

myLeoSettings.leo: vertical</t>
<t tx="ekr.20240927151701.103"></t>
<t tx="ekr.20240927151701.104"></t>
<t tx="ekr.20240927151701.105">@language rest
@wrap

See #3456.

</t>
<t tx="ekr.20240927151701.106"></t>
<t tx="ekr.20240927151701.107"></t>
<t tx="ekr.20240927151701.108"># leonine</t>
<t tx="ekr.20240927151701.109"></t>
<t tx="ekr.20240927151701.110"></t>
<t tx="ekr.20240927151701.111"></t>
<t tx="ekr.20240927151701.112"></t>
<t tx="ekr.20240927151701.113">True: (Recommended) Make a "Recovered Nodes" node whenever
Leo reads a file that has been changed outside of Leo.
</t>
<t tx="ekr.20240927151701.114"></t>
<t tx="ekr.20240927151701.115"></t>
<t tx="ekr.20240927151701.116"></t>
<t tx="ekr.20240927151701.117"></t>
<t tx="ekr.20240927151701.118"></t>
<t tx="ekr.20240927151701.119"></t>
<t tx="ekr.20240927151701.120"></t>
<t tx="ekr.20240927151701.121"></t>
<t tx="ekr.20240927151701.122"></t>
<t tx="ekr.20240927151701.123"></t>
<t tx="ekr.20240927151701.124"></t>
<t tx="ekr.20240927151701.126">Set to True to enable node appearance modifications
See tree-declutter-patterns
</t>
<t tx="ekr.20240927151701.144"></t>
<t tx="ekr.20240927151701.145">Only supported with the mod_tempfname.py plugin.

True: The plugin will store temporary files utilizing cleaner
file names (no unique number is appended to the node's headline text).
Unique temporary directory paths are used to insure unique files are
created by creating temporary directories reflecting each node's ancestor
nodes in the Leo outline. Note: Do not have multiple sibling nodes (nodes
having the same parent node) in Leo with the same headline text. There will
be a conflict if both are opened in an external editor at the same time.

False: The plugin will store temporary files with an appended
unique number to insure unique temporary filenames.
</t>
<t tx="ekr.20240927151701.146">True: check all @&lt;file&gt; nodes in the outline for changes in corresponding external files.</t>
<t tx="ekr.20240927151701.147"></t>
<t tx="ekr.20240927151701.148"></t>
<t tx="ekr.20240927151701.149"></t>
<t tx="ekr.20240927151701.150"></t>
<t tx="ekr.20240927151701.151"></t>
<t tx="ekr.20240927151701.152">It is *strange* to set this to True!</t>
<t tx="ekr.20240927151701.153">@language rest

To test #2041 &amp; #2094

The @bool use-find-dialog and @bool minibuffer-find-mode settings comprise
a tri-state setting, as shown in this table:
    
minibuffer-find-mode    use-find-dialog     mode: Ctrl-F puts focus in
--------------------    ---------------     --------------------------
    True                    Ignored         minibuffer
    False                   True            dialog
    False                   False           Find tab in the log pane

*All modes*

- Start the search with Ctrl-F (start-search).
- Enter the find pattern.
- (Optional) Use &lt;Tab&gt; to enter the search pattern.
- Use &lt;Enter&gt; to start the search.

*dialog and find tab modes*

- Non-functional "buttons" remind you of key bindings.

*minibuffer mode*

- Use Ctrl-G as always to leave the minibuffer.
- The Find tab is not made visible, but the status area shows the settings.</t>
<t tx="ekr.20240927151701.154">@language rest

The @bool use-find-dialog and @bool minibuffer-find-mode settings comprise
a tri-state setting, as shown in this table:
    
minibuffer-find-mode    use-find-dialog     mode: Ctrl-F puts focus in
--------------------    ---------------     --------------------------
    True                    Ignored         minibuffer
    False                   True            dialog
    False                   False           Find tab in the log pane

*All modes*

- Start the seas with Ctrl-F (start-search).
- Enter the find pattern.
- (Optional) Use &lt;Tab&gt; to enter the search pattern.
- Use &lt;Enter&gt; to start the search.

*dialog and find tab modes*

- Non-functional "buttons" remind you of key bindings.

*minibuffer mode*

- Use Ctrl-G as always to leave the minibuffer.
- The Find tab is not made visible, but the status area shows the settings.</t>
<t tx="ekr.20240927151701.155">Added on-popover to import-html-tags (for leovue)</t>
<t tx="ekr.20240927151701.156"># lowercase html tags, one per line.
# Adds ons-popover tag for LeoVue.

a
abbr
acronym
address
applet
area
b
base
basefont
bdo
big
blockquote
body
br
button
caption
center
cite
code
col
colgroup
dd
del
dfn
dir
div
dl
dt
em
fieldset
font
form
frame
frameset
head
h1
h2
h3
h4
h5
h6
hr
html
i
iframe
img
input
ins
kbd
label
legend
li
link
map
menu
meta
noframes
noscript
object
ol
ons-popover
optgroup
option
p
param
pre
q
s
samp
script
select
small
span
strike
strong
style
sub
sup
table
tbody
td
textarea
tfoot
th
thead
title
tr
tt
u
ul
var</t>
<t tx="ekr.20240927151701.157"># lowercase xml tags, one per line.

html
body
head
div
table
</t>
<t tx="ekr.20240927151701.170" annotate="7d71002858080000007072696f7269747971014d0f27580a000000707269736574646174657102580a000000323032312d30332d33307103752e"># Recommended plugins, from leoSettings.leo:

plugins_menu.py
mod_scripting.py
nav_qt.py
viewrendered.py


# contextmenu.py      # Required by the vim.py and xemacs.py plugins.
</t>
<t tx="ekr.20240927151701.176"></t>
<t tx="ekr.20240927151701.177"># True: show vr pane when opening a file.</t>
<t tx="ekr.20240927151701.178"># True: hide the vr pane for text-only renderings.</t>
<t tx="ekr.20240927151701.179"></t>
<t tx="ekr.20240927151701.184"></t>
<t tx="ekr.20240927151701.190">Only difference from myLeoSettings.leo

Note: EKRWinowsDark.leo defines comment1_font

All three @color settings work.
The @font setting does not work.
</t>
<t tx="ekr.20240927151701.191">Bold</t>
<t tx="ekr.20240927151701.192">Italics</t>
<t tx="ekr.20240927151701.193"></t>
<t tx="ekr.20240927151701.194"># bold keywords defined in forth-bold-words</t>
<t tx="ekr.20240927151701.195"></t>
<t tx="ekr.20240927151701.196"># Note: the default font size is 12.
rest_comment1_family = None
rest_comment1_size = 12pt
rest_comment1_slant = italic
rest_comment1_weight = None
</t>
<t tx="ekr.20240927151701.202"># Note: Use jj instead of escape to end insert mode.</t>
<t tx="ekr.20240927151701.203" __bookmarks="7d7100580700000069735f6475706571014930300a732e"></t>
<t tx="ekr.20240927151701.206">@language python

"""
Back up this .leo file.

os.environ['LEO_BACKUP'] must be the path to an existing (writable) directory.
"""
c.backup_helper(sub_dir='ekr-tbo-in-rust')
</t>
<t tx="ekr.20240927151701.207">@language python

print(p.gnx)</t>
<t tx="ekr.20240927151701.229">@language python
"""Recursively import all python files in a directory and clean the result."""
@tabwidth -4 # For a better match.
g.cls()
&lt;&lt; rust dir_list &gt;&gt;

dir_ = r'C:\Python\Python3.12\Lib\site-packages\coverage'
dir_ = r'C:\Python\Python3.12\Lib\site-packages\mypyc'
dir_ = r'C:\Python\Python3.12\Lib\site-packages\findimports.py'
dir_ = r'C:\Repos\ruff\crates'

c.recursiveImport(
    dir_=dir_,
    kind = '@clean', # '@auto', '@clean', '@nosent','@file',
    recursive = True,
    safe_at_file = True,
    # '.html', '.js', '.json', '.py', '.rs', '.svg', '.ts', '.tsx']
    # '.codon', '.cpp', '.cc', '.el', '.scm',
    theTypes = ['.py', 'rs'],
    verbose = False,
)
if 1:
    last = c.lastTopLevel()
    last.expand()
    if last.hasChildren():
        last.firstChild().expand()
    c.redraw(last)
print('Done')</t>
<t tx="ekr.20240927151701.230">dir_list = (
    r'C:\Repos\RustPython\common\src',
    r'C:\Repos\RustPython\compiler\codegen\src',
    r'C:\Repos\RustPython\compiler\core\src',
    r'C:\Repos\RustPython\compiler\src',
    r'C:\Repos\RustPython\compiler\codegen\src',  # compile.rs: AST to bytecode.
    r'C:\Repos\RustPython\compiler\core\src', # bytecode.rs: implements bytecodes.
    
    r'C:\Repos\RustPython\derive\src',
    r'C:\Repos\RustPython\derive-impl\src',
    r'C:\Repos\RustPython\pylib\src',
    r'C:\Repos\RustPython\src',
    r'C:\Repos\RustPython\stdlib\src',
    r'C:\Repos\RustPython\vm\src', # compiler.rs.
    r'C:\Repos\RustPython\vm\src\stdlib', # ast.rs  Also, many .rs versions of stdlib.
    r'C:\Repos\RustPython\vm\src\vm',  # compile.rs.
    r'C:\Repos\RustPython\vm\src\stdlib\ast', # gen.rs automatically generated by ast/asdl_rs.py.
)
</t>
<t tx="ekr.20240927151701.44"></t>
<t tx="ekr.20240927151701.45"></t>
<t tx="ekr.20240927151701.46">
</t>
<t tx="ekr.20240927151701.47"></t>
<t tx="ekr.20240927151701.48"># This node contains the commands needed to execute a program in a particular language.

# Format: language-name: command

# Create a temporary file if c.p is not any kind of @&lt;file&gt; node.

# Compute the final command as follows:

# 1. If command contains &lt;FILE&gt;, replace &lt;FILE&gt; with the full path to the external file.
# 2. If command contains &lt;NO-FILE&gt;, just remove &lt;NO-FILE&gt;.
# 3. Otherwise, append the full path to the external file to the command.

go: go run . &lt;NO-FILE&gt;
python: python
rust: rustc
</t>
<t tx="ekr.20240927151701.49"># This node contains the regex pattern to determine the line number in error messages.
# Format: language-name: regex pattern
#
# Patterns must define two groups, in either order:
# One group, containing only digits, defines the line number.
# The other group defines the file name.

go: ^\s*(.*):([0-9]+):([0-9]+):.+$
python: ^\s*File "(.+)", line ([0-9]+), in .+$
rust: ^\s*--&gt; (.+):([0-9]+):([0-9]+)\s*$</t>
<t tx="ekr.20240927151701.50">cargo-run
backup
</t>
<t tx="ekr.20240927151701.51"># legacy: (default) Leo's legacy layout
# big-tree: replaces @bool big-outline-pane</t>
<t tx="ekr.20240927151701.9" __bookmarks="7d7100580700000069735f6475706571014930300a732e">@language rest
@wrap

The @settings tree contains all active settings. 

Settings outside this tree have no effect.</t>
<t tx="ekr.20240927151701.92"></t>
<t tx="ekr.20240927151701.93"></t>
<t tx="ekr.20240927151701.94">True: same as recent_files_group, except that even files (basenames) which are unique
have their containing path listed in the submenu - so visual clutter is reduced
but you can still see where things come from before you load them.

False: don't use submenus for multiple path entries, unless recent_files_group
is true (and recent_files_omit_directories is False)
</t>
<t tx="ekr.20240927151701.95"></t>
<t tx="ekr.20240927151701.96">True: show user tips on startup.</t>
<t tx="ekr.20240927151701.97"></t>
<t tx="ekr.20240927151701.98"></t>
<t tx="ekr.20240927151701.99"></t>
<t tx="ekr.20240927152759.1">@language python
g.cls()
import os
import subprocess

if c.changed:
    c.save()
command = 'cargo run'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240927153018.1"></t>
<t tx="ekr.20240927154009.1"></t>
<t tx="ekr.20240927154016.1">@language rest
@nowrap
@nosearch

code:

ekr-tbo-in-rust: https://github.com/edreamleo/ekr-tbo-in-rust
ruff_python_parser: https://github.com/astral-sh/ruff/tree/main/crates/ruff_python_parser/src
lexer.rs: https://github.com/astral-sh/ruff/blob/main/crates/ruff_python_parser/src/lexer.rs

docs:
Rust Book: https://doc.rust-lang.org/stable/book/title-page.html

**Summary**

- LB::make_input_list is too slow.
  This code can not be significantly faster than Leo's python beautifier!
  
- LB:beautify is not the problem. It is fast enough.
</t>
<t tx="ekr.20240927154323.1">@language rest
@wrap
@nosearch

Post: Use pointers, not objects.
- Pointers keep borrow checker happy, provided lifetimes are given.

- Add index field to InputTok.
- Improve test1.py: it doesn't check enough.
- Finish LB:beautify.
</t>
<t tx="ekr.20240928073118.1">@language python
g.cls()
import os
import subprocess

if c.changed:
    c.save()
command = 'cargo fmt'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240928185643.1">@language rest
@wrap

Python (with extra tracing code in tbo.init_tokens_from_file:

&gt; python -c "import leo.core.leoTokens" --all --report leo\core\leoFrame.py
tbo: 0.03 sec. dirty: 0   checked: 1   beautified: 0   in leo\core\leoFrame.py

       read:   0.28 ms
make_tokens:  29.45 ms
      total:  29.73 ms
      
Rust, with nanosecond resolution.

=== Empty make_tokens loop.

leoFrame.py

     files: 1, tokens: 14619, ws tokens: 5156
       read:    0.5 ms
make_tokens:   10.7 ms  Empty loop
   beautify:    7.3 ms
      write:    0.0 ms
      total:   18.5 ms

=== Latest:

leoFrame.py

     files: 1, tokens: 14619, ws tokens: 5156
       read:    0.55 ms
make_tokens:   11.01 ms
   annotate:    6.85 ms
   beautify:    4.07 ms
      write:    0.00 ms
      total:   22.50 ms
      
Only about 10% faster than the Python beautifier.
</t>
<t tx="ekr.20240929024648.120">#[allow(dead_code)]
#[derive(Debug)]
struct InputTok&lt;'a&gt; {
    index: u32,
    kind: &amp;'a str,
    value: &amp;'a str,
}

impl &lt;'a&gt; InputTok&lt;'_&gt; {
    fn new(index: u32, kind: &amp;'a str, value: &amp;'a str) -&gt; InputTok&lt;'a&gt; {
        InputTok {
            index: index,
            kind: kind,
            value: value,
        }
    }
}
</t>
<t tx="ekr.20240929074037.1">#[derive(Debug)]
pub struct Beautifier {
    // Set in LB:beautify_one_file...
    args: Vec&lt;String&gt;,
    files_list: Vec&lt;String&gt;,
    stats: Stats,
    output_list: Vec&lt;String&gt;,
}

///// Temporary.
#[allow(dead_code)]
#[allow(non_snake_case)]
impl Beautifier {
    @others
}
</t>
<t tx="ekr.20240929074037.10">fn do_Complex(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Complex", tok_value);
}
</t>
<t tx="ekr.20240929074037.100">fn do_StartModule(&amp;mut self) {
    // self.add_output_string("StartModule", "");
    println!("do_StartModule");
}
</t>
<t tx="ekr.20240929074037.101">fn do_Tilde(&amp;mut self) {
    self.add_output_string("Tilde", "~");
}
</t>
<t tx="ekr.20240929074037.102">fn do_True(&amp;mut self) {
    self.add_output_string("True", "True");
}
</t>
<t tx="ekr.20240929074037.103">fn do_Try(&amp;mut self) {
    self.add_output_string("Try", "try");
}
</t>
<t tx="ekr.20240929074037.104">fn do_Type(&amp;mut self) {
    self.add_output_string("Type", "type");
}
</t>
<t tx="ekr.20240929074037.105">fn do_Vbar(&amp;mut self) {
    self.add_output_string("Vbar", "|");
}
</t>
<t tx="ekr.20240929074037.106">fn do_VbarEqual(&amp;mut self) {
    self.add_output_string("VbarEqual", "|=");
}
</t>
<t tx="ekr.20240929074037.107">fn do_While(&amp;mut self) {
    self.add_output_string("While", "while");
}
</t>
<t tx="ekr.20240929074037.108">fn do_With(&amp;mut self) {
    self.add_output_string("With", "with");
}
</t>
<t tx="ekr.20240929074037.109">fn do_Yield(&amp;mut self) {
    self.add_output_string("Yield", "yield");
}
</t>
<t tx="ekr.20240929074037.11">fn do_Float(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Float", tok_value);
}
</t>
<t tx="ekr.20240929074037.110">fn enabled(&amp;self, arg: &amp;str) -&gt; bool {
    //! Beautifier::enabled: return true if the given command-line argument is enabled.
    //! Example:  x.enabled("--report");
    return self.args.contains(&amp;arg.to_string());
}
</t>
<t tx="ekr.20240929074037.111">fn get_args(&amp;mut self) {
    //! Beautifier::get_args: Set the args and files_list ivars.
    let args: Vec&lt;String&gt; = env::args().collect();
    let valid_args = vec![
        "--all",
        "--beautified",
        "--diff",
        "-h",
        "--help",
        "--report",
        "--write",
    ];
    for (i, arg) in args.iter().enumerate() {
        if i &gt; 0 {
            if valid_args.contains(&amp;arg.as_str()) {
                self.args.push(arg.to_string())
            } else if arg.as_str().starts_with("--") || arg.as_str().starts_with("--") {
                println!("Ignoring invalid arg: {arg}");
            } else {
                println!("File: {arg}");
                self.files_list.push(arg.to_string());
            }
        }
    }
}
</t>
<t tx="ekr.20240929074037.112">fn make_input_list&lt;'a&gt;(&amp;mut self, contents: &amp;'a str) -&gt; Vec&lt;InputTok&lt;'a&gt;&gt; {
    //! Return an input_list from the tokens given by the RustPython lex.
    let mut n_tokens: u64 = 0;
    let mut n_ws_tokens: u64 = 0;
    let mut prev_start: usize = 0;
    let mut result: Vec&lt;InputTok&gt; = Vec::new();
    let mut index: u32 = 0;
    for token_tuple in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        use Tok::*;
        let (token, range) = token_tuple;
        let tok_value = &amp;contents[range];

        // The gem: create a whitespace pseudo-tokens.
        // This code adds maybe about 1 ms when beautifying leoFrame.py.
        // With the gem: 14.1 - 14.5 ms. Without: 13.1 - 13.7 ms.
        let start_i = usize::from(range.start());
        let end_i = usize::from(range.end());
        if start_i &gt; prev_start {
            let ws = &amp;contents[prev_start..start_i];
            result.push(InputTok::new(index, "ws", ws));
            n_ws_tokens += 1
        }
        prev_start = end_i;

        &lt;&lt; Calculate class_name using match token &gt;&gt;
        n_tokens += 1;
        result.push(InputTok::new(index, class_name, tok_value));
        index += 1;
    }
    // Update counts.
    self.stats.n_tokens += n_tokens;
    self.stats.n_ws_tokens += n_ws_tokens;
    return result;
}
</t>
<t tx="ekr.20240929074037.113">fn beautify(&amp;mut self, annotated_tokens: &amp;Vec&lt;AnnotatedInputTok&gt;) -&gt; String {
    //! Beautify the annotated, creating the output String.

    // Init ivars first.
    // self.input_token = None;  // ???
    // self.pending_lws = "";  // ???
    // self.pending_ws = "";  // ???
    // self.prev_output_kind = None;    // ???
    // self.prev_output_value = None;  // ???

    // // Init state.
    // self.gen_token("file-start", "");
    // self.push_state("file-start");

    // // The main loop:
    // prev_line_number: i32 = 0;
    // for (self.index, self.input_token) in enumerate(input_tokens):
        // // Set global for visitors.
        // if prev_line_number != self.input_token.line_number {
            // prev_line_number = self.input_token.line_number;
        // }
        // // Call the proper visitor.
        // if self.verbatim {
            // self.do_verbatim();
        // } else {
            // // Use match ???
            // func = getattr(self, f"do_{self.input_token.kind}", self.no_visitor)
            // func()
        // }
        
    for annotated_token in annotated_tokens {
        // println!("beautify: {annotated_token:?}");
        &lt;&lt; LB: beautify: dispatch on annotated_token.kind &gt;&gt;
    }

    let mut result = String::new();
    for output_token in &amp;self.output_list {
        result.push_str(output_token);
    }
    if false {
        let n = result.len();
        println!("result: {n} characters");
    }
    return result;
}
</t>
<t tx="ekr.20240929074037.114">pub fn new() -&gt; Beautifier {
    let mut x = Beautifier {
        // Set in beautify_one_file
        args: Vec::new(),
        files_list: Vec::new(),
        output_list: Vec::new(),
        stats: Stats::new(),
    };
    x.get_args();
    return x;
}
</t>
<t tx="ekr.20240929074037.115">fn show_args(&amp;self) {
    println!("Command-line arguments...");
    for (i, arg) in self.args.iter().enumerate() {
        if i &gt; 0 {
            println!("  {arg}");
        }
    }
    for file_arg in self.files_list.iter() {
        println!("  {file_arg}");
    }
}
</t>
<t tx="ekr.20240929074037.116">fn show_help(&amp;self) {
    //! Beautifier::show_help: print the help messages.
    println!(
        "{}",
        textwrap::dedent(
            "
        Beautify or diff files.

        -h --help:      Print this help message and exit.
        --all:          Beautify all files, even unchanged files.
        --beautified:   Report beautified files individually, even if not written.
        --diff:         Show diffs instead of changing files.
        --report:       Print summary report.
        --write:        Write beautifed files (dry-run mode otherwise).
    "
        )
    );
}
</t>
<t tx="ekr.20240929074037.12">fn do_Int(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Int", tok_value);
}
</t>
<t tx="ekr.20240929074037.13">fn do_Name(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Name", tok_value);
}
</t>
<t tx="ekr.20240929074037.14">fn do_String(&amp;mut self, tok_value: &amp;str) {
    // correct.
    // print!("{tok_value}");

    // incorrect.
    // let quote = if *triple_quoted {"'''"} else {"'"};
    // print!("{:?}:{quote}{value}{quote}", kind);

    self.add_output_string("String", tok_value);
}
</t>
<t tx="ekr.20240929074037.15"></t>
<t tx="ekr.20240929074037.16">fn do_Dedent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Dedent", tok_value);
}
</t>
<t tx="ekr.20240929074037.17">fn do_Indent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Indent", tok_value);
}
</t>
<t tx="ekr.20240929074037.18">fn do_Newline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240929074037.19">fn do_NonLogicalNewline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240929074037.2">#[allow(unused_variables)]
fn add_output_string(&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add value to the output list.
    //! kind is for debugging.
    if !value.is_empty() {
        self.output_list.push(value.to_string())
    }
}
</t>
<t tx="ekr.20240929074037.20"></t>
<t tx="ekr.20240929074037.21">fn do_Amper(&amp;mut self) {
    self.add_output_string("Amper", "&amp;");
}
</t>
<t tx="ekr.20240929074037.22">fn do_AmperEqual(&amp;mut self) {
    self.add_output_string("AmperEqual", "&amp;=");
}
</t>
<t tx="ekr.20240929074037.23">fn do_And(&amp;mut self) {
    self.add_output_string("And", "and");
}
</t>
<t tx="ekr.20240929074037.24">fn do_As(&amp;mut self) {
    self.add_output_string("As", "as");
}
</t>
<t tx="ekr.20240929074037.25">fn do_Assert(&amp;mut self) {
    self.add_output_string("Assert", "assert");
}
</t>
<t tx="ekr.20240929074037.26">fn do_Async(&amp;mut self) {
    self.add_output_string("Async", "async");
}
</t>
<t tx="ekr.20240929074037.27">fn do_At(&amp;mut self) {
    self.add_output_string("At", "@");
}
</t>
<t tx="ekr.20240929074037.28">fn do_AtEqual(&amp;mut self) {
    self.add_output_string("AtEqual", "@=");
}
</t>
<t tx="ekr.20240929074037.29">fn do_Await(&amp;mut self) {
    self.add_output_string("Await", "await");
}
</t>
<t tx="ekr.20240929074037.30">fn do_Break(&amp;mut self) {
    self.add_output_string("Break", "break");
}
</t>
<t tx="ekr.20240929074037.31">fn do_Case(&amp;mut self) {
    self.add_output_string("Case", "case");
}
</t>
<t tx="ekr.20240929074037.32">fn do_CircumFlex(&amp;mut self) {
    self.add_output_string("CircumFlex", "^");
}
</t>
<t tx="ekr.20240929074037.33">fn do_CircumflexEqual(&amp;mut self) {
    self.add_output_string("CircumflexEqual", "^=");
}
</t>
<t tx="ekr.20240929074037.34">fn do_Class(&amp;mut self) {
    self.add_output_string("Class", "class");
}
</t>
<t tx="ekr.20240929074037.35">fn do_Colon(&amp;mut self) {
    self.add_output_string("Colon", ":");
}
</t>
<t tx="ekr.20240929074037.36">fn do_ColonEqual(&amp;mut self) {
    self.add_output_string("ColonEqual", ":=");
}
</t>
<t tx="ekr.20240929074037.37">fn do_Comma(&amp;mut self) {
    self.add_output_string("Comma", ",");
}
</t>
<t tx="ekr.20240929074037.38">fn do_Continue(&amp;mut self) {
    self.add_output_string("Continue", "continue");
}
</t>
<t tx="ekr.20240929074037.39">fn do_Def(&amp;mut self) {
    self.add_output_string("Def", "def");
}
</t>
<t tx="ekr.20240929074037.4">pub fn beautify_all_files(&amp;mut self) {
    // for file_name in self.files_list.clone() {
    for file_name in self.files_list.clone() {
        self.beautify_one_file(&amp;file_name);
    }
}

</t>
<t tx="ekr.20240929074037.40">fn do_Del(&amp;mut self) {
    self.add_output_string("Del", "del");
}
</t>
<t tx="ekr.20240929074037.41">fn do_Dot(&amp;mut self) {
    self.add_output_string("Dot", ".");
}
</t>
<t tx="ekr.20240929074037.42">fn do_DoubleSlash(&amp;mut self) {
    self.add_output_string("DoubleSlash", "//");
}
</t>
<t tx="ekr.20240929074037.43">fn do_DoubleSlashEqual(&amp;mut self) {
    self.add_output_string("DoubleSlashEqual", "//=");
}
</t>
<t tx="ekr.20240929074037.44">fn do_DoubleStar(&amp;mut self) {
    self.add_output_string("DoubleStar", "**");
}
</t>
<t tx="ekr.20240929074037.45">fn do_DoubleStarEqual(&amp;mut self) {
    self.add_output_string("DoubleStarEqual", "**=");
}
</t>
<t tx="ekr.20240929074037.46">fn do_Elif(&amp;mut self) {
    self.add_output_string("Elif", "elif");
}
</t>
<t tx="ekr.20240929074037.47">fn do_Ellipsis(&amp;mut self) {
    self.add_output_string("Ellipsis", "...");
}
</t>
<t tx="ekr.20240929074037.48">fn do_Else(&amp;mut self) {
    self.add_output_string("Else", "else");
}
</t>
<t tx="ekr.20240929074037.49">fn do_EndOfFile(&amp;mut self) {
    self.add_output_string("EndOfFile", "EOF");
}
</t>
<t tx="ekr.20240929074037.5">fn beautify_one_file(&amp;mut self, file_name: &amp;str) {
    self.stats.n_files += 1;
    if true {
        // Testing only: print the short file name.
        let file_path = path::Path::new(file_name);
        let os_str = file_path.file_name().unwrap(); // &amp;OsStr
        let short_file_name = os_str.to_str().unwrap();
        println!("{short_file_name}");
    }
    // Read the file into contents (a String).
    let t1 = std::time::Instant::now();
    let contents = fs::read_to_string(file_name).expect("Error reading{file_name}");
    self.stats.read_time += t1.elapsed().as_nanos();
    // Create (an immutable!) list of input tokens.
    let t2 = std::time::Instant::now();
    let input_tokens = self.make_input_list(&amp;contents);
    self.stats.make_tokens_time += t2.elapsed().as_nanos();
    // Annotate tokens (the prepass).
    let t3 = std::time::Instant::now();
    let mut annotator = Annotator::new(&amp;input_tokens);
    let annotated_tokens = annotator.annotate();
    self.stats.annotation_time += t3.elapsed().as_nanos();
    // Beautify.
    let t4 = std::time::Instant::now();
    self.beautify(&amp;annotated_tokens);
    self.stats.beautify_time += t4.elapsed().as_nanos();
}
</t>
<t tx="ekr.20240929074037.50">fn do_EqEqual(&amp;mut self) {
    self.add_output_string("EqEqual", "==");
}
</t>
<t tx="ekr.20240929074037.51">fn do_Equal(&amp;mut self) {
    self.add_output_string("Equal", "=");
}
</t>
<t tx="ekr.20240929074037.52">fn do_Except(&amp;mut self) {
    self.add_output_string("Except", "except");
}
</t>
<t tx="ekr.20240929074037.53">fn do_False(&amp;mut self) {
    self.add_output_string("False", "False");
}
</t>
<t tx="ekr.20240929074037.54">fn do_Finally(&amp;mut self) {
    self.add_output_string("Finally", "finally");
}
</t>
<t tx="ekr.20240929074037.55">fn do_For(&amp;mut self) {
    self.add_output_string("For", "for");
}
</t>
<t tx="ekr.20240929074037.56">fn do_From(&amp;mut self) {
    self.add_output_string("From", "from");
}
</t>
<t tx="ekr.20240929074037.57">fn do_Global(&amp;mut self) {
    self.add_output_string("Global", "global");
}
</t>
<t tx="ekr.20240929074037.58">fn do_Greater(&amp;mut self) {
    self.add_output_string("Greater", "&gt;");
}
</t>
<t tx="ekr.20240929074037.59">fn do_GreaterEqual(&amp;mut self) {
    self.add_output_string("GreaterEqual", "&gt;-");
}
</t>
<t tx="ekr.20240929074037.60">fn do_If(&amp;mut self) {
    self.add_output_string("If", "if");
}
</t>
<t tx="ekr.20240929074037.61">fn do_Import(&amp;mut self) {
    self.add_output_string("Import", "import");
}
</t>
<t tx="ekr.20240929074037.62">fn do_In(&amp;mut self) {
    self.add_output_string("In", "in");
}
</t>
<t tx="ekr.20240929074037.63">fn do_Is(&amp;mut self) {
    self.add_output_string("Is", "is");
}
</t>
<t tx="ekr.20240929074037.64">fn do_Lambda(&amp;mut self) {
    self.add_output_string("Lambda", "lambda");
}
</t>
<t tx="ekr.20240929074037.65">fn do_Lbrace(&amp;mut self) {
    self.add_output_string("Lbrace", "[");
}
</t>
<t tx="ekr.20240929074037.66">fn do_LeftShift(&amp;mut self) {
    self.add_output_string("LeftShift", "&lt;&lt;");
}
</t>
<t tx="ekr.20240929074037.67">fn do_LeftShiftEqual(&amp;mut self) {
    self.add_output_string("LeftShiftEqual", "&lt;&lt;=");
}
</t>
<t tx="ekr.20240929074037.68">fn do_Less(&amp;mut self) {
    self.add_output_string("Less", "&lt;");
}
</t>
<t tx="ekr.20240929074037.69">fn do_LessEqual(&amp;mut self) {
    self.add_output_string("LessEqual", "&lt;=");
}
</t>
<t tx="ekr.20240929074037.7"></t>
<t tx="ekr.20240929074037.70">fn do_Lpar(&amp;mut self) {
    self.add_output_string("Lpar", "(");
}
</t>
<t tx="ekr.20240929074037.71">fn do_Lsqb(&amp;mut self) {
    self.add_output_string("Lsqb", "[");
}
</t>
<t tx="ekr.20240929074037.72">fn do_Match(&amp;mut self) {
    self.add_output_string("Match", "match");
}
</t>
<t tx="ekr.20240929074037.73">fn do_Minus(&amp;mut self) {
    self.add_output_string("Minus", "-");
}
</t>
<t tx="ekr.20240929074037.74">fn do_MinusEqual(&amp;mut self) {
    self.add_output_string("MinusEqual", "-=");
}
</t>
<t tx="ekr.20240929074037.75">fn do_None(&amp;mut self) {
    self.add_output_string("None", "None");
}
</t>
<t tx="ekr.20240929074037.76">fn do_Nonlocal(&amp;mut self) {
    self.add_output_string("Nonlocal", "nonlocal");
}
</t>
<t tx="ekr.20240929074037.77">fn do_Not(&amp;mut self) {
    self.add_output_string("Not", "not");
}
</t>
<t tx="ekr.20240929074037.78">fn do_NotEqual(&amp;mut self) {
    self.add_output_string("NotEqual", "!=");
}
</t>
<t tx="ekr.20240929074037.79">fn do_Or(&amp;mut self) {
    self.add_output_string("Or", "or");
}
</t>
<t tx="ekr.20240929074037.8"></t>
<t tx="ekr.20240929074037.80">fn do_Pass(&amp;mut self) {
    self.add_output_string("Pass", "pass");
}
</t>
<t tx="ekr.20240929074037.81">fn do_Percent(&amp;mut self) {
    self.add_output_string("Percent", "%");
}
</t>
<t tx="ekr.20240929074037.82">fn do_PercentEqual(&amp;mut self) {
    self.add_output_string("PercentEqual", "%=");
}
</t>
<t tx="ekr.20240929074037.83">fn do_Plus(&amp;mut self) {
    self.add_output_string("Plus", "+");
}
</t>
<t tx="ekr.20240929074037.84">fn do_PlusEqual(&amp;mut self) {
    self.add_output_string("PlusEqual", "+=");
}
</t>
<t tx="ekr.20240929074037.85">fn do_Raise(&amp;mut self) {
    self.add_output_string("Raise", "raise");
}
</t>
<t tx="ekr.20240929074037.86">fn do_Rarrow(&amp;mut self) {
    self.add_output_string("Rarrow", "-&gt;");
}
</t>
<t tx="ekr.20240929074037.87">fn do_Rbrace(&amp;mut self) {
    self.add_output_string("Rbrace", "]");
}
</t>
<t tx="ekr.20240929074037.88">fn do_Return(&amp;mut self) {
    self.add_output_string("Return", "return");
}
</t>
<t tx="ekr.20240929074037.89">fn do_RightShift(&amp;mut self) {
    self.add_output_string("RightShift", "&gt;&gt;");
}
</t>
<t tx="ekr.20240929074037.9">fn do_Comment(&amp;mut self, tok_value: &amp;str) {
    // print!("{tok_value}");  // Correct.
    // print!("{value} ");  // Wrong!
    self.add_output_string("Comment", tok_value);
}
</t>
<t tx="ekr.20240929074037.90">fn do_RightShiftEqual(&amp;mut self) {
    self.add_output_string("RightShiftEqual", "&gt;&gt;=");
}
</t>
<t tx="ekr.20240929074037.91">fn do_Rpar(&amp;mut self) {
    self.add_output_string("Rpar", ")");
}
</t>
<t tx="ekr.20240929074037.92">fn do_Rsqb(&amp;mut self) {
    self.add_output_string("Rsqb", "]");
}
</t>
<t tx="ekr.20240929074037.93">fn do_Semi(&amp;mut self) {
    self.add_output_string("Semi", ";");
}
</t>
<t tx="ekr.20240929074037.94">fn do_Slash(&amp;mut self) {
    self.add_output_string("Slash", "/");
}
</t>
<t tx="ekr.20240929074037.95">fn do_SlashEqual(&amp;mut self) {
    self.add_output_string("SlashEqual", "/=");
}
</t>
<t tx="ekr.20240929074037.96">fn do_Star(&amp;mut self) {
    self.add_output_string("Star", "*");
}
</t>
<t tx="ekr.20240929074037.97">fn do_StarEqual(&amp;mut self) {
    self.add_output_string("StarEqual", "*=");
}
</t>
<t tx="ekr.20240929074037.98">fn do_StartExpression(&amp;mut self) {
    // self.add_output_string("StartExpression", "");
}
</t>
<t tx="ekr.20240929074037.99">fn do_StartInteractive(&amp;mut self) {
    // self.add_output_string("StartModule", "");
}
</t>
<t tx="ekr.20240929074547.1">// Allow unused write_time
#[allow(dead_code)]
#[derive(Debug)]
pub struct Stats {
    // Cumulative statistics for all files.
    n_files: u64,     // Number of files.
    n_tokens: u64,    // Number of tokens.
    n_ws_tokens: u64, // Number of pseudo-ws tokens.

    // Timing stat, in microseconds...
    annotation_time: u128,
    beautify_time: u128,
    make_tokens_time: u128,
    read_time: u128,
    write_time: u128,
}

// Calling Stats.report is optional.
#[allow(dead_code)]
impl Stats {
    @others
}
</t>
<t tx="ekr.20240929075236.1">fn report(&amp;mut self) {
    // Cumulative counts.
    let n_files = self.n_files;
    let n_tokens = self.n_tokens;
    let n_ws_tokens = self.n_ws_tokens;
    // Print cumulative timing stats, in ms.
    let annotation_time = self.fmt_ns(self.annotation_time);
    let beautify_time = self.fmt_ns(self.beautify_time);
    let make_tokens_time = self.fmt_ns(self.make_tokens_time);
    let read_time = self.fmt_ns(self.read_time);
    let write_time = self.fmt_ns(self.write_time);
    let total_time_ns = self.annotation_time
        + self.beautify_time
        + self.make_tokens_time
        + self.read_time
        + self.write_time;
    let total_time = self.fmt_ns(total_time_ns);
    println!("");
    println!("     files: {n_files}, tokens: {n_tokens}, ws tokens: {n_ws_tokens}");
    println!("       read: {read_time:&gt;7} ms");
    println!("make_tokens: {make_tokens_time:&gt;7} ms");
    println!("   annotate: {annotation_time:&gt;7} ms");
    println!("   beautify: {beautify_time:&gt;7} ms");
    println!("      write: {write_time:&gt;7} ms");
    println!("      total: {total_time:&gt;7} ms");
}
</t>
<t tx="ekr.20240929080242.1">fn fmt_ns(&amp;mut self, t: u128) -&gt; String {
    //! Convert nanoseconds to fractional milliseconds.
    let ms = t / 1000000;
    let micro = (t % 1000000) / 10000; // 2-places only.
                                       // println!("t: {t:8} ms: {ms:03} micro: {micro:02}");
    return f!("{ms:4}.{micro:02}");
}

</t>
<t tx="ekr.20240929084852.1">@language rest
@wrap

Whether a value is on the stack or the heap affects how the language behaves.

Rules:
- Each value in Rust has an *owner*.
- There can only be one owner at a time.
- When the owner goes out of scope, the value will be dropped.
  **The scope ends when value last used**.

String literals can't be mutated.  String objects are on the heap and can be mutated.
The *drop* function releases heap objects.

Move is a shallow copy:
@language rust
    let s1 = String::from("hello");
    let s2 = s1;
    println!("{s1}, world!");  // wrong.
@language rest

*clone* makes a deep copy.

*small (stack-only) values are copied when passed as params*.

Stack-only values have *copy trait*.
Can't add `copy` trait if object implements `drop`.

Ownership and functions:

Passing a variable to a function will move or copy, just as assignment does. 

Return Values and Scope:

Returning values can also transfer ownership.

References and borrowing:

'&amp;' represents a reference.
References are immutable by defaault.
If you have a mutable reference to a value, you can have no other references to that value. 

Rules of reference:

- At any time, you can have either one mutable reference or any number of immutable references.
- References must always be valid.
</t>
<t tx="ekr.20240930063514.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git commit'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930063546.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git status'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930063740.1">@language python
import os
import subprocess

if c.changed:
    c.save()
for command in ('git add *.rs', 'git status'):
    subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930064435.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git reset'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930064622.1">@language python
import os
import subprocess
if c.changed:
    c.save()
command = 'git push'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930101156.1">@language python
import os
import subprocess

if c.changed:
    c.save()
for command in ('git add *.leo', 'git status'):
    subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20241001055017.1">@language rust

// A trait defines the functionality a particular type has and can share with other types. 
// Traits are like interfaces.

pub trait Summary {
    fn summarize(&amp;self) -&gt; String;
}

impl Summary for NewsArticle {
    fn summarize(&amp;self) -&gt; String {
        format!("{}, by {} ({})",, self.author, self.location)
    }
}

// Traits can have default implementations for some or all functions.
pub trait Summary {
    fn summarize_author(&amp;self) -&gt; String;

    fn summarize(&amp;self) -&gt; String {
        format!("(Read more from {}...)", self.summarize_author())
    }
}

// ***&amp; Traits as parameters: var: &amp;impl TraitName.

pub fn notify(item1: &amp;impl Summary, item2: &amp;impl Summary) {...}

// *** Trait-bound syntax.  &lt;T: Trait&gt;(item: &amp;T).

pub fn notify&lt;T: Summary&gt;(item1: &amp;T, item2: &amp;T) {...}

// *** Multiple trait bounds Trail + Trait

pub fn notify(item: &amp;(impl Summary + Display)) {...}

pub fn notify&lt;T: Summary + Display&gt;(item: &amp;T) {...}

// *** Where syntax:

fn some_function&lt;T: Display + Clone, U: Clone + Debug&gt;(t: &amp;T, u: &amp;U) -&gt; i32 {...}

fn some_function&lt;T, U&gt;(t: &amp;T, u: &amp;U) -&gt; i32
where
    T: Display + Clone,
    U: Clone + Debug,
{...}

// *** Returning types that implement traits:

// The function can only return a single type.

fn returns_summarizable() -&gt; impl Summary {...}

// The ability to specify a return type only by the trait it implements is
// especially useful in the context of closures and iterators.

// We can call o.to_string method defined by the ToString trait on
// any type that implements the Display trait. 
</t>
<t tx="ekr.20241001093308.1">fn test() {
    test_vec();
    test_struct();
}
</t>
<t tx="ekr.20241001100954.1">pub fn new() -&gt; Stats {
    let x = Stats {
        // Cumulative counts.
        n_files: 0,     // Number of files.
        n_tokens: 0,    // Number of tokens.
        n_ws_tokens: 0, // Number of pseudo-ws tokens.

        // Timing stats, in nanoseconds...
        annotation_time: 0,
        beautify_time: 0,
        make_tokens_time: 0,
        read_time: 0,
        write_time: 0,
    };
    return x;
}
</t>
<t tx="ekr.20241001104914.1"></t>
<t tx="ekr.20241002062655.1">let kind = annotated_token.kind;
let value = annotated_token.kind;
match kind {
    // Some of these could be replaced by inline code.
    "And" =&gt; self.do_And(),
    "As" =&gt; self.do_As(),
    "Assert" =&gt; self.do_Assert(),
    "At" =&gt; self.do_At(),
    "Break" =&gt; self.do_Break(),
    "Class" =&gt; self.do_Class(),
    "Colon" =&gt; self.do_Colon(),
    "ColonEqual" =&gt; self.do_ColonEqual(),
    "Comma" =&gt; self.do_Comma(),
    "Comment" =&gt; self.do_Comment(value),
    "Complex" =&gt; self.do_Complex(value),
    "Continue" =&gt; self.do_Continue(),
    "Dedent" =&gt; self.do_Dedent(value),
    "Def" =&gt; self.do_Def(),
    "Del" =&gt; self.do_Del(),
    "Dot" =&gt; self.do_Dot(),
    "DoubleStar" =&gt; self.do_DoubleStar(),
    "Elif" =&gt; self.do_Elif(),
    "Else" =&gt; self.do_Else(),
    "Equal" =&gt; self.do_Equal(),
    "EqEqual" =&gt; self.do_EqEqual(),
    "Except" =&gt; self.do_Except(),
    "Greater" =&gt; self.do_Greater(),
    "GreaterEqual" =&gt; self.do_GreaterEqual(),
    "False" =&gt; self.do_False(),
    "Finally" =&gt; self.do_Finally(),
    "Float" =&gt; self.do_Float(value),
    "For" =&gt; self.do_For(),
    "From" =&gt; self.do_From(),
    "If" =&gt; self.do_If(),
    "In" =&gt; self.do_In(),
    "Import" =&gt; self.do_Import(),
    "Indent" =&gt; self.do_Indent(value),
    "Int" =&gt; self.do_Int(value),
    "Is" =&gt; self.do_Is(),
    "Less" =&gt; self.do_Less(),
    "LessEqual" =&gt; self.do_LessEqual(),
    "Lbrace" =&gt; self.do_Lbrace(),
    "Lpar" =&gt; self.do_Lpar(),
    "Lsqb" =&gt; self.do_Lsqb(),
    "Minus" =&gt; self.do_Minus(),
    "MinusEqual" =&gt; self.do_MinusEqual(),
    "Name" =&gt; self.do_Name(value),
    "Newline" =&gt; self.do_Newline(),
    "None" =&gt; self.do_None(),
    "NonLogicalNewline" =&gt; self.do_NonLogicalNewline(),
    "Not" =&gt; self.do_Not(),
    "NotEqual" =&gt; self.do_NotEqual(),
    "Or" =&gt; self.do_Or(),
    "Pass" =&gt; self.do_Pass(),
    "Percent" =&gt; self.do_Percent(),
    "Plus" =&gt; self.do_Plus(),
    "PlusEqual" =&gt; self.do_PlusEqual(),
    "Raise" =&gt; self.do_Raise(),
    "Rarrow" =&gt; self.do_Rarrow(),
    "Rbrace" =&gt; self.do_Rbrace(),
    "Return" =&gt; self.do_Return(),
    "Rpar" =&gt; self.do_Rpar(),
    "Rsqb" =&gt; self.do_Rsqb(),
    "Star" =&gt; self.do_Star(),
    "String" =&gt; self.do_String(value),
    "True" =&gt; self.do_True(),
    "Try" =&gt; self.do_Try(),
    "While" =&gt; self.do_While(),
    "With" =&gt; self.do_With(),
    "ws" =&gt; self.do_ws(kind, value),
    _ =&gt; println!("No visitor for: {kind}"),
}
</t>
<t tx="ekr.20241002071143.1">// *** Temporary
#[allow(unused_variables)]
fn do_ws(&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Handle the "ws" pseudo-token.
    //! Put the whitespace only if if ends with backslash-newline.

    // To do.

    // let last_token = self.input_tokens[self.index - 1];
    // let is_newline = kind in ("nl", "newline");
    // if is_newline {
    // self.pending_lws = val;
    // self.pending_ws = "";
    // }
    // else if "\\\n" in val {
    // self.pending_lws = "";
    // self.pending_ws = val;
    // }
    // else {
    // self.pending_ws = val
    // }
}
</t>
<t tx="ekr.20241002113506.1">// Variant names are necessary, but otherwise not used.
#[allow(unused_variables)]
let class_name = match token {
    // Tokens with values...
    Comment(value) =&gt; "Comment",
    Complex { real, imag } =&gt; "Complex",
    Float { value } =&gt; "Float",
    Int { value } =&gt; "Int",
    Name { name } =&gt; "Name",
    Tok::String {
        value,
        kind,
        triple_quoted,
    } =&gt; "String",

    // Common tokens...
    Class =&gt; "Class",
    Dedent =&gt; "Dedent",
    Def =&gt; "Def",
    Indent =&gt; "Indent",
    Newline =&gt; "Newline",
    NonLogicalNewline =&gt; "NonLogicalNewline",

    // All other tokens...
    Amper =&gt; "Amper",
    AmperEqual =&gt; "AmperEqual",
    And =&gt; "And",
    As =&gt; "As",
    Assert =&gt; "Assert",
    Async =&gt; "Async",
    At =&gt; "At",
    AtEqual =&gt; "AtEqual",
    Await =&gt; "Await",
    Break =&gt; "Break",
    Case =&gt; "Case",
    CircumFlex =&gt; "CircumFlex",
    CircumflexEqual =&gt; "CircumflexEqual",
    Colon =&gt; "Colon",
    ColonEqual =&gt; "ColonEqual",
    Comma =&gt; "Comma",
    Continue =&gt; "Continue",
    Del =&gt; "Del",
    Dot =&gt; "Dot",
    DoubleSlash =&gt; "DoubleSlash",
    DoubleSlashEqual =&gt; "DoubleSlashEqual",
    DoubleStar =&gt; "DoubleStar",
    DoubleStarEqual =&gt; "DoubleStarEqual",
    Elif =&gt; "Elif",
    Ellipsis =&gt; "Ellipsis",
    Else =&gt; "Else",
    EndOfFile =&gt; "EndOfFile",
    EqEqual =&gt; "EqEqual",
    Equal =&gt; "Equal",
    Except =&gt; "Except",
    False =&gt; "False",
    Finally =&gt; "Finally",
    For =&gt; "For",
    From =&gt; "From",
    Global =&gt; "Global",
    Greater =&gt; "Greater",
    GreaterEqual =&gt; "GreaterEqual",
    If =&gt; "If",
    Import =&gt; "Import",
    In =&gt; "In",
    Is =&gt; "Is",
    Lambda =&gt; "Lambda",
    Lbrace =&gt; "Lbrace",
    LeftShift =&gt; "LeftShift",
    LeftShiftEqual =&gt; "LeftShiftEqual",
    Less =&gt; "Less",
    LessEqual =&gt; "LessEqual",
    Lpar =&gt; "Lpar",
    Lsqb =&gt; "Lsqb",
    Match =&gt; "Match",
    Minus =&gt; "Minus",
    MinusEqual =&gt; "MinusEqual",
    None =&gt; "None",
    Nonlocal =&gt; "Nonlocal",
    Not =&gt; "Not",
    NotEqual =&gt; "NotEqual",
    Or =&gt; "Or",
    Pass =&gt; "Pass",
    Percent =&gt; "Percent",
    PercentEqual =&gt; "PercentEqual",
    Plus =&gt; "Plus",
    PlusEqual =&gt; "PlusEqual",
    Raise =&gt; "Raise",
    Rarrow =&gt; "Rarrow",
    Rbrace =&gt; "Rbrace",
    Return =&gt; "Return",
    RightShift =&gt; "RightShift",
    RightShiftEqual =&gt; "RightShiftEqual",
    Rpar =&gt; "Rpar",
    Rsqb =&gt; "Rsqb",
    Semi =&gt; "Semi",
    Slash =&gt; "Slash",
    SlashEqual =&gt; "SlashEqual",
    Star =&gt; "Star",
    StarEqual =&gt; "StarEqual",
    StartExpression =&gt; "StartExpression",
    StartInteractive =&gt; "StartInteractive",
    StartModule =&gt; "StartModule",
    Tilde =&gt; "Tilde",
    True =&gt; "True",
    Try =&gt; "Try",
    Type =&gt; "Type",
    Vbar =&gt; "Vbar",
    VbarEqual =&gt; "VbarEqual",
    While =&gt; "While",
    With =&gt; "With",
    Yield =&gt; "Yield",
};
</t>
<t tx="ekr.20241002163554.1">fn string_to_static_str(&amp;self, s: String) -&gt; &amp;'static str {
    Box::leak(s.into_boxed_str())
}

</t>
<t tx="ekr.20241003093722.1">@language rust
pub fn main() {
    // Main line of beautifier.
    let mut x = Beautifier::new();
    if true {
        // testing.
        println!("");
        for file_path in [
            "C:\\Repos\\ekr-tbo-in-rust\\test\\test1.py",
            // "C:\\Repos\\leo-editor\\leo\\core\\leoFrame.py",
            // "C:\\Repos\\leo-editor\\leo\\core\\leoApp.py"
        ] {
            x.beautify_one_file(&amp;file_path);
        }
        // x.stats.report();
    } else {
        if x.enabled("--help") || x.enabled("-h") {
            x.show_help();
            return;
        }
        x.show_args();
        x.beautify_all_files();
    }
}
</t>
<t tx="ekr.20241003094145.1">#[derive(Clone, Debug)]
#[allow(dead_code)]
pub struct TestTok {
    value: i32,
}
</t>
<t tx="ekr.20241003094218.1">fn test_vec() {
    //! Test code for Vec.
    let mut v: Vec&lt;i32&gt; = Vec::new();
    push_vec(&amp;mut v, 1);
    push_vec(&amp;mut v, 2);
    for i in [3, 4, 6] {
        push_vec(&amp;mut v, i);
    }
    let mut i = 7;
    while i &lt; 10 {
        push_vec(&amp;mut v, i);
        i += 1
    }
    println!("");
    println!("v: {v:?}");
}

fn push_vec(v: &amp;mut Vec&lt;i32&gt;, val: i32) {
    v.push(val);
}
</t>
<t tx="ekr.20241003094218.2">fn test_struct() {
    //! Test code for Vec.
    let mut v: Vec&lt;TestTok&gt; = Vec::new();
    push_struct(&amp;mut v, 1);
    push_struct(&amp;mut v, 2);
    for i in [3, 4, 6] {
        push_struct(&amp;mut v, i);
    }
    let mut i = 7;
    while i &lt; 10 {
        push_struct(&amp;mut v, i);
        i += 1
    }
    println!("");
    for z in &amp;v {
        // or just v.
        println!("{z:?}");
    }
    let tok = &amp;v[0]; // v[0] fails.
    println!("\ntok: {tok:?}");

    // A data race happens when these three behaviors occur:

    // - Two or more pointers access the same data at the same time.
    // - At least one of the pointers is being used to write to the data.
    // - There’s no mechanism being used to synchronize access to the data.

    // So This fails
    // {
    // let tok = v[0];
    // println!("{tok:?}");
    // }
}

fn push_struct(v: &amp;mut Vec&lt;TestTok&gt;, val: i32) {
    let mut tok = TestTok { value: 0 };
    tok.value = val; // To test mutability.
    v.push(tok);
}
</t>
<t tx="ekr.20241004073317.1"></t>
<t tx="ekr.20241004095735.1">fn annotate(&amp;mut self) -&gt; Vec::&lt;AnnotatedInputTok&gt; {
    //! Do the prepass, returning tokens annotated with context.
    let mut result = Vec::new();

    // Create self.index_dict.
    self.pre_scan();

    // Create the annotated tokens using self.index_dict.
    {
        let input_tokens_len = self.input_tokens.len();
        let dict_len = &amp;self.index_dict.len();
        println!("");
        println!("annotate: self.input_tokens.len(): {input_tokens_len}");
        println!("annotate: self.index_dict: {dict_len}");
        println!("");
    }
    for (i, token) in self.input_tokens.into_iter().enumerate() {
        // *** println!("annotate: token: {token:?}");
        let context = match self.index_dict.get(&amp;i) {
            Some(x) =&gt; x,
            None =&gt; "",
        };
        // *** println!("annotate: context: {context:?}");
        let annotated_token = AnnotatedInputTok::new(&amp;context, &amp;token.kind, &amp;token.value);
        // ***
        // let annotated_token = AnnotatedInputTok {
            // // context: context.to_string(), kind: &amp;token.kind, value: &amp;token.value
            // context: context, kind: &amp;token.kind, value: &amp;token.value
        // };
        result.push(annotated_token);
    }
    return result;
}
</t>
<t tx="ekr.20241004095931.1">// *** Strange.
#[allow(dead_code)]
#[derive(Debug)]
struct AnnotatedInputTok&lt;'a&gt; {
    // context: String,
    context: &amp;'a str,
    kind: &amp;'a str,
    value: &amp;'a str,
}

impl &lt;'a&gt; AnnotatedInputTok&lt;'_&gt; {
    fn new(context: &amp;'a str, kind: &amp;'a str, value: &amp;'a str) -&gt; AnnotatedInputTok&lt;'a&gt; {
        AnnotatedInputTok {
            context: context,
            kind: kind,
            value: value,
        }
    }
}
</t>
<t tx="ekr.20241004110721.1">#[allow(dead_code)]
struct Annotator&lt;'a&gt; {
    // Classes of tokens
    insignificant_tokens: [&amp;'a str; 7],
    op_kinds: [&amp;'a str; 29],
    // The present input token...
    input_tokens: &amp;'a Vec&lt;InputTok&lt;'a&gt;&gt;,
    index: u32,  // The index within the tokens array of the token being scanned.
    index_dict: HashMap&lt;usize, String&gt;,
    lws: String,  // Leading whitespace. Required!
    // For whitespace.
    curly_brackets_level: u32,  // Number of unmatched '{' tokens.
    paren_level: u32,  // Number of unmatched '(' tokens.
    square_brackets_stack: Vec&lt;bool&gt;,  // A stack of bools, for     gen_word().
    indent_level: u32,  // Set only by do_indent and do_dedent.
    // Parse state.
    decorator_seen: bool,  // Set by do_name for do_op.
    in_arg_list: u32,  // &gt; 0 if in an arg list of a def.
    in_doc_part: bool,
    state_stack: Vec&lt;ParseState&gt;,  // Stack of ParseState objects.
    valid_contexts: [&amp;'a str; 8],  // *** will be 7 w/o the "test" context.
    verbatim: bool,  // True: don't beautify.
}

impl Annotator&lt;'_&gt; {
    @others
}
</t>
<t tx="ekr.20241004112826.1">#[allow(dead_code)]
#[derive(Debug)]
struct ParseState {
    &lt;&lt; docstring: ParseState &gt;&gt;
    kind: String,
    value: String,
}
</t>
<t tx="ekr.20241004113118.1">@language rest
@doc

A class representing items in the parse state stack.

The present states:

'file-start': Ensures the stack stack is never empty.

'decorator': The last '@' was a decorator.

    do_op():    push_state('decorator')
    do_name():  pops the stack if state.kind == 'decorator'.

'indent': The indentation level for 'class' and 'def' names.

    do_name():      push_state('indent', self.level)
    do_dendent():   pops the stack once or
                    twice if state.value == self.level.

</t>
<t tx="ekr.20241004153742.1">fn new&lt;'a&gt;(input_tokens: &amp;'a Vec&lt;InputTok&gt;) -&gt; Annotator&lt;'a&gt; {
    Annotator {
        curly_brackets_level: 0,
        decorator_seen: false,
        in_arg_list: 0,  // &gt; 0 if in an arg list of a def.
        in_doc_part: false,
        indent_level: 0,
        index: 0,
        index_dict: HashMap::new(),
        input_tokens: input_tokens,
        insignificant_tokens: [
            &lt;&lt; define Annotator::insignificant_tokens &gt;&gt;
        ],
        lws: String::new(),
        op_kinds: [
            &lt;&lt; define Annotator::op_kinds &gt;&gt;
        ],
        paren_level: 0,
        state_stack: Vec::new(),
        square_brackets_stack: Vec::new(),
        valid_contexts: [
            "test",  // *** testing only.
            "annotation", "arg", "complex-slice", "dict",
            "import", "initializer", "simple-slice"],
        verbatim: false, 
    }
}
</t>
<t tx="ekr.20241004153802.1">fn pre_scan(&amp;mut self) {
    //! Scan the entire file in one iterative pass, adding context (in self.index_dict)
    //! to a few kinds of tokens as follows:
    //!
    //! Token   Possible Contexts (or None)
    //! =====   ===========================
    //! ":"     "annotation", "dict", "complex-slice", "simple-slice"
    //! "="     "annotation", "initializer"
    //! "*"     "arg"
    //! "**"    "arg"
    //! "."     "import"

    // Push a dummy token on the scan stack so it is never empty.
    let mut scan_stack: Vec&lt;ScanState&gt; = Vec::new();
    let dummy_token = InputTok::new(0, "dummy", "");
    let dummy_state = ScanState::new("dummy-scan-state", &amp;dummy_token);
    scan_stack.push(dummy_state);
    // Init prev_token to a dummy value.
    let mut prev_token = &amp;dummy_token;
    // The main loop...
    let mut in_import = false;
    for (i, token) in self.input_tokens.into_iter().enumerate() {
        let (kind, value) = (token.kind, token.value);
        if false {  // *** Testing only
            self.set_context(i, "test");
        }
        // println!("pre_scan: {kind:&gt;20} {value:?}");
        if kind == "Newline" {
            &lt;&lt; pre-scan newline tokens &gt;&gt;
        }
        else if self.op_kinds.contains(&amp;kind) {
            // *** println!("   OP: kind: {kind:&gt;12} value: {value:?}");  // ***
            &lt;&lt; pre-scan op tokens &gt;&gt;
        }
        else if kind == "Name" {
            // println!("Name: {value:?}");  // ***
            &lt;&lt; pre-scan name tokens &gt;&gt;
        }
        else if ["Class", "Def"].contains(&amp;kind) {
            // println!("{kind}");
            self.set_context(i, "test");
        }
        else if kind == "ws" {  // ***
        }
        else {
            // println!("Other: {kind:?}");
        }
        // Remember the previous significant token.
        if !self.insignificant_tokens.contains(&amp;kind) {
            prev_token = &amp;token;
        }
    }
    // Sanity check.
    if scan_stack.len() &gt; 1 || scan_stack[0].kind != "dummy-scan-state" {
        println!("");
        println!("pre_scan: non-empty scan_stack");
        for scan_state in scan_stack {
            println!("{scan_state:?}");
        }
    }
}
</t>
<t tx="ekr.20241004154345.2">// "import" and "from x import" statements may span lines.
// "ws" tokens represent continued lines like this:   ws: " \\\n    "

if in_import &amp;&amp; scan_stack.len() == 0 {
    in_import = false;
}
</t>
<t tx="ekr.20241004154345.3">// top_state: Optional[ScanState] = scan_stack[-1] if scan_stack else None
let mut top_state = scan_stack[scan_stack.len() - 1].clone();

if false {  // ***
    println!("   OP: kind: {kind:&gt;12} value: {value:?}");
}

// Handle "[" and "]".
if value == "[" {
    scan_stack.push(ScanState::new("slice", &amp;token));
}
else if  value == "]" {
    assert!(top_state.kind == "slice");
    self.finish_slice(i, &amp;top_state);
    scan_stack.pop();
}

// Handle "{" and "}".
if value == "{" {
    scan_stack.push(ScanState::new("dict", &amp;token));
}
else if value == "}" {
    assert!(top_state.kind == "dict");
    self.finish_dict(i, &amp;top_state);
    scan_stack.pop();
}

// Handle "(" and ")"
else if value == "(" {
    if self.is_python_keyword(&amp;prev_token) || prev_token.kind != "name" {
        scan_stack.push(ScanState::new("(", &amp;token));
    }
    else {
        scan_stack.push(ScanState::new("arg", &amp;token));
    }
}
else if value == ")" {
    assert!(["arg", "("].contains(&amp;top_state.kind));
    if top_state.kind == "arg" {
        self.finish_arg(i, &amp;top_state);
    }
    scan_stack.pop();
}

// Handle interior tokens in "arg" and "slice" states.
if true { // *** top_state.kind != "dummy-scan-state" {
    if value == ":" &amp;&amp; ["dict", "slice"].contains(&amp;top_state.kind) {
        top_state.indices.push(i);
    }
    //  *** else if top_state.kind == "arg" &amp;&amp; ["**", "*", "=", ":", ","].contains(&amp;value) {
    else if ["**", "*", "=", ":", ","].contains(&amp;value) {
        println!("FOUND: kind: {kind:&gt;12} value: {value:?}");
        top_state.indices.push(i);
    }
}

// Handle "." and "(" tokens inside "import" and "from" statements.
if in_import &amp;&amp; ["(", "."].contains(&amp;value) {
    self.set_context(i, "import");
}
</t>
<t tx="ekr.20241004154345.4">// *** Python
    // *** WRONG: in Rust, "From" and "Import" are separate tokens.

    // prev_is_yield = prev_token and prev_token.kind == 'name' and prev_token.value == 'yield'
    // if value in ('from', 'import') and not prev_is_yield:
        // # 'import' and 'from x import' statements should be at the outer level.
        // assert not scan_stack, scan_stack
        // in_import = True
        

let prev_is_yield = prev_token.kind == "name" &amp;&amp; prev_token.value == "yield";
if !prev_is_yield &amp;&amp; (value == "from" || value == "import") {
    // "import" and "from x import" statements should be at the outer level.
    assert!(scan_stack.len() == 1 &amp;&amp; scan_stack[0].kind == "dummy");
    in_import = true;
}
</t>
<t tx="ekr.20241004154345.5">// *** Python
// def finish_arg(self, end: int, state: Optional[ScanState]) -&gt; None:
    // """Set context for all ':' when scanning from '(' to ')'."""

    // # Sanity checks.
    // if not state:
        // return
    // assert state.kind == 'arg', repr(state)
    // token = state.token
    // assert token.value == '(', repr(token)
    // values = state.value
    // assert isinstance(values, list), repr(values)
    // i1 = token.index
    // assert i1 &lt; end, (i1, end)
    // if not values:
        // return

    // # Compute the context for each *separate* '=' token.
    // equal_context = 'initializer'
    // for i in values:
        // token = self.input_tokens[i]
        // assert token.kind == 'op', repr(token)
        // if token.value == ',':
            // equal_context = 'initializer'
        // elif token.value == ':':
            // equal_context = 'annotation'
        // elif token.value == '=':
            // self.set_context(i, equal_context)
            // equal_context = 'initializer'

    // # Set the context of all outer-level ':', '*', and '**' tokens.
    // prev: Optional[InputToken] = None
    // for i in range(i1, end):
        // token = self.input_tokens[i]
        // if token.kind not in self.insignificant_kinds:
            // if token.kind == 'op':
                // if token.value in ('*', '**'):
                    // if self.is_unary_op_with_prev(prev, token):
                        // self.set_context(i, 'arg')
                // elif token.value == '=':
                    // # The code above has set the context.
                    // assert token.context in ('initializer', 'annotation'), (i, repr(token.context))
                // elif token.value == ':':
                    // self.set_context(i, 'annotation')
            // prev = token

fn finish_arg(&amp;mut self, end: usize, state: &amp;ScanState) {
    //! Set context for all ":" when scanning from "(" to ")".
    
    println!("finish_arg: {end} {state:?}");
    
    // Sanity checks.
    if state.kind == "dummy" {
        println!("finish_arg: dummy state!");
        return
    }
    assert!(state.kind == "arg");
    assert!(state.token.value == "(");
    if state.indices.len() == 0 {
        return;
    }
    let i1 = state.indices[0];
    assert!(i1 &lt; end);

    // Compute the context for each *separate* "=" token.
    let mut equal_context = "initializer";
    for i in state.indices.clone() {
        let token: &amp;InputTok = &amp;self.input_tokens[i];
        println!("finish_arg: {i} {token:?}");
        assert!(token.kind == "op");
        if token.value == "," {
            equal_context = "initializer";
        }
        else if token.value == ":" {
            equal_context = "annotation";
        }
        else if token.value == "=" {
            self.set_context(i, equal_context);
            equal_context = "initializer";
        }
    }
    // Set the context of all outer-level ":", "*", and "**" tokens.
    let mut prev_token = &amp;InputTok::new(0, "dummy", "");
    for i in i1..end {
        let token = &amp;self.input_tokens[i];
        if !self.insignificant_tokens.contains(&amp;token.kind) {
            if token.kind == "op" {
                if ["*", "**"].contains(&amp;token.value) {
                    if self.is_unary_op_with_prev(&amp;prev_token, &amp;token) {
                        self.set_context(i, "arg");
                    }
                }
                else if token.value == "=" {
                    // The code above has set the context.
                    // assert token.context in ("initializer", "annotation"), (i, repr(token.context))
                }
                else if token.value == ":" {
                    self.set_context(i, "annotation")
                }
            }
            prev_token = token;
        }
    }
}
</t>
<t tx="ekr.20241004154345.6">fn finish_slice(&amp;mut self, end: usize, state: &amp;ScanState) {
    //! Set context for all ":" when scanning from "[" to "]".

    // Sanity checks.
    assert!(state.kind == "slice");
    
    let token = state.token;
    assert!(token.value == "[");

    let indices = &amp;state.indices;
    
    // *** let mut i1 = token.index;
    let i1 = 0;
    // assert i1 &lt; end, (i1, end)

    // Do nothing if there are no ":" tokens in the slice.
    if indices.len() == 0 {
        return;
    }

    // Compute final context by scanning the tokens.
    let mut final_context = "simple-slice";
    let mut inter_colon_tokens = 0;
    let mut prev_token = token;
    for i in i1 + 1..end - 1 {
        let token = &amp;self.input_tokens[i];
        let (kind, value) = (token.kind, token.value);
        if !self.insignificant_tokens.contains(&amp;kind) {
            if kind == "op" {
                if *value == *"." {
                    // Ignore "." tokens and any preceding "name" token.
                    if prev_token.kind == "name" {
                        inter_colon_tokens -= 1;
                    }
                }
                else if *value == *":" {
                    inter_colon_tokens = 0;
                }
                else if *value == *"-" || *value == *"+" {
                    // Ignore unary "-" or "+" tokens.
                    if !self.is_unary_op_with_prev(&amp;prev_token, &amp;token) {
                        inter_colon_tokens += 1;
                        if inter_colon_tokens &gt; 1 {
                            final_context = "complex-slice";
                            break;
                        }
                    }
                }
                else if *value == *"~" {
                    // "~" is always a unary op.
                }
                else {
                    // All other ops contribute.
                    inter_colon_tokens += 1;
                    if inter_colon_tokens &gt; 1 {
                        final_context = "complex-slice";
                        break;
                    }
                }
            }
            else {
                inter_colon_tokens += 1;
                if inter_colon_tokens &gt; 1 {
                    final_context = "complex-slice";
                    break;
                }
            }
            prev_token = token;
        }
    }
    // Set the context of all outer-level ":" tokens.
    for i in indices {
        self.set_context(*i, final_context);
    }    
}
</t>
<t tx="ekr.20241004154345.7">// ***
#[allow(unused_variables)]
fn finish_dict(&amp;mut self, end: usize, state: &amp;ScanState) {
    //! Set context for all ":" when scanning from "{" to "}"
    //! 
    //! Strictly speaking, setting this context is unnecessary because
    //! Annotator.gen_colon generates the same code regardless of this context.
    //! 
    //! In other words, this method can be a do-nothing!

    // Sanity checks.
    if state.kind == "Dummy" {
        return;
    }
    assert!(state.kind == "dict");

    let token = state.token;
    assert!(token.value == "{");

    // *** Rewrite
        // let i1 = token.index;
        // assert i1 &lt; end, (i1, end)

    // Set the context for all ":" tokens.
    let indices = &amp;state.indices;
    for i in indices {
        self.set_context(*i, "dict");
    }
}
</t>
<t tx="ekr.20241004163018.1">fn set_context(&amp;mut self, i: usize, context: &amp;str) {
    //! Set self.index_dict[i], but only if it does not already exist!

    if !self.valid_contexts.contains(&amp;context) {
        println!("Unexpected context! {context:?}");
    }
    if false {  // Debugging.
        let token = &amp;self.input_tokens[i];
        let token_kind = token.kind;
        let token_value = token.value;
        println!("set_context: {token_kind:20}: {context:20} {token_value}");
    }
    if !self.index_dict.contains_key(&amp;i) {
        // println!("set_context: {i} {context:?}");
        self.index_dict.insert(i, context.to_string());
    }
}
</t>
<t tx="ekr.20241005091217.1">// def is_python_keyword(self, token: Optional[InputToken]) -&gt; bool:
    // """Return True if token is a 'name' token referring to a Python keyword."""
    // if not token or token.kind != 'name':
        // return False
    // return keyword.iskeyword(token.value) or keyword.issoftkeyword(token.value)
    
// Keywords:
// False      await      else       import     pass
// None       break      except     in         raise
// True       class      finally    is         return
// and        continue   for        lambda     try
// as         def        from       nonlocal   while
// assert     del        global     not        with
// async      elif       if         or         yield

// Soft keywords:
// match, case, type and _

// *** Remove leading underscores.
fn is_python_keyword(&amp;self, _token: &amp;InputTok) -&gt; bool {
    return false;

    // *** Not ready yet.
        // //! Return True if token is a 'name' token referring to a Python keyword.
        // if token.kind != "name" {
            // return false;
        // }
        // // let word = &amp;token.value;  // &amp;String
        // return false;  // ***
}
</t>
<t tx="ekr.20241005092549.1">// def is_unary_op_with_prev(self, prev: Optional[InputToken], token: InputToken) -&gt; bool:
    // """
    // Return True if token is a unary op in the context of prev, the previous
    // significant token.
    // """
    // if token.value == '~':  # pragma: no cover
        // return True
    // if prev is None:
        // return True  # pragma: no cover
    // assert token.value in '**-+', repr(token.value)
    // if prev.kind in ('number', 'string'):
        // return_val = False
    // elif prev.kind == 'op' and prev.value in ')]':
         // # An unnecessary test?
        // return_val = False  # pragma: no cover
    // elif prev.kind == 'op' and prev.value in '{([:,':
        // return_val = True
    // elif prev.kind != 'name':
        // # An unnecessary test?
        // return_val = True  # pragma: no cover
    // else:
        // # prev is a'name' token.
        // return self.is_python_keyword(token)
    // return return_val

// *** Remove leading underscores.
fn is_unary_op_with_prev(&amp;self, _prev_token: &amp;InputTok, _token: &amp;InputTok) -&gt; bool {
    return false;  // ***
}
</t>
<t tx="ekr.20241007005233.1">@language python

"""
    test1.py: a test file to test context generation.
    
    Token   Possible Contexts (or None)
    =====   ===========================
    ":"     "annotation", "dict", "complex-slice", "simple-slice"
    "="     "annotation", "initializer"
    "*"     "arg"
    "**"    "arg"
    "."     "import"
"""

class TestClass:

    def test_ops(self, arg1, *args, **kwargs):
        a = 2
        print(a)
</t>
<t tx="ekr.20241007005513.1">@language python
@nosearch
</t>
<t tx="ekr.20241007005513.17">class TestTokens(BaseTest):
    """Unit tests for tokenizing."""

    @others
</t>
<t tx="ekr.20241007005513.18">def show_example_dump(self):  # pragma: no cover

    # Will only be run when enabled explicitly.

    contents = """
print('line 1')
print('line 2')
print('line 3')
"""
    contents, tokens = self.make_data(contents)
    dump_contents(contents)
    dump_tokens(tokens)
</t>
<t tx="ekr.20241007005513.19">def test_bs_nl_tokens(self):
    # Test https://bugs.python.org/issue38663.

    contents = """
print \
    ('abc')
"""
    self.check_roundtrip(contents)
</t>
<t tx="ekr.20241007005513.20">def test_continuation_1(self):

    contents = """
a = (3,4,
    5,6)
y = [3, 4,
    5]
z = {'a': 5,
    'b':15, 'c':True}
x = len(y) + 5 - a[
    3] - a[2] + len(z) - z[
    'b']
"""
    self.check_roundtrip(contents)
</t>
<t tx="ekr.20241007005513.21">def test_continuation_2(self):
    # Backslash means line continuation, except for comments
    contents = (
        'x=1+\\\n    2'
        '# This is a comment\\\n    # This also'
    )
    self.check_roundtrip(contents)
</t>
<t tx="ekr.20241007005513.22">def test_continuation_3(self):

    contents = """
# Comment \\\n
x = 0
"""
    self.check_roundtrip(contents)
</t>
<t tx="ekr.20241007005513.23">def test_string_concatentation_1(self):
    # Two *plain* string literals on the same line
    self.check_roundtrip("""'abc' 'xyz'""")
</t>
<t tx="ekr.20241007005513.24">def test_string_concatentation_2(self):
    # f-string followed by plain string on the same line
    self.check_roundtrip("""f'abc' 'xyz'""")
</t>
<t tx="ekr.20241007005513.25">def test_string_concatentation_3(self):
    # plain string followed by f-string on the same line
    self.check_roundtrip("""'abc' f'xyz'""")
</t>
<t tx="ekr.20241007005513.26">class TestTokenBasedOrange(BaseTest):
    """
    Tests for the TokenBasedOrange class.

    Note: TokenBasedOrange never inserts or deletes lines.
    """
    @others
</t>
<t tx="ekr.20241007005513.29">def test_annotations(self):  # Required for full coverage.

    table = (
        # Case 0.
        """s: str = None\n""",
        # Case 1.
        ("""
            def annotated_f(s: str = None, x=None) -&gt; None:
                pass
        """),
        # Case 2.
        ("""
            def f1():
                self.rulesetName : str = ''
        """),
    )
    for i, contents in enumerate(table):
        contents, tokens = self.make_data(contents)
        expected = self.blacken(contents).rstrip() + '\n'
        results = self.beautify(contents, tokens)
        if results != expected:  # pragma: no cover
            g.printObj(contents, tag='Contents')
            g.printObj(expected, tag='Expected (blackened)')
            g.printObj(results, tag='Results')
        self.assertEqual(results, expected)

</t>
<t tx="ekr.20241007005513.30">def test_at_doc_part(self):

    contents = """
@ Line 1
Line 2
@c

print('hi')
"""
    contents, tokens = self.make_data(contents)
    expected = contents.rstrip() + '\n'
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.31">def test_backslash_newline(self):
    """
    This test is necessarily different from black, because orange doesn't
    delete semicolon tokens.
    """
    contents = r"""
print(a);\
print(b)
print(c); \
print(d)
"""
    contents, tokens = self.make_data(contents)
    expected = contents.rstrip() + '\n'
    # expected = self.blacken(contents).rstrip() + '\n'
    results = self.beautify(contents, tokens)
    # g.printObj(tokens, tag='Tokens')
    # g.printObj(results, tag='Results')
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.32">def test_blank_lines_after_function(self):

    contents = """
# Comment line 1.
# Comment line 2.

def spam():
    pass
    # Properly indented comment.

# Comment line3.
# Comment line4.
a = 2
"""
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    if results != expected:
        g.printObj(results, tag='Results')
        g.printObj(expected, tag='Expected')
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.33">def test_blank_lines_after_function_2(self):

    contents = """
# Leading comment line 1.
# Leading comment lines 2.

def spam():
    pass

# Trailing comment line.
a = 2
"""
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.34">def test_blank_lines_after_function_3(self):

    # From leoAtFile.py.
    contents = """
def writeAsisNode(self, p):
    print('1')

    def put(s):
        print('2')

    # Trailing comment 1.
    # Trailing comment 2.
    print('3')
"""

    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    if results != expected:
        g.printObj(tokens, tag='Tokens')
        g.printObj(results, tag='Results')
        g.printObj(expected, tag='Expected')
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.35">def test_comment_indented(self):

    table = (
        """
        if 1:
            pass
                # An indented comment.
        """,

        """
        table = (
            # Regular comment.
        )
        """
    )
    fails = 0
    for contents in table:
        contents, tokens = self.make_data(contents)
        expected = contents
        results = self.beautify(contents, tokens)
        if results != expected:  # pragma: no cover
            fails += 1
            print(f"Fail: {fails}")
            g.printObj(results, tag='Results')
            g.printObj(expected, tag='Expected')
    assert not fails, fails
</t>
<t tx="ekr.20241007005513.36">def test_comment_space_after_delim(self):

    table = (
        # Test 1.
        (
            """#No space after delim.\n""",
            """# No space after delim.\n""",
        ),
        # Test 2.  Don't change bang lines.
        (
            """#! /usr/bin/env python\n""",
            """#! /usr/bin/env python\n""",
        ),
        # Test 3.  Don't change ### comments.
        (
            """### To do.\n""",
            """### To do.\n""",
        ),
    )
    fails = 0
    for contents, expected in table:
        contents, tokens = self.make_data(contents)
        results = self.beautify(contents, tokens)
        message = (
            f"\n"
            f"  contents: {contents!r}\n"
            f"  expected: {expected!r}\n"
            f"       got: {results!r}")
        if results != expected:  # pragma: no cover
            fails += 1
            print(f"Fail: {fails}\n{message}")
    assert not fails, fails
</t>
<t tx="ekr.20241007005513.37">def test_decorators(self):

    table = (
    # Case 0.
    """
@my_decorator(1)
def func():
    pass
""",
    # Case 1.
    """
if 1:
    @my_decorator
    def func():
        pass
""",
    # Case 2.
    '''
@g.commander_command('promote')
def promote(self, event=None, undoFlag=True):
    """Make all children of the selected nodes siblings of the selected node."""
''',
    )
    for i, contents in enumerate(table):
        contents, tokens = self.make_data(contents)
        expected = contents
        results = self.beautify(contents, tokens)
        if results != expected:
            g.trace('Fail:', i)  # pragma: no cover
            g.printObj(contents, tag='Contents')
            g.printObj(results, tag='Results')
        self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.38">def test_dont_delete_blank_lines(self):

    contents = """
class Test:

    def test_func():

        pass

    a = 2
"""
    contents, tokens = self.make_data(contents)
    expected = contents.rstrip() + '\n'
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.39">def test_leo_sentinels_1(self):

    # Careful: don't put a sentinel into the file directly.
    # That would corrupt leoAst.py.
    sentinel = '#@+node:ekr.20200105143308.54: ** test'
    contents = f"""
{sentinel}
def spam():
    pass
"""
    contents, tokens = self.make_data(contents)
    expected = contents.rstrip() + '\n'
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.40">def test_leo_sentinels_2(self):

    # Careful: don't put a sentinel into the file directly.
    # That would corrupt leoAst.py.
    sentinel = '#@+node:ekr.20200105143308.54: ** test'
    contents = f"""
{sentinel}
class TestClass:
    pass
"""
    contents, tokens = self.make_data(contents)
    expected = contents.rstrip() + '\n'
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.41">def test_lines_before_class(self):

    contents = """
a = 2
class aClass:
    pass
"""
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.42">def test_multi_line_imports(self):

    # The space between 'import' and '(' is correct.
    contents = """
        from .module1 import (
            w1,
            w2,
        )
        from .module1 import \\
            w
        from .module1 import (
            w1,
            w2,
        )
        import leo.core.leoGlobals \\
            as g
    """
    contents, tokens = self.make_data(contents)
    expected = contents.strip() + '\n'
    results = self.beautify(contents, tokens)
    if results != expected:
        g.printObj(tokens, tag='Tokens')
        g.printObj(results, tag='Results')
        g.printObj(expected, tag='Expected')
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.43">def test_multi_line_pet_peeves(self):

    contents = """
        if x == 4: pass
        if x == 4 : pass
        print (x, y); x, y = y, x
        print (x , y) ; x , y = y , x
        if(1):
            pass
        elif(2):
            pass
        while(3):
            pass
    """
    # At present Orange doesn't split lines...
    expected = self.prep(
        """
            if x == 4: pass
            if x == 4: pass
            print(x, y); x, y = y, x
            print(x, y); x, y = y, x
            if (1):
                pass
            elif (2):
                pass
            while (3):
                pass
        """)
    contents, tokens = self.make_data(contents)
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.44">def test_multi_line_statements(self):

    contents = """
        if 1:  # Simulate indent.
            def orange_command(
                s: str,
            ) -&gt; None:
                pass
        if 1:  # The trailing ]) causes the problem.
            return ''.join(
                ['%s%s' % (sep, self.dump_ast(z, level + 1)) for z in node])
    """

    expected = self.prep(
        """
        if 1:  # Simulate indent.
            def orange_command(
                s: str,
            ) -&gt; None:
                pass
        if 1:  # The trailing ]) causes the problem.
            return ''.join(
                ['%s%s' % (sep, self.dump_ast(z, level + 1)) for z in node])
        """)
    contents, tokens = self.make_data(contents)
    results = self.beautify(contents, tokens)
    if results != expected:
        g.printObj(results, tag='Results')
        g.printObj(expected, tag='Expected')
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.45">def test_one_line_pet_peeves(self):

    # One-line pet peeves, except those involving slices and unary ops.

    # See https://peps.python.org/pep-0008/#pet-peeves
    # See https://peps.python.org/pep-0008/#other-recommendations

    tag = 'test_one_line_pet_peeves'

    # Except where noted, all entries are expected values...
    table = (
        ### Duplicate entry to fail first.
        """f(a[1 + 2])""",
    
        # Assignments...
        """a = b * c""",
        """a = b + c""",
        """a = b - c""",
        # * and **, inside and outside function calls.
        """f(*args)""",
        """f(**kwargs)""",
        """f(*args, **kwargs)""",
        """f(a, *args)""",
        """f(a=2, *args)""",
        # Calls...
        """f(-1)""",
        """f(-1 &lt; 2)""",
        """f(1)""",
        """f(2 * 3)""",
        """f(2 + name)""",
        """f(a)""",
        """f(a.b)""",
        """f(a=2 + 3, b=4 - 5, c= 6 * 7, d=8 / 9, e=10 // 11)""",
        """f(a[1 + 2])""",
        """f({key: 1})""",
        """t = (0,)""",
        """x, y = y, x""",
        # Dicts...
        """d = {key: 1}""",
        """d['key'] = a[i]""",
        # Trailing comments: expect two spaces.
        """whatever # comment""",
        """whatever  # comment""",
        """whatever   # comment""",
        # Word ops...
        """v1 = v2 and v3 if v3 not in v4 or v5 in v6 else v7""",
        """print(v7 for v8 in v9)""",
        # Returns...
        """return -1""",
    )
    for i, contents in enumerate(table):
        description = f"{tag} part {i}"
        contents, tokens = self.make_data(contents, description=description)
        expected = self.blacken(contents)
        results = self.beautify(contents, tokens, filename=description)
        if results != expected:  # pragma: no cover
            print('')
            print(
                f"TestTokenBasedOrange.{tag}: FAIL\n"
                f"  contents: {contents.rstrip()}\n"
                f"     black: {expected.rstrip()}\n"
                f"    orange: {results.rstrip() if results else 'None'}")
        self.assertEqual(results, expected, msg=description)
</t>
<t tx="ekr.20241007005513.46">def test_percent_op(self):

    # leo/plugins/writers/basewriter.py, line 38
    contents = """at.os('%s@+node:%s%s' % (delim, s, delim2))"""
    contents, tokens = self.make_data(contents)
    expected = self.blacken(contents).rstrip() + '\n'
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.47">def test_relative_imports(self):

    # #2533.
    contents = """
        from .module1 import w
        from . module2 import x
        from ..module1 import y
        from .. module2 import z
        from . import a
        from.import b
        from .. import c
        from..import d
        from leo.core import leoExternalFiles
        import leo.core.leoGlobals as g
    """
    expected = self.prep(
    """
        from .module1 import w
        from .module2 import x
        from ..module1 import y
        from ..module2 import z
        from . import a
        from . import b
        from .. import c
        from .. import d
        from leo.core import leoExternalFiles
        import leo.core.leoGlobals as g
    """)
    contents, tokens = self.make_data(contents)
    results = self.beautify(contents, tokens)
    if results != expected:  # pragma: no cover
        # g.printObj(tokens, tag='Tokens')
        g.printObj(results, tag='Results')
        g.printObj(expected, tag='Expected')
    self.assertEqual(expected, results)
</t>
<t tx="ekr.20241007005513.48">def test_slice(self):

    # Test one-line pet peeves involving slices.

    # See https://peps.python.org/pep-0008/#pet-peeves
    # See https://peps.python.org/pep-0008/#other-recommendations

    tag = 'test_slice'

    # Except where noted, all entries are expected values....
    table = (

        # Differences between leoAst.py and leoTokens.py.
        # tbo.cmd changes several files, including these.
        # In all cases, the differences make leoTokens.py *more*
        # compatible with Black than leoAst.py!

            # From leoAst.py.
            """val = val[:i] + '# ' + val[i + 1 :]\n""",
            # From leoApp.py.
            """
                for name in rf.getRecentFiles()[:n]:
                    pass
            """,
            # From leoUndo.py.
            """s.extend(body_lines[-trailing:])\n""",
            # From leoTokens.py.
            # The expected value of the two last lines is the last line.
            """
                if line1.startswith(tag) and line1.endswith(tag2):
                    e = line1[n1 : -n2].strip()
                    e = line1[n1:-n2].strip()
            """,

        # Legacy tests...
            """a[:-1]""",
            """a[: 1 if True else 2 :]""",
            """a[1 : 1 + 2]""",
            """a[lower:]""",
            """a[lower::]""",
            """a[:upper]""",
            """a[:upper:]""",
            """a[::step]""",
            """a[lower:upper:]""",
            """a[lower:upper:step]""",
            """a[lower + offset : upper + offset]""",
            """a[: upper_fn(x) :]""",
            """a[: upper_fn(x) : step_fn(x)]""",
            """a[:: step_fn(x)]""",
            """a[: upper_fn(x) :]""",
            """a[: upper_fn(x) : 2 + 1]""",
            """a[:]""",
            """a[::]""",
            """a[1:]""",
            """a[1::]""",
            """a[:2]""",
            """a[:2:]""",
            """a[::3]""",
            """a[1:2]""",
            """a[1:2:]""",
            """a[:2:3]""",
            """a[1:2:3]""",
    )

    for i, contents in enumerate(table):
        description = f"{tag} part {i}"
        contents, tokens = self.make_data(contents, description=description)
        expected = self.blacken(contents)
        results = self.beautify(contents, tokens, filename=description)
        if results != expected:  # pragma: no cover
            print('')
            print('TestTokenBasedOrange')
            # g.printObj(contents, tag='Contents')
            g.printObj(expected, tag='Expected (Black)')
            g.printObj(results, tag='Results')
        self.assertEqual(expected, results)
</t>
<t tx="ekr.20241007005513.49">def test_star_star_operator(self):

    # Don't rely on black for this test.
    contents = """a = b ** c"""
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
</t>
<t tx="ekr.20241007005513.50">def test_unary_ops(self):

    # One-line pet peeves involving unary ops but *not* slices.

    # See https://peps.python.org/pep-0008/#pet-peeves
    # See https://peps.python.org/pep-0008/#other-recommendations

    tag = 'test_unary_op'

    # All entries are expected values....
    table = (
        # Calls...
        """f(-1)""",
        """f(-1 &lt; 2)""",
        # Dicts...
        """d = {key: -3}""",
        """d['key'] = a[-i]""",
        # Unary minus...
        """v = -1 if a &lt; b else -2""",
        # ~
        """a = ~b""",
        """c = a[:~e:]""",
        # """d = a[f() - 1 :]""",
        # """e = a[2 - 1 :]""",
        # """e = a[b[2] - 1 :]""",
    )
    for i, contents in enumerate(table):
        description = f"{tag} part {i}"
        contents, tokens = self.make_data(contents, description=description)
        expected = self.blacken(contents)
        results = self.beautify(contents, tokens, filename=description)
        if results != expected:  # pragma: no cover
            print('')
            print(
                f"TestTokenBasedOrange.{tag}:\n"
                f"  contents: {contents.rstrip()}\n"
                f"     black: {expected.rstrip()}\n"
                f"    orange: {results.rstrip() if results else 'None'}")
        self.assertEqual(results, expected, msg=description)
</t>
<t tx="ekr.20241007005513.51">def test_verbatim(self):

    contents = """
@nobeautify

def addOptionsToParser(self, parser, trace_m):

    add = parser.add_option

    def add_bool(option, help, dest=None):
        add(option, action='store_true', dest=dest, help=help)

    add_bool('--diff',          'use Leo as an external git diff')
    # add_bool('--dock',          'use a Qt dock')
    add_bool('--fullscreen',    'start fullscreen')
    add_bool('--init-docks',    'put docks in default positions')
    # Multiple bool values.
    add('-v', '--version', action='store_true',
        help='print version number and exit')

# From leoAtFile.py
noDirective     =  1 # not an at-directive.
allDirective    =  2 # at-all (4.2)
docDirective    =  3 # @doc.

@beautify
"""
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected, msg=contents)
</t>
<t tx="ekr.20241007005513.52">def test_verbatim_with_pragma(self):

    contents = """
# pragma: no beautify

def addOptionsToParser(self, parser, trace_m):

    add = parser.add_option

    def add_bool(option, help, dest=None):
        add(option, action='store_true', dest=dest, help=help)

    add_bool('--diff',          'use Leo as an external git diff')
    # add_bool('--dock',          'use a Qt dock')
    add_bool('--fullscreen',    'start fullscreen')
    add_other('--window-size',  'initial window size (height x width)', m='SIZE')
    add_other('--window-spot',  'initial window position (top x left)', m='SPOT')
    # Multiple bool values.
    add('-v', '--version', action='store_true',
        help='print version number and exit')

# pragma: beautify
"""
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected, msg=contents)
</t>
<t tx="ekr.20241007005513.53">def test_verbatim2(self):

    # We *do* want this test to contain verbatim sentinels.
    contents = """
@beautify
@nobeautify
@ Starts doc part
More doc part.
The @c ends the doc part.
@c
    """
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    # g.printObj(results, tag='Results')
    # g.printObj(expected, tag='Expected')
    self.assertEqual(results, expected, msg=contents)
</t>
<t tx="ekr.20241007011856.1">@language python
@nosearch
</t>
<t tx="ekr.20241007011856.10">def beautify_file(filename: str) -&gt; bool:
    """
    Beautify the given file, writing it if has changed.
    """
    settings: dict[str, Any] = {
        'all': False,  # Don't beautify all files.
        'beautified': True,  # Report changed files.
        'diff': False,  # Don't show diffs.
        'report': True,  # Report changed files.
        'write': True,  # Write changed files.
    }
    tbo = TokenBasedOrange(settings)
    return tbo.beautify_file(filename)
</t>
<t tx="ekr.20241007011856.11">def main() -&gt; None:  # pragma: no cover
    """Run commands specified by sys.argv."""
    args, settings_dict, arg_files = scan_args()
    cwd = os.getcwd()

    # Calculate requested files.
    requested_files: list[str] = []
    for path in arg_files:
        if path.endswith('.py'):
            requested_files.append(os.path.join(cwd, path))
        else:
            root_dir = os.path.join(cwd, path)
            requested_files.extend(
                glob.glob(f'{root_dir}**{os.sep}*.py', recursive=True)
            )
    if not requested_files:
        # print(f"No files in {arg_files!r}")
        return

    # Calculate the actual list of files.
    modified_files = g.getModifiedFiles(cwd)

    def is_dirty(path: str) -&gt; bool:
        return os.path.abspath(path) in modified_files

    # Compute the files to be checked.
    if args.all:
        # Handle all requested files.
        to_be_checked_files = requested_files
    else:
        # Handle only modified files.
        to_be_checked_files = [z for z in requested_files if is_dirty(z)]

    # Compute the dirty files among the to-be-checked files.
    dirty_files = [z for z in to_be_checked_files if is_dirty(z)]

    # Do the command.
    if to_be_checked_files:
        orange_command(arg_files, requested_files, dirty_files, to_be_checked_files, settings_dict)
</t>
<t tx="ekr.20241007011856.12">def orange_command(
    arg_files: list[str],
    requested_files: list[str],
    dirty_files: list[str],
    to_be_checked_files: list[str],
    settings: Optional[Settings] = None,
) -&gt; None:  # pragma: no cover
    """The outer level of the 'tbo/orange' command."""
    t1 = time.process_time()
    # n_tokens = 0
    n_beautified = 0
    if settings is None:
        settings = {}
    for filename in to_be_checked_files:
        if os.path.exists(filename):
            tbo = TokenBasedOrange(settings)
            beautified = tbo.beautify_file(filename)
            if beautified:
                n_beautified += 1
            # n_tokens += len(tbo.input_tokens)
        else:
            print(f"file not found: {filename}")
    # Report the results.
    t2 = time.process_time()
    if n_beautified or settings.get('report'):
        print(
            f"tbo: {t2-t1:4.2f} sec. "
            f"dirty: {len(dirty_files):&lt;3} "
            f"checked: {len(to_be_checked_files):&lt;3} "
            f"beautified: {n_beautified:&lt;3} in {','.join(arg_files)}"
        )
</t>
<t tx="ekr.20241007011856.13">def scan_args() -&gt; tuple[Any, dict[str, Any], list[str]]:  # pragma: no cover
    description = textwrap.dedent(
    """Beautify or diff files""")
    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument('PATHS', nargs='*', help='directory or list of files')
    add2 = parser.add_argument

    # Arguments.
    add2('-a', '--all', dest='all', action='store_true',
        help='Beautify all files, even unchanged files')
    add2('-b', '--beautified', dest='beautified', action='store_true',
        help='Report beautified files individually, even if not written')
    add2('-d', '--diff', dest='diff', action='store_true',
        help='show diffs instead of changing files')
    add2('-r', '--report', dest='report', action='store_true',
        help='show summary report')
    add2('-w', '--write', dest='write', action='store_true',
        help='write beautifed files (dry-run mode otherwise)')

    # Create the return values, using EKR's prefs as the defaults.
    parser.set_defaults(
        all=False, beautified=False, diff=False, report=False, write=False,
        tab_width=4,
    )
    args: Any = parser.parse_args()
    files = args.PATHS

    # Create the settings dict, ensuring proper values.
    settings_dict: dict[str, Any] = {
        'all': bool(args.all),
        'beautified': bool(args.beautified),
        'diff': bool(args.diff),
        'report': bool(args.report),
        'write': bool(args.write)
    }
    return args, settings_dict, files
</t>
<t tx="ekr.20241007011856.14"></t>
<t tx="ekr.20241007011856.15">class InternalBeautifierError(Exception):
    """
    An internal error in the beautifier.

    Errors in the user's source code may raise standard Python errors
    such as IndentationError or SyntaxError.
    """
</t>
<t tx="ekr.20241007011856.16">class InputToken:  # leoTokens.py.
    """A class representing a TBO input token."""

    __slots__ = (
        'context',
        'index',
        'kind',
        'line',
        'line_number',
        'value',
    )

    def __init__(
        self, kind: str, value: str, index: int, line: str, line_number: int,
    ) -&gt; None:
        self.context: Optional[str] = None
        self.index = index
        self.kind = kind
        self.line = line  # The entire line containing the token.
        self.line_number = line_number
        self.value = value

    def __repr__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;5} {self.kind:&gt;8}"
        return f"Token {s}: {self.show_val(20):22}"

    def __str__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;5} {self.kind:&gt;8}"
        return f"Token {s}: {self.show_val(20):22}"

    def to_string(self) -&gt; str:
        """Return the contribution of the token to the source file."""
        return self.value if isinstance(self.value, str) else ''

    @others
</t>
<t tx="ekr.20241007011856.17">def brief_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token."""
    token_s = f"{self.kind:&gt;10} : {self.show_val(10):12}"
    return f"&lt;line: {self.line_number} index: {self.index:3} {token_s}&gt;"

</t>
<t tx="ekr.20241007011856.18">def dump(self) -&gt; str:  # pragma: no cover
    """Dump a token and related links."""
    return (
        f"{self.line_number:4} "
        f"{self.index:&gt;5} {self.kind:&gt;15} "
        f"{self.show_val(100)}"
    )
</t>
<t tx="ekr.20241007011856.19">def dump_header(self) -&gt; None:  # pragma: no cover
    """Print the header for token.dump"""
    print(
        f"\n"
        f"         node    {'':10} token {'':10}   token\n"
        f"line index class {'':10} index {'':10} kind value\n"
        f"==== ===== ===== {'':10} ===== {'':10} ==== =====\n")
</t>
<t tx="ekr.20241007011856.20">def error_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token for error message."""
    return f"index: {self.index:&lt;3} {self.kind:&gt;12} {self.show_val(20):&lt;20}"
</t>
<t tx="ekr.20241007011856.21">def show_val(self, truncate_n: int = 8) -&gt; str:  # pragma: no cover
    """Return the token.value field."""
    if self.kind in ('dedent', 'indent', 'newline', 'ws'):
        # val = str(len(self.value))
        val = repr(self.value)
    elif self.kind == 'string' or self.kind.startswith('fstring'):
        # repr would be confusing.
        val = g.truncate(self.value, truncate_n)
    else:
        val = g.truncate(repr(self.value), truncate_n)
    return val
</t>
<t tx="ekr.20241007011856.22">class Tokenizer:
    """
    Use Python's tokenizer module to create InputTokens
    See: https://docs.python.org/3/library/tokenize.html
    """

    __slots__ = (
        'contents',
        'fstring_line',
        'fstring_line_number',
        'fstring_values',
        'lines',
        'offsets',
        'prev_offset',
        'token_index',
        'token_list',
    )

    def __init__(self) -&gt; None:
        self.contents: str = ''
        self.offsets: list[int] = [0]  # Index of start of each line.
        self.prev_offset = -1
        self.token_index = 0
        self.token_list: list[InputToken] = []
        # Describing the scanned f-string...
        self.fstring_line: Optional[str] = None
        self.fstring_line_number: Optional[int] = None
        self.fstring_values: Optional[list[str]] = None

    @others
</t>
<t tx="ekr.20241007011856.23">def add_token(self, kind: str, line: str, line_number: int, value: str,) -&gt; None:
    """
    Add an InputToken to the token list.

    Convert fstrings to simple strings.
    """
    if self.fstring_values is None:
        if kind == 'fstring_start':
            self.fstring_line = line
            self.fstring_line_number = line_number
            self.fstring_values = [value]
            return
    else:
        # Accumulating an f-string.
        self.fstring_values.append(value)
        if kind != 'fstring_end':
            return
        # Create a single 'string' token from the saved values.
        kind = 'string'
        value = ''.join(self.fstring_values)
        # Use the line and line number of the 'string-start' token.
        line = self.fstring_line or ''
        line_number = self.fstring_line_number or 0
        # Clear the saved values.
        self.fstring_line = None
        self.fstring_line_number = None
        self.fstring_values = None

    tok = InputToken(kind, value, self.token_index, line, line_number)
    self.token_index += 1
    self.token_list.append(tok)
</t>
<t tx="ekr.20241007011856.24">def check_results(self, contents: str) -&gt; None:

    # Split the results into lines.
    result = ''.join([z.to_string() for z in self.token_list])
    result_lines = g.splitLines(result)
    # Check.
    ok = result == contents and result_lines == self.lines
    assert ok, (
        f"\n"
        f"      result: {result!r}\n"
        f"    contents: {contents!r}\n"
        f"result_lines: {result_lines}\n"
        f"       lines: {self.lines}"
    )
</t>
<t tx="ekr.20241007011856.25">def check_round_trip(self, contents: str, tokens: list[InputToken]) -&gt; bool:
    result = self.tokens_to_string(tokens)
    ok = result == contents
    if not ok:  # pragma: no cover
        print('\nRound-trip check FAILS')
        print('Contents...\n')
        print(contents)
        print('\nResult...\n')
        print(result)
    return ok
</t>
<t tx="ekr.20241007011856.26">def create_input_tokens(
    self,
    contents: str,
    five_tuples: Generator,
) -&gt; list[InputToken]:
    """
    InputTokenizer.create_input_tokens.

    Return list of InputToken's from tokens, a list of 5-tuples.
    """
    # Remember the contents for debugging.
    self.contents = contents

    # Create the physical lines.
    self.lines = contents.splitlines(True)

    # Create the list of character offsets of the start of each physical line.
    last_offset = 0
    for line in self.lines:
        last_offset += len(line)
        self.offsets.append(last_offset)

    # Create self.token_list.
    for five_tuple in five_tuples:
        self.do_token(contents, five_tuple)

    # Print the token list when tracing.
    self.check_results(contents)
    return self.token_list
</t>
<t tx="ekr.20241007011856.27">def do_token(self, contents: str, five_tuple: tuple) -&gt; None:
    """
    Handle the given token, optionally including between-token whitespace.

    https://docs.python.org/3/library/tokenize.html
    https://docs.python.org/3/library/token.html

    five_tuple is a named tuple with these fields:
    - type:     The token type;
    - string:   The token string.
    - start:    (srow: int, scol: int) The row (line_number!) and column
                where the token begins in the source.
    - end:      (erow: int, ecol: int)) The row (line_number!) and column
                where the token ends in the source;
    - line:     The *physical line on which the token was found.
    """
    import token as token_module

    # Unpack..
    tok_type, val, start, end, line = five_tuple
    s_row, s_col = start  # row/col offsets of start of token.
    e_row, e_col = end  # row/col offsets of end of token.
    line_number = s_row
    kind = token_module.tok_name[tok_type].lower()
    # Calculate the token's start/end offsets: character offsets into contents.
    s_offset = self.offsets[max(0, s_row - 1)] + s_col
    e_offset = self.offsets[max(0, e_row - 1)] + e_col
    # tok_s is corresponding string in the line.
    tok_s = contents[s_offset:e_offset]
    # Add any preceding between-token whitespace.
    ws = contents[self.prev_offset:s_offset]
    if ws:
        # Create the 'ws' pseudo-token.
        self.add_token('ws', line, line_number, ws)
    # Always add token, even if it contributes no text!
    self.add_token(kind, line, line_number, tok_s)
    # Update the ending offset.
    self.prev_offset = e_offset
</t>
<t tx="ekr.20241007011856.28">def make_input_tokens(self, contents: str) -&gt; list[InputToken]:
    """
    Return a list  of InputToken objects using tokenize.tokenize.

    Perform consistency checks and handle all exceptions.
    """
    try:
        five_tuples = tokenize.tokenize(
            io.BytesIO(contents.encode('utf-8')).readline)
    except Exception as e:  # pragma: no cover
        print(f"make_input_tokens: exception {e!r}")
        return []
    tokens = self.create_input_tokens(contents, five_tuples)
    if 1:
        # True: 2.9 sec. False: 2.8 sec.
        assert self.check_round_trip(contents, tokens)
    return tokens
</t>
<t tx="ekr.20241007011856.29">def tokens_to_string(self, tokens: list[InputToken]) -&gt; str:
    """Return the string represented by the list of tokens."""
    if tokens is None:  # pragma: no cover
        # This indicates an internal error.
        print('')
        print('===== No tokens ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20241007011856.30">class ParseState:
    """
    A class representing items in the parse state stack.

    The present states:

    'file-start': Ensures the stack stack is never empty.

    'decorator': The last '@' was a decorator.

        do_op():    push_state('decorator')
        do_name():  pops the stack if state.kind == 'decorator'.

    'indent': The indentation level for 'class' and 'def' names.

        do_name():      push_state('indent', self.level)
        do_dendent():   pops the stack once or
                        twice if state.value == self.level.
    """

    __slots__ = ('kind', 'value')

    def __init__(self, kind: str, value: Union[int, str, None]) -&gt; None:
        self.kind = kind
        self.value = value

    def __repr__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover

    def __str__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover
</t>
<t tx="ekr.20241007011856.31">class ScanState:  # leoTokens.py.
    """
    A class representing tbo.pre_scan's scanning state.

    Valid (kind, value) pairs:

       kind  Value
       ====  =====
      'args' None
      'from' None
    'import' None
     'slice' list of colon indices
      'dict' list of colon indices

    """

    __slots__ = ('kind', 'token', 'value')

    def __init__(self, kind: str, token: InputToken) -&gt; None:
        self.kind = kind
        self.token = token
        self.value: list[int] = []  # Not always used.

    def __repr__(self) -&gt; str:  # pragma: no cover
        return f"ScanState: i: {self.token.index:&lt;4} kind: {self.kind} value: {self.value}"

    def __str__(self) -&gt; str:  # pragma: no cover
        return f"ScanState: i: {self.token.index:&lt;4} kind: {self.kind} value: {self.value}"
</t>
<t tx="ekr.20241007011856.32">class TokenBasedOrange:  # Orange is the new Black.

    &lt;&lt; TokenBasedOrange: docstring &gt;&gt;
    &lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;
    &lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;

    @others
</t>
<t tx="ekr.20241007011856.33">@language rest
@wrap

"""
Leo's token-based beautifier, three times faster than the beautifier in leoAst.py.

**Design**

The *pre_scan* method is the heart of the algorithm. It sets context
for the `:`, `=`, `**` and `.` tokens *without* using the parse tree.
*pre_scan* calls three *finishers*.

Each finisher uses a list of *relevant earlier tokens* to set the
context for one kind of (input) token. Finishers look behind (in the
stream of input tokens) with essentially no cost.

After the pre-scan, *tbo.beautify* (the main loop) calls *visitors*
for each separate type of *input* token.

Visitors call *code generators* to generate strings in the output
list, using *lazy evaluation* to generate whitespace.
"""
</t>
<t tx="ekr.20241007011856.34">__slots__ = [
    # Command-line arguments.
    'all', 'beautified', 'diff', 'report', 'write',

    # Global data.
    'contents', 'filename', 'input_tokens', 'output_list', 'tab_width',
    'insignificant_tokens',  # New.

    # Token-related data for visitors.
    'index', 'input_token', 'line_number',
    'pending_lws', 'pending_ws', 'prev_output_kind', 'prev_output_value',  # New.

    # Parsing state for visitors.
    'decorator_seen', 'in_arg_list', 'in_doc_part', 'state_stack', 'verbatim',

    # Whitespace state. Don't even *think* about changing these!
    'curly_brackets_level', 'indent_level', 'lws', 'paren_level', 'square_brackets_stack',

    # Regular expressions.
    'at_others_pat', 'beautify_pat', 'comment_pat', 'end_doc_pat',
    'nobeautify_pat', 'node_pat', 'start_doc_pat',
]
</t>
<t tx="ekr.20241007011856.35">insignificant_kinds = (
    'comment', 'dedent', 'encoding', 'endmarker', 'indent', 'newline', 'nl', 'ws',
)

# 'name' tokens that may appear in expressions.
operator_keywords = (
    'await',  # Debatable.
    'and', 'in', 'not', 'not in', 'or',  # Operators.
    'True', 'False', 'None',  # Values.
)
</t>
<t tx="ekr.20241007011856.36">def __init__(self, settings: Optional[Settings] = None):
    """Ctor for Orange class."""

    # Set default settings.
    if settings is None:
        settings = {}

    # Hard-code 4-space tabs.
    self.tab_width = 4

    # Define tokens even for empty files.
    self.input_token: InputToken = None
    self.input_tokens: list[InputToken] = []
    self.lws: str = ""  # Set only by Indent/Dedent tokens.
    self.pending_lws: str = ""
    self.pending_ws: str = ""

    # Set by gen_token and all do_* methods that bypass gen_token.
    self.prev_output_kind: str = None
    self.prev_output_value: str = None

    # Set ivars from the settings dict *without* using setattr.
    self.all = settings.get('all', False)
    self.beautified = settings.get('beautified', False)
    self.diff = settings.get('diff', False)
    self.report = settings.get('report', False)
    self.write = settings.get('write', False)

    # The list of tokens that tbo._next/_prev skip.
    self.insignificant_tokens = (
        'comment', 'dedent', 'indent', 'newline', 'nl', 'ws',
    )

    # General patterns.
    self.beautify_pat = re.compile(
        r'#\s*pragma:\s*beautify\b|#\s*@@beautify|#\s*@\+node|#\s*@[+-]others|#\s*@[+-]&lt;&lt;')
    self.comment_pat = re.compile(r'^(\s*)#[^@!# \n]')
    self.nobeautify_pat = re.compile(r'\s*#\s*pragma:\s*no\s*beautify\b|#\s*@@nobeautify')

    # Patterns from FastAtRead class, specialized for python delims.
    self.node_pat = re.compile(r'^(\s*)#@\+node:([^:]+): \*(\d+)?(\*?) (.*)$')  # @node
    self.start_doc_pat = re.compile(r'^\s*#@\+(at|doc)?(\s.*?)?$')  # @doc or @
    self.at_others_pat = re.compile(r'^(\s*)#@(\+|-)others\b(.*)$')  # @others

    # Doc parts end with @c or a node sentinel. Specialized for python.
    self.end_doc_pat = re.compile(r"^\s*#@(@(c(ode)?)|([+]node\b.*))$")
</t>
<t tx="ekr.20241007011856.37"></t>
<t tx="ekr.20241007011856.38">def dump_token_range(self, i1: int, i2: int, tag: Optional[str] = None) -&gt; None:  # pragma: no cover
    """Dump the given range of input tokens."""
    if tag:
        print(tag)
    for token in self.input_tokens[i1 : i2 + 1]:
        print(token.dump())
</t>
<t tx="ekr.20241007011856.39">def internal_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about an error in the beautifier itself."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        # '\n\n'
        'Error in token-based beautifier!\n'
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
        "Please report this message to Leo's developers"
    )
</t>
<t tx="ekr.20241007011856.4"></t>
<t tx="ekr.20241007011856.40">def user_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about a user error."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
    )
</t>
<t tx="ekr.20241007011856.41">def oops(self, message: str) -&gt; None:  # pragma: no cover
    """Raise InternalBeautifierError."""
    raise InternalBeautifierError(self.internal_error_message(message))
</t>
<t tx="ekr.20241007011856.42"></t>
<t tx="ekr.20241007011856.43">def no_visitor(self) -&gt; None:  # pragma: no cover
    self.oops(f"Unknown kind: {self.input_token.kind!r}")

def beautify(self,
    contents: str, filename: str, input_tokens: list[InputToken],
) -&gt; str:
    """
    The main line. Create output tokens and return the result as a string.

    beautify_file and beautify_file_def call this method.
    """
    &lt;&lt; tbo.beautify: init ivars &gt;&gt;

    try:
        # Pre-scan the token list, setting context.s
        self.pre_scan()

        # Init ivars first.
        self.input_token = None
        self.pending_lws = ''
        self.pending_ws = ''
        self.prev_output_kind = None
        self.prev_output_value = None

        # Init state.
        self.gen_token('file-start', '')
        self.push_state('file-start')

        # The main loop:
        prev_line_number: int = 0
        for self.index, self.input_token in enumerate(input_tokens):
            # Set global for visitors.
            if prev_line_number != self.input_token.line_number:
                prev_line_number = self.input_token.line_number
            # Call the proper visitor.
            if self.verbatim:
                self.do_verbatim()
            else:
                func = getattr(self, f"do_{self.input_token.kind}", self.no_visitor)
                func()

        # Return the result.
        result = ''.join(self.output_list)
        return result

    # Make no change if there is any error.
    except InternalBeautifierError as e:  # pragma: no cover
        # oops calls self.internal_error_message to creates e.
        print(repr(e))
    except AssertionError as e:  # pragma: no cover
        print(self.internal_error_message(repr(e)))
    return contents
</t>
<t tx="ekr.20241007011856.44"># Debugging vars...
self.contents = contents
self.filename = filename
self.line_number: Optional[int] = None

# The input and output lists...
self.output_list: list[str] = []
self.input_tokens = input_tokens  # The list of input tokens.

# State vars for whitespace.
self.curly_brackets_level = 0  # Number of unmatched '{' tokens.
self.paren_level = 0  # Number of unmatched '(' tokens.
self.square_brackets_stack: list[bool] = []  # A stack of bools, for self.gen_word().
self.indent_level = 0  # Set only by do_indent and do_dedent.

# Parse state.
self.decorator_seen = False  # Set by do_name for do_op.
self.in_arg_list = 0  # &gt; 0 if in an arg list of a def.
self.in_doc_part = False
self.state_stack: list[ParseState] = []  # Stack of ParseState objects.

# Leo-related state.
self.verbatim = False  # True: don't beautify.

# Ivars describing the present input token...
self.index = 0  # The index within the tokens array of the token being scanned.
self.lws = ''  # Leading whitespace. Required!
</t>
<t tx="ekr.20241007011856.45">def beautify_file(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    TokenBasedOrange: Beautify the the given external file.

    Return True if the file was beautified.
    """
    if 0:
        print(
            f"all: {int(self.all)} "
            f"beautified: {int(self.beautified)} "
            f"diff: {int(self.diff)} "
            f"report: {int(self.report)} "
            f"write: {int(self.write)} "
            f"{g.shortFileName(filename)}"
        )
    self.filename = filename
    contents, tokens = self.init_tokens_from_file(filename)
    if not (contents and tokens):
        return False  # Not an error.
    if not isinstance(tokens[0], InputToken):
        self.oops(f"Not an InputToken: {tokens[0]!r}")

    # Beautify the contents, returning the original contents on any error.
    results = self.beautify(contents, filename, tokens)

    # Ignore changes only to newlines.
    if self.regularize_newlines(contents) == self.regularize_newlines(results):
        return False

    # Print reports.
    if self.beautified:  # --beautified.
        print(f"tbo: beautified: {g.shortFileName(filename)}")
    if self.diff:  # --diff.
        print(f"Diffs: {filename}")
        self.show_diffs(contents, results)

    # Write the (changed) file .
    if self.write:  # --write.
        self.write_file(filename, results)
    return True
</t>
<t tx="ekr.20241007011856.46">def init_tokens_from_file(self, filename: str) -&gt; tuple[
    str, list[InputToken]
]:  # pragma: no cover
    """
    Create the list of tokens for the given file.
    Return (contents, encoding, tokens).
    """
    self.indent_level = 0
    self.filename = filename
    t1 = time.perf_counter_ns()
    contents = g.readFile(filename)
    t2 = time.perf_counter_ns()
    if not contents:
        self.input_tokens = []
        return '', []
    t3 = time.perf_counter_ns()
    self.input_tokens = input_tokens = Tokenizer().make_input_tokens(contents)
    t4 = time.perf_counter_ns()
    if 1:
        print(f"       read: {(t2-t1)/1000000:6.2f} ms")  # μ
        print(f"make_tokens: {(t4-t3)/1000000:6.2f} ms")
        print(f"      total: {(t4-t1)/1000000:6.2f} ms")
    return contents, input_tokens
</t>
<t tx="ekr.20241007011856.47">def regularize_newlines(self, s: str) -&gt; str:
    """Regularize newlines within s."""
    return s.replace('\r\n', '\n').replace('\r', '\n')
</t>
<t tx="ekr.20241007011856.48">def write_file(self, filename: str, s: str) -&gt; None:  # pragma: no cover
    """
    Write the string s to the file whose name is given.

    Handle all exceptions.

    Before calling this function, the caller should ensure
    that the file actually has been changed.
    """
    try:
        s2 = g.toEncodedString(s)  # May raise exception.
        with open(filename, 'wb') as f:
            f.write(s2)
    except Exception as e:  # pragma: no cover
        print(f"Error {e!r}: {filename!r}")
</t>
<t tx="ekr.20241007011856.49">def show_diffs(self, s1: str, s2: str) -&gt; None:  # pragma: no cover
    """Print diffs between strings s1 and s2."""
    filename = self.filename
    lines = list(difflib.unified_diff(
        g.splitLines(s1),
        g.splitLines(s2),
        fromfile=f"Old {filename}",
        tofile=f"New {filename}",
    ))
    print('')
    print(f"Diffs for {filename}")
    for line in lines:
        print(line)
</t>
<t tx="ekr.20241007011856.5">def dump_contents(contents: str, tag: str = 'Contents') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    for i, z in enumerate(g.splitLines(contents)):
        print(f"{i+1:&lt;3} ", z.rstrip())
    print('')
</t>
<t tx="ekr.20241007011856.50"># Visitors (tbo.do_* methods) handle input tokens.
# Generators (tbo.gen_* methods) create zero or more output tokens.
</t>
<t tx="ekr.20241007011856.51">def do_comment(self) -&gt; None:
    """Handle a comment token."""
    val = self.input_token.value
    &lt;&lt; do_comment: update comment-related state &gt;&gt;

    # Generate the comment.
    self.pending_lws = ''
    self.pending_ws = ''
    entire_line = self.input_token.line.lstrip().startswith('#')

    if entire_line:
        # The comment includes all ws.
        # #1496: No further munging needed.
        val = self.input_token.line.rstrip()
        # #3056: Insure one space after '#' in non-sentinel comments.
        #        Do not change bang lines or '##' comments.
        if m := self.comment_pat.match(val):
            i = len(m.group(1))
            val = val[:i] + '# ' + val[i + 1 :]
    else:
        # Exactly two spaces before trailing comments.
        val = '  ' + val.rstrip()
    self.gen_token('comment', val)
</t>
<t tx="ekr.20241007011856.52"># Leo-specific code...
if self.node_pat.match(val):
    # Clear per-node state.
    self.in_doc_part = False
    self.verbatim = False
    self.decorator_seen = False
    # Do *not* clear other state, which may persist across @others.
        # self.curly_brackets_level = 0
        # self.in_arg_list = 0
        # self.indent_level = 0
        # self.lws = ''
        # self.paren_level = 0
        # self.square_brackets_stack = []
        # self.state_stack = []
else:
    # Keep track of verbatim mode.
    if self.beautify_pat.match(val):
        self.verbatim = False
    elif self.nobeautify_pat.match(val):
        self.verbatim = True
    # Keep trace of @doc parts, to honor the convention for splitting lines.
    if self.start_doc_pat.match(val):
        self.in_doc_part = True
    if self.end_doc_pat.match(val):
        self.in_doc_part = False
</t>
<t tx="ekr.20241007011856.53">def do_dedent(self) -&gt; None:
    """Handle dedent token."""
    # Note: other methods use self.indent_level.
    self.indent_level -= 1
    self.lws = self.indent_level * self.tab_width * ' '
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'dedent'
</t>
<t tx="ekr.20241007011856.54">def do_encoding(self) -&gt; None:
    """Handle the encoding token."""
</t>
<t tx="ekr.20241007011856.55">def do_endmarker(self) -&gt; None:
    """Handle an endmarker token."""

    # Ensure exactly one newline at the end of file.
    if self.prev_output_kind not in (
        'indent', 'dedent', 'line-indent', 'newline',
    ):
        self.output_list.append('\n')
    self.pending_lws = ''  # Defensive.
    self.pending_ws = ''  # Defensive.
</t>
<t tx="ekr.20241007011856.56">consider_message = 'consider using python/Tools/scripts/reindent.py'

def do_indent(self) -&gt; None:
    """Handle indent token."""

    # Only warn about indentation errors.
    if '\t' in self.input_token.value:  # pragma: no cover
        print(f"Found tab character in {self.filename}")
        print(self.consider_message)
    elif (len(self.input_token.value) % self.tab_width) != 0:  # pragma: no cover
        print(f"Indentation error in {self.filename}")
        print(self.consider_message)

    # Handle the token!
    new_indent = self.input_token.value
    old_indent = self.indent_level * self.tab_width * ' '
    if new_indent &gt; old_indent:
        self.indent_level += 1
    elif new_indent &lt; old_indent:  # pragma: no cover (defensive)
        print(f"\n===== do_indent: can not happen {new_indent!r}, {old_indent!r}")

    self.lws = new_indent
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'indent'
</t>
<t tx="ekr.20241007011856.57"></t>
<t tx="ekr.20241007011856.58">def do_name(self) -&gt; None:
    """Handle a name token."""
    name = self.input_token.value
    if name in self.operator_keywords:
        self.gen_word_op(name)
    else:
        self.gen_word(name)
</t>
<t tx="ekr.20241007011856.59">def gen_word(self, s: str) -&gt; None:
    """Add a word request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word', s)
    self.gen_blank()
</t>
<t tx="ekr.20241007011856.6">def dump_lines(tokens: list[InputToken], tag: str = 'lines') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    for z in tokens:
        if z.line.strip():
            print(z.line.rstrip())
        else:
            print(repr(z.line))
    print('')
</t>
<t tx="ekr.20241007011856.60">def gen_word_op(self, s: str) -&gt; None:
    """Add a word-op request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word-op', s)
    self.gen_blank()
</t>
<t tx="ekr.20241007011856.61"></t>
<t tx="ekr.20241007011856.62">def do_newline(self) -&gt; None:
    """
    do_newline: Handle a regular newline.

    From https://docs.python.org/3/library/token.html

    NEWLINE tokens end *logical* lines of Python code.
    """

    self.output_list.append('\n')
    self.pending_lws = ''  # Set only by 'dedent', 'indent' or 'ws' tokens.
    self.pending_ws = ''
    self.prev_output_kind = 'newline'
    self.prev_output_value = '\n'
</t>
<t tx="ekr.20241007011856.63">def do_nl(self) -&gt; None:
    """
    do_nl: Handle a continuation line.

    From https://docs.python.org/3/library/token.html

    NL tokens end *physical* lines. They appear when when a logical line of
    code spans multiple physical lines.
    """
    return self.do_newline()
</t>
<t tx="ekr.20241007011856.64">def do_number(self) -&gt; None:
    """Handle a number token."""
    self.gen_blank()
    self.gen_token('number', self.input_token.value)
</t>
<t tx="ekr.20241007011856.65"></t>
<t tx="ekr.20241007011856.66">def do_op(self) -&gt; None:
    """Handle an op token."""
    val = self.input_token.value

    if val == '.':
        self.gen_dot_op()
    elif val == '@':
        self.gen_token('op-no-blanks', val)
        self.push_state('decorator')
    elif val == ':':
        # Treat slices differently.
        self.gen_colon()
    elif val in ',;':
        # Pep 8: Avoid extraneous whitespace immediately before
        # comma, semicolon, or colon.
        self.pending_ws = ''
        self.gen_token('op', val)
        self.gen_blank()
    elif val in '([{':
        # Pep 8: Avoid extraneous whitespace immediately inside
        # parentheses, brackets or braces.
        self.gen_lt()
    elif val in ')]}':
        # Ditto.
        self.gen_rt()
    elif val == '=':
        self.gen_equal_op()
    elif val in '~+-':
        self.gen_possible_unary_op()
    elif val == '*':
        self.gen_star_op()
    elif val == '**':
        self.gen_star_star_op()
    else:
        # Pep 8: always surround binary operators with a single space.
        # '==','+=','-=','*=','**=','/=','//=','%=','!=','&lt;=','&gt;=','&lt;','&gt;',
        # '^','~','*','**','&amp;','|','/','//',
        # Pep 8: If operators with different priorities are used, consider
        # adding whitespace around the operators with the lowest priorities.
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20241007011856.67">def gen_colon(self) -&gt; None:
    """Handle a colon."""
    val = self.input_token.value
    context = self.input_token.context

    self.pending_ws = ''
    if context == 'complex-slice':
        if self.prev_output_value not in '[:':
            self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
    elif context == 'simple-slice':
        self.gen_token('op-no-blanks', val)
    elif context == 'dict':
        self.gen_token('op', val)
        self.gen_blank()
    else:
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20241007011856.68">def gen_dot_op(self) -&gt; None:
    """Handle the '.' input token."""
    context = self.input_token.context

    # Get the previous significant **input** token.
    # This is the only call to next(i) anywhere!
    next_i = self._next(self.index)
    next = 'None' if next_i is None else self.input_tokens[next_i]
    import_is_next = next and next.kind == 'name' and next.value == 'import'

    if context == 'import':
        if (
            self.prev_output_kind == 'word'
            and self.prev_output_value in ('from', 'import')
        ):
            self.gen_blank()
            op = 'op' if import_is_next else 'op-no-blanks'
            self.gen_token(op, '.')
        elif import_is_next:
            self.gen_token('op', '.')
            self.gen_blank()
        else:
            self.pending_ws = ''
            self.gen_token('op-no-blanks', '.')
    else:
        self.pending_ws = ''
        self.gen_token('op-no-blanks', '.')
</t>
<t tx="ekr.20241007011856.69">def _next(self, i: int) -&gt; Optional[int]:
    """
    Return the next *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.

    The **Global Token Ratio** is tbo.n_scanned_tokens / len(tbo.tokens),
    where tbo.n_scanned_tokens is the total number of calls calls to
    tbo.next or tbo.prev.

    For Leo's sources, this ratio ranges between 0.48 and 1.51!

    The orange_command function warns if this ratio is greater than 2.5.
    Previous versions of this code suffered much higher ratios.
    """
    i += 1
    while i &lt; len(self.input_tokens):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            # g.trace(f"token: {token!r}")
            return i
        i += 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20241007011856.7">def dump_results(results: list[str], tag: str = 'Results') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    print(''.join(results))
    print('')
</t>
<t tx="ekr.20241007011856.70">def gen_equal_op(self) -&gt; None:

    val = self.input_token.value
    context = self.input_token.context

    if context == 'initializer':
        # Pep 8: Don't use spaces around the = sign when used to indicate
        #        a keyword argument or a default parameter value.
        #        However, when combining an argument annotation with a default value,
        #        *do* use spaces around the = sign.
        self.pending_ws = ''
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20241007011856.71">def gen_lt(self) -&gt; None:
    """Generate code for a left paren or curly/square bracket."""
    val = self.input_token.value
    assert val in '([{', repr(val)

    # Update state vars.
    if val == '(':
        self.paren_level += 1
    elif val == '[':
        self.square_brackets_stack.append(False)
    else:
        self.curly_brackets_level += 1

    # Generate or suppress the leading blank.
    # Update self.in_arg_list if necessary.
    if self.input_token.context == 'import':
        self.gen_blank()
    elif self.prev_output_kind in ('op', 'word-op'):
        self.gen_blank()
    elif self.prev_output_kind == 'word':
        # Only suppress blanks before '(' or '[' for non-keywords.
        if val == '{' or self.prev_output_value in (
            'if', 'else', 'elif', 'return', 'for', 'while',
        ):
            self.gen_blank()
        elif val == '(':
            self.in_arg_list += 1
            self.pending_ws = ''
        else:
            self.pending_ws = ''
    elif self.prev_output_kind != 'line-indent':
        self.pending_ws = ''

    # Output the token!
    self.gen_token('op-no-blanks', val)
</t>
<t tx="ekr.20241007011856.72">def gen_possible_unary_op(self) -&gt; None:
    """Add a unary or binary op to the token list."""
    val = self.input_token.value
    if self.is_unary_op(self.index, val):
        prev = self.input_token
        if prev.kind == 'lt':
            self.gen_token('op-no-blanks', val)
        else:
            self.gen_blank()
            self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()

</t>
<t tx="ekr.20241007011856.73">def is_unary_op(self, i: int, val: str) -&gt; bool:

    if val == '~':
        return True
    if val not in '+-':  # pragma: no cover
        return False

    # Get the previous significant **input** token.
    # This is the only call to _prev(i) anywhere!
    prev_i = self._prev(i)
    prev_token = None if prev_i is None else self.input_tokens[prev_i]
    kind = prev_token.kind if prev_token else ''
    value = prev_token.value if prev_token else ''

    if kind in ('number', 'string'):
        return_val = False
    elif kind == 'op' and value in ')]':
        return_val = False
    elif kind == 'op' and value in '{([:':
        return_val = True
    elif kind != 'name':
        return_val = True
    else:
        # The hard case: prev_token is a 'name' token.
        # Any Python keyword indicates a unary operator.
        return_val = keyword.iskeyword(value) or keyword.issoftkeyword(value)
    return return_val
</t>
<t tx="ekr.20241007011856.74">def _prev(self, i: int) -&gt; Optional[int]:
    """
    Return the previous *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.
    """
    i -= 1
    while i &gt;= 0:
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            return i
        i -= 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20241007011856.75">def gen_rt(self) -&gt; None:
    """Generate code for a right paren or curly/square bracket."""
    val = self.input_token.value
    assert val in ')]}', repr(val)

    # Update state vars.
    if val == ')':
        self.paren_level -= 1
        self.in_arg_list = max(0, self.in_arg_list - 1)
    elif val == ']':
        self.square_brackets_stack.pop()
    else:
        self.curly_brackets_level -= 1

    if self.prev_output_kind != 'line-indent':
        self.pending_ws = ''
    self.gen_token('rt', val)
</t>
<t tx="ekr.20241007011856.76">def gen_star_op(self) -&gt; None:
    """Put a '*' op, with special cases for *args."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20241007011856.77">def gen_star_star_op(self) -&gt; None:
    """Put a ** operator, with a special case for **kwargs."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20241007011856.78">def push_state(self, kind: str, value: Union[int, str, None] = None) -&gt; None:
    """Append a state to the state stack."""
    state = ParseState(kind, value)
    self.state_stack.append(state)
</t>
<t tx="ekr.20241007011856.79">def do_string(self) -&gt; None:
    """
    Handle a 'string' token.

    The Tokenizer converts all f-string tokens to a single 'string' token.
    """
    # Careful: continued strings may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    self.gen_token('string', val)
    self.gen_blank()
</t>
<t tx="ekr.20241007011856.8">def dump_tokens(tokens: list[InputToken], tag: str = 'Tokens') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    if not tokens:
        return
    print(
        "Note: values shown are repr(value) "
        "*except* for 'string' and 'fstring*' tokens."
    )
    tokens[0].dump_header()
    for z in tokens:
        print(z.dump())
    print('')
</t>
<t tx="ekr.20241007011856.80">def do_verbatim(self) -&gt; None:
    """
    Handle one token in verbatim mode.
    End verbatim mode when the appropriate comment is seen.
    """
    kind = self.input_token.kind
    #
    # Careful: tokens may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    if kind == 'comment':
        if self.beautify_pat.match(val):
            self.verbatim = False
        val = val.rstrip()
        self.gen_token('comment', val)
        return
    if kind == 'indent':
        self.indent_level += 1
        self.lws = self.indent_level * self.tab_width * ' '
    if kind == 'dedent':
        self.indent_level -= 1
        self.lws = self.indent_level * self.tab_width * ' '
    self.gen_token('verbatim', val)
</t>
<t tx="ekr.20241007011856.81">def do_ws(self) -&gt; None:
    """
    Handle the "ws" pseudo-token.  See Tokenizer.itok.do_token (the gem).

    Put the whitespace only if if ends with backslash-newline.
    """
    val = self.input_token.value
    last_token = self.input_tokens[self.index - 1]

    if last_token.kind in ('nl', 'newline'):
        self.pending_lws = val
        self.pending_ws = ''
    elif '\\\n' in val:
        self.pending_lws = ''
        self.pending_ws = val
    else:
        self.pending_ws = val
</t>
<t tx="ekr.20241007011856.82">def gen_blank(self) -&gt; None:
    """
    Queue a *request* a blank.
    Change *neither* prev_output_kind *nor* pending_lws.
    """

    prev_kind = self.prev_output_kind
    if prev_kind == 'op-no-blanks':
        # A demand that no blank follows this op.
        self.pending_ws = ''
    elif prev_kind == 'hard-blank':
        # Eat any further blanks.
        self.pending_ws = ''
    elif prev_kind in (
        'dedent',
        'file-start',
        'indent',
        'line-indent',
        'newline',
    ):
        # Suppress the blank, but do *not* change the pending ws.
        pass
    elif self.pending_ws:
        # Use the existing pending ws.
        pass
    else:
        self.pending_ws = ' '
</t>
<t tx="ekr.20241007011856.83">def gen_token(self, kind: str, value: Any) -&gt; None:
    """Add an output token to the code list."""

    if self.pending_lws:
        self.output_list.append(self.pending_lws)
    elif self.pending_ws:
        self.output_list.append(self.pending_ws)

    self.output_list.append(value)
    self.pending_lws = ''
    self.pending_ws = ''
    self.prev_output_value = value
    self.prev_output_kind = kind
</t>
<t tx="ekr.20241007011856.84"># The parser calls scanner methods to move through the list of input tokens.
</t>
<t tx="ekr.20241007011856.85">def pre_scan(self) -&gt; None:
    """
    Scan the entire file in one iterative pass, adding context to a few
    kinds of tokens as follows:

    Token   Possible Contexts (or None)
    =====   ===========================
    ':'     'annotation', 'dict', 'complex-slice', 'simple-slice'
    '='     'annotation', 'initializer'
    '*'     'arg'
    '**'    'arg'
    '.'     'import'
    """

    # The main loop.
    in_import = False
    scan_stack: list[ScanState] = []
    prev_token: Optional[InputToken] = None
    for i, token in enumerate(self.input_tokens):
        kind, value = token.kind, token.value
        if kind in 'newline':
            &lt;&lt; pre-scan 'newline' tokens &gt;&gt;
        elif kind == 'op':
            &lt;&lt; pre-scan 'op' tokens &gt;&gt;
        elif kind == 'name':
            &lt;&lt; pre-scan 'name' tokens &gt;&gt;
        # Remember the previous significant token.
        if kind not in self.insignificant_kinds:
            prev_token = token
    # Sanity check.
    if scan_stack:  # pragma: no cover
        print('pre_scan: non-empty scan_stack')
        print(scan_stack)
</t>
<t tx="ekr.20241007011856.86"># 'import' and 'from x import' statements may span lines.
# 'ws' tokens represent continued lines like this:   ws: ' \\\n    '
if in_import and not scan_stack:
    in_import = False
</t>
<t tx="ekr.20241007011856.87">top_state: Optional[ScanState] = scan_stack[-1] if scan_stack else None

# Handle '[' and ']'.
if value == '[':
    scan_stack.append(ScanState('slice', token))
elif value == ']':
    assert top_state and top_state.kind == 'slice'
    self.finish_slice(i, top_state)
    scan_stack.pop()

# Handle '{' and '}'.
if value == '{':
    scan_stack.append(ScanState('dict', token))
elif value == '}':
    assert top_state and top_state.kind == 'dict'
    self.finish_dict(i, top_state)
    scan_stack.pop()

# Handle '(' and ')'
elif value == '(':
    if self.is_python_keyword(prev_token) or prev_token and prev_token.kind != 'name':
        state_kind = '('
    else:
        state_kind = 'arg'
    scan_stack.append(ScanState(state_kind, token))
elif value == ')':
    assert top_state and top_state.kind in ('(', 'arg'), repr(top_state)
    if top_state.kind == 'arg':
        self.finish_arg(i, top_state)
    scan_stack.pop()

# Handle interior tokens in 'arg' and 'slice' states.
if top_state:
    if top_state.kind in ('dict', 'slice') and value == ':':
        top_state.value.append(i)
    if top_state.kind == 'arg' and value in '**=:,':
        top_state.value.append(i)

# Handle '.' and '(' tokens inside 'import' and 'from' statements.
if in_import and value in '(.':
    self.set_context(i, 'import')
</t>
<t tx="ekr.20241007011856.88">prev_is_yield = prev_token and prev_token.kind == 'name' and prev_token.value == 'yield'
if value in ('from', 'import') and not prev_is_yield:
    # 'import' and 'from x import' statements should be at the outer level.
    assert not scan_stack, scan_stack
    in_import = True
</t>
<t tx="ekr.20241007011856.89">def finish_arg(self, end: int, state: Optional[ScanState]) -&gt; None:
    """Set context for all ':' when scanning from '(' to ')'."""

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'arg', repr(state)
    token = state.token
    assert token.value == '(', repr(token)
    values = state.value
    assert isinstance(values, list), repr(values)
    i1 = token.index
    assert i1 &lt; end, (i1, end)
    if not values:
        return

    # Compute the context for each *separate* '=' token.
    equal_context = 'initializer'
    for i in values:
        token = self.input_tokens[i]
        assert token.kind == 'op', repr(token)
        if token.value == ',':
            equal_context = 'initializer'
        elif token.value == ':':
            equal_context = 'annotation'
        elif token.value == '=':
            self.set_context(i, equal_context)
            equal_context = 'initializer'

    # Set the context of all outer-level ':', '*', and '**' tokens.
    prev: Optional[InputToken] = None
    for i in range(i1, end):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_kinds:
            if token.kind == 'op':
                if token.value in ('*', '**'):
                    if self.is_unary_op_with_prev(prev, token):
                        self.set_context(i, 'arg')
                elif token.value == '=':
                    # The code above has set the context.
                    assert token.context in ('initializer', 'annotation'), (i, repr(token.context))
                elif token.value == ':':
                    self.set_context(i, 'annotation')
            prev = token
</t>
<t tx="ekr.20241007011856.9">def input_tokens_to_string(tokens: list[InputToken]) -&gt; str:  # pragma: no cover
    """Return the string represented by the list of tokens."""
    if tokens is None:
        # This indicates an internal error.
        print('')
        print('===== input token list is None ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20241007011856.90">def finish_slice(self, end: int, state: ScanState) -&gt; None:
    """Set context for all ':' when scanning from '[' to ']'."""

    # Sanity checks.
    assert state.kind == 'slice', repr(state)
    token = state.token
    assert token.value == '[', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Do nothing if there are no ':' tokens in the slice.
    if not colons:
        return

    # Compute final context by scanning the tokens.
    final_context = 'simple-slice'
    inter_colon_tokens = 0
    prev = token
    for i in range(i1 + 1, end - 1):
        token = self.input_tokens[i]
        kind, value = token.kind, token.value
        if kind not in self.insignificant_kinds:
            if kind == 'op':
                if value == '.':
                    # Ignore '.' tokens and any preceding 'name' token.
                    if prev and prev.kind == 'name':  # pragma: no cover
                        inter_colon_tokens -= 1
                elif value == ':':
                    inter_colon_tokens = 0
                elif value in '-+':
                    # Ignore unary '-' or '+' tokens.
                    if not self.is_unary_op_with_prev(prev, token):
                        inter_colon_tokens += 1
                        if inter_colon_tokens &gt; 1:
                            final_context = 'complex-slice'
                            break
                elif value == '~':
                    # '~' is always a unary op.
                    pass
                else:
                    # All other ops contribute.
                    inter_colon_tokens += 1
                    if inter_colon_tokens &gt; 1:
                        final_context = 'complex-slice'
                        break
            else:
                inter_colon_tokens += 1
                if inter_colon_tokens &gt; 1:
                    final_context = 'complex-slice'
                    break
            prev = token

    # Set the context of all outer-level ':' tokens.
    for i in colons:
        self.set_context(i, final_context)
</t>
<t tx="ekr.20241007011856.91">def finish_dict(self, end: int, state: Optional[ScanState]) -&gt; None:
    """
    Set context for all ':' when scanning from '{' to '}'

    Strictly speaking, setting this context is unnecessary because
    tbo.gen_colon generates the same code regardless of this context.

    In other words, this method can be a do-nothing!
    """

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'dict', repr(state)
    token = state.token
    assert token.value == '{', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Set the context for all ':' tokens.
    for i in colons:
        self.set_context(i, 'dict')
</t>
<t tx="ekr.20241007011856.92">def is_unary_op_with_prev(self, prev: Optional[InputToken], token: InputToken) -&gt; bool:
    """
    Return True if token is a unary op in the context of prev, the previous
    significant token.
    """
    if token.value == '~':  # pragma: no cover
        return True
    if prev is None:
        return True  # pragma: no cover
    assert token.value in '**-+', repr(token.value)
    if prev.kind in ('number', 'string'):
        return_val = False
    elif prev.kind == 'op' and prev.value in ')]':
         # An unnecessary test?
        return_val = False  # pragma: no cover
    elif prev.kind == 'op' and prev.value in '{([:,':
        return_val = True
    elif prev.kind != 'name':
        # An unnecessary test?
        return_val = True  # pragma: no cover
    else:
        # prev is a'name' token.
        return self.is_python_keyword(token)
    return return_val
</t>
<t tx="ekr.20241007011856.93">def is_python_keyword(self, token: Optional[InputToken]) -&gt; bool:
    """Return True if token is a 'name' token referring to a Python keyword."""
    if not token or token.kind != 'name':
        return False
    return keyword.iskeyword(token.value) or keyword.issoftkeyword(token.value)
</t>
<t tx="ekr.20241007011856.94">def set_context(self, i: int, context: str) -&gt; None:
    """
    Set self.input_tokens[i].context, but only if it does not already exist!

    See the docstring for pre_scan for details.
    """

    trace = False  # Do not delete the trace below.

    valid_contexts = (
        'annotation', 'arg', 'complex-slice', 'simple-slice',
        'dict', 'import', 'initializer',
    )
    if context not in valid_contexts:
        self.oops(f"Unexpected context! {context!r}")  # pragma: no cover

    token = self.input_tokens[i]

    if trace:  # pragma: no cover
        token_s = f"&lt;{token.kind}: {token.show_val(12)}&gt;"
        ignore_s = 'Ignore' if token.context else ' ' * 6
        print(f"{i:3} {ignore_s} token: {token_s} context: {context}")

    if not token.context:
        token.context = context
</t>
<t tx="ekr.20241007063926.1"></t>
<t tx="ekr.20241007085552.1">"dummy", // placeholder so the token stack is never empty.
"ws",  // pseudo-token.
"Comment", "Dedent", "Indent", "Newline", "Nl",  // Real tokens.
</t>
<t tx="ekr.20241007085705.1">"And",
"Colon", "ColonEqual", "Comma", "Dot", "DoubleStar",
"Equal", "EqEqual", "Greater", "GreaterEqual",
"Is",
"Less", "LessEqual", "Lbrace", "Lpar", "Lsqb",
"Minus", "MinusEqual",
"Not", "NotEqual",
"Or",
"Percent", "Plus", "PlusEqual",
"Rarrow", "Rbrace", "Rpar", "Rsqb",
"Star",
</t>
<t tx="ekr.20241007130048.1"># ekr-tbo-in-rust

Leo's Token Based beautifier (TBO) transliterated to Rust.

This is a learning project. It won't ever be fast enough to replace Leo's Python beautifier.
</t>
<t tx="ekr.20241007141942.1">@nosearch

def pre_scan(self) -&gt; None:
    """
    Scan the entire file in one iterative pass, adding context to a few
    kinds of tokens as follows:

    Token   Possible Contexts (or None)
    =====   ===========================
    ':'     'annotation', 'dict', 'complex-slice', 'simple-slice'
    '='     'annotation', 'initializer'
    '*'     'arg'
    '**'    'arg'
    '.'     'import'
    """

    # The main loop.
    in_import = False
    scan_stack: list[ScanState] = []
    prev_token: Optional[InputToken] = None
    for i, token in enumerate(self.input_tokens):
        kind, value = token.kind, token.value
        if kind in 'newline':
            &lt;&lt; pre-scan 'newline' tokens &gt;&gt;
        elif kind == 'op':
            &lt;&lt; pre-scan 'op' tokens &gt;&gt;
        elif kind == 'name':
            &lt;&lt; pre-scan 'name' tokens &gt;&gt;
        # Remember the previous significant token.
        if kind not in self.insignificant_kinds:
            prev_token = token
    # Sanity check.
    if scan_stack:  # pragma: no cover
        print('pre_scan: non-empty scan_stack')
        print(scan_stack)
</t>
<t tx="ekr.20241007141942.2"># 'import' and 'from x import' statements may span lines.
# 'ws' tokens represent continued lines like this:   ws: ' \\\n    '
if in_import and not scan_stack:
    in_import = False
</t>
<t tx="ekr.20241007141942.3">top_state: Optional[ScanState] = scan_stack[-1] if scan_stack else None

# Handle '[' and ']'.
if value == '[':
    scan_stack.append(ScanState('slice', token))
elif value == ']':
    assert top_state and top_state.kind == 'slice'
    self.finish_slice(i, top_state)
    scan_stack.pop()

# Handle '{' and '}'.
if value == '{':
    scan_stack.append(ScanState('dict', token))
elif value == '}':
    assert top_state and top_state.kind == 'dict'
    self.finish_dict(i, top_state)
    scan_stack.pop()

# Handle '(' and ')'
elif value == '(':
    if self.is_python_keyword(prev_token) or prev_token and prev_token.kind != 'name':
        state_kind = '('
    else:
        state_kind = 'arg'
    scan_stack.append(ScanState(state_kind, token))
elif value == ')':
    assert top_state and top_state.kind in ('(', 'arg'), repr(top_state)
    if top_state.kind == 'arg':
        self.finish_arg(i, top_state)
    scan_stack.pop()

# Handle interior tokens in 'arg' and 'slice' states.
if top_state:
    if top_state.kind in ('dict', 'slice') and value == ':':
        top_state.value.append(i)
    if top_state.kind == 'arg' and value in '**=:,':
        top_state.value.append(i)

# Handle '.' and '(' tokens inside 'import' and 'from' statements.
if in_import and value in '(.':
    self.set_context(i, 'import')
</t>
<t tx="ekr.20241007141942.4">prev_is_yield = prev_token and prev_token.kind == 'name' and prev_token.value == 'yield'
if value in ('from', 'import') and not prev_is_yield:
    # 'import' and 'from x import' statements should be at the outer level.
    assert not scan_stack, scan_stack
    in_import = True
</t>
<t tx="ekr.20241007141942.5">def finish_arg(self, end: int, state: Optional[ScanState]) -&gt; None:
    """Set context for all ':' when scanning from '(' to ')'."""

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'arg', repr(state)
    token = state.token
    assert token.value == '(', repr(token)
    values = state.value
    assert isinstance(values, list), repr(values)
    i1 = token.index
    assert i1 &lt; end, (i1, end)
    if not values:
        return

    # Compute the context for each *separate* '=' token.
    equal_context = 'initializer'
    for i in values:
        token = self.input_tokens[i]
        assert token.kind == 'op', repr(token)
        if token.value == ',':
            equal_context = 'initializer'
        elif token.value == ':':
            equal_context = 'annotation'
        elif token.value == '=':
            self.set_context(i, equal_context)
            equal_context = 'initializer'

    # Set the context of all outer-level ':', '*', and '**' tokens.
    prev: Optional[InputToken] = None
    for i in range(i1, end):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_kinds:
            if token.kind == 'op':
                if token.value in ('*', '**'):
                    if self.is_unary_op_with_prev(Some(prev), token):
                        self.set_context(i, 'arg')
                elif token.value == '=':
                    # The code above has set the context.
                    assert token.context in ('initializer', 'annotation'), (i, repr(token.context))
                elif token.value == ':':
                    self.set_context(i, 'annotation')
            prev = token
</t>
<t tx="ekr.20241007141942.6">def finish_slice(self, end: int, state: ScanState) -&gt; None:
    """Set context for all ':' when scanning from '[' to ']'."""

    # Sanity checks.
    assert state.kind == 'slice', repr(state)
    token = state.token
    assert token.value == '[', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Do nothing if there are no ':' tokens in the slice.
    if not colons:
        return

    # Compute final context by scanning the tokens.
    final_context = 'simple-slice'
    inter_colon_tokens = 0
    prev = token
    for i in range(i1 + 1, end - 1):
        token = self.input_tokens[i]
        kind, value = token.kind, token.value
        if kind not in self.insignificant_kinds:
            if kind == 'op':
                if value == '.':
                    # Ignore '.' tokens and any preceding 'name' token.
                    if prev and prev.kind == 'name':  # pragma: no cover
                        inter_colon_tokens -= 1
                elif value == ':':
                    inter_colon_tokens = 0
                elif value in '-+':
                    # Ignore unary '-' or '+' tokens.
                    if not self.is_unary_op_with_prev(prev, token):
                        inter_colon_tokens += 1
                        if inter_colon_tokens &gt; 1:
                            final_context = 'complex-slice'
                            break
                elif value == '~':
                    # '~' is always a unary op.
                    pass
                else:
                    # All other ops contribute.
                    inter_colon_tokens += 1
                    if inter_colon_tokens &gt; 1:
                        final_context = 'complex-slice'
                        break
            else:
                inter_colon_tokens += 1
                if inter_colon_tokens &gt; 1:
                    final_context = 'complex-slice'
                    break
            prev = token

    # Set the context of all outer-level ':' tokens.
    for i in colons:
        self.set_context(i, final_context)
</t>
<t tx="ekr.20241007141942.7">def finish_dict(self, end: int, state: Optional[ScanState]) -&gt; None:
    """
    Set context for all ':' when scanning from '{' to '}'

    Strictly speaking, setting this context is unnecessary because
    tbo.gen_colon generates the same code regardless of this context.

    In other words, this method can be a do-nothing!
    """

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'dict', repr(state)
    token = state.token
    assert token.value == '{', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Set the context for all ':' tokens.
    for i in colons:
        self.set_context(i, 'dict')
</t>
<t tx="ekr.20241007144301.1"></t>
<t tx="ekr.20241007184818.1"></t>
</tnodes>
</leo_file>
