<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: https://leo-editor.github.io/leo-editor/leo_toc.html -->
<leo_file xmlns:leo="https://leo-editor.github.io/leo-editor/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20240927151701.1" descendentVnodeUnknownAttributes="7d7100285803000000302e3071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a73735805000000302e302e3771067d71075808000000616e6e6f7461746571087d71092858080000007072696f72697479710a4d0f27580a00000070726973657464617465710b580a000000323032312d30332d3330710c75735803000000302e31710d7d710e580b0000005f5f626f6f6b6d61726b73710f7d7110580700000069735f6475706571114930300a7373752e"><vh>Startup</vh>
<v t="ekr.20240927151701.9" descendentVnodeUnknownAttributes="7d71002858010000003071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a73735803000000302e3771067d71075808000000616e6e6f7461746571087d71092858080000007072696f72697479710a4d0f27580a00000070726973657464617465710b580a000000323032312d30332d3330710c7573752e"><vh>@settings</vh>
<v t="ekr.20240927151701.44"><vh>@bool allow-text-zoom = True</vh></v>
<v t="ekr.20240927151701.45"><vh>@bool check-python-code-on-write = False</vh></v>
<v t="ekr.20240927151701.46"><vh>@bool use-german-keyboard = False</vh></v>
<v t="ekr.20240927151701.47"><vh>@bool use-mouse-expand-gestures = False</vh></v>
<v t="ekr.20240927151701.48"><vh>@data exec-script-commands</vh></v>
<v t="ekr.20240927151701.49"><vh>@data exec-script-patterns</vh></v>
<v t="ekr.20240927151701.50"><vh>@data history-list</vh></v>
<v t="ekr.20240927151701.170" descendentVnodeUnknownAttributes="7d710058010000003071017d71025808000000616e6e6f7461746571037d71042858080000007072696f7269747971054d0f27580a000000707269736574646174657106580a000000323032312d30332d333071077573732e"><vh>@enabled-plugins</vh></v>
<v t="ekr.20240927151701.51"><vh>@string qt-layout-name = legacy</vh></v>
<v t="ekr.20240927151701.63"><vh>Abbreviation settings</vh>
<v t="ekr.20240927151701.64"><vh>@bool enable-abbreviations = True</vh></v>
<v t="ekr.20240927151701.65"><vh>@outline-data tree-abbreviations</vh>
<v t="ekr.20240927151701.66"><vh>@organizer 1</vh>
<v t="ekr.20240927151701.67"><vh>@organizer 2</vh>
<v t="ekr.20240927151701.68"><vh>demo;;</vh>
<v t="ekr.20240927151701.69"><vh>@@button MyDemo @key=Ctrl-9</vh>
<v t="ekr.20240927151701.70"><vh>&lt;&lt; imports &gt;&gt;</vh></v>
<v t="ekr.20240927151701.71"><vh>script_string</vh></v>
<v t="ekr.20240927151701.72"><vh>class myDemo</vh></v>
<v t="ekr.20240927151701.73"><vh>wrappers</vh></v>
</v>
</v>
</v>
<v t="ekr.20240927151701.74"><vh>per-commander-plugin;;</vh>
<v t="ekr.20240927151701.75"><vh>@@file pluginname.py</vh>
<v t="ekr.20240927151701.76"><vh>&lt;&lt; docstring &gt;&gt;</vh></v>
<v t="ekr.20240927151701.77"><vh>&lt;&lt; version history &gt;&gt;</vh></v>
<v t="ekr.20240927151701.78"><vh>&lt;&lt; imports &gt;&gt;</vh></v>
<v t="ekr.20240927151701.79"><vh>init</vh></v>
<v t="ekr.20240927151701.80"><vh>onCreate</vh></v>
<v t="ekr.20240927151701.81"><vh>class pluginController</vh>
<v t="ekr.20240927151701.82"><vh>__init__</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20240927151701.83"><vh>importer;;</vh>
<v t="ekr.20240927151701.84"><vh>@@file importers/{|{x=name}|}.py</vh>
<v t="ekr.20240927151701.85"><vh>class {|{x=cap_name}|}_Importer</vh>
<v t="ekr.20240927151701.86"><vh>{|{x=name}|}.Overrides</vh>
<v t="ekr.20240927151701.87"><vh>{|{x=name}|}.clean_headline</vh></v>
<v t="ekr.20240927151701.88"><vh>{|{x=name}|}.clean_nodes</vh></v>
</v>
</v>
<v t="ekr.20240927151701.89"><vh>class class {|{x=cap_name}|}_ScanState</vh>
<v t="ekr.20240927151701.90"><vh>{|{x=name}|}_state.level</vh></v>
<v t="ekr.20240927151701.91"><vh>{|{x=name}|}_state.update</vh></v>
</v>
</v>
</v>
</v>
</v>
<v t="ekr.20240927151701.92"><vh>Appearance settings</vh>
<v t="ekr.20240927151701.93"><vh>@bool log-pane-wraps = False</vh></v>
<v t="ekr.20240927151701.94"><vh>@bool recent-files-group-always = True</vh></v>
<v t="ekr.20240927151701.95"><vh>@bool show-iconbar = True</vh></v>
<v t="ekr.20240927151701.96"><vh>@bool show-tips = False</vh></v>
<v t="ekr.20240927151701.97"><vh>@bool stayInTreeAfterSelect = True</vh></v>
<v t="ekr.20240927151701.98"><vh>@bool use-chapter-tabs = False</vh></v>
<v t="ekr.20240927151701.99"><vh>@bool use-chapters = False</vh></v>
<v t="ekr.20240927151701.100"><vh>@bool use-gutter = False</vh></v>
<v t="ekr.20240927151701.101"><vh>@int qweb-view-font-size = 30</vh></v>
<v t="ekr.20240927151701.102"><vh>@string initial-split-orientation = v</vh></v>
</v>
<v t="ekr.20240927151701.103"><vh>Coloring settings</vh>
<v t="ekr.20240927151701.104"><vh>@bool color-doc-parts-as-rest = True</vh></v>
<v t="ekr.20240927151701.105"><vh>@bool use-pygments = False</vh></v>
<v t="ekr.20240927151701.106"><vh>@bool use-pygments-styles = False</vh></v>
<v t="ekr.20240927151701.107"><vh>@color head-bg = @mistyrose2</vh></v>
<v t="ekr.20240927151701.108"><vh>@string pygments-style-name = leonine</vh></v>
<v t="ekr.20240927151701.109"><vh>@string target-language = rust</vh></v>
</v>
<v t="ekr.20240927151701.110"><vh>Command settings</vh>
<v t="ekr.20240927151701.111"><vh>@bool create-at-persistence-nodes-automatically = False</vh></v>
<v t="ekr.20240927151701.112"><vh>@bool enable-persistence = False</vh></v>
<v t="ekr.20240927151701.113"><vh>@bool make-node-conflicts-node = True</vh></v>
<v t="ekr.20240927151701.184"><vh>@bool qt-use-scintilla = False</vh></v>
<v t="ekr.20240927151701.114"><vh>@bool run-pyflakes-on-write = False</vh></v>
<v t="ekr.20240927151701.126"><vh>@bool tree-declutter = False</vh></v>
<v t="ekr.20240927151701.115"><vh>@bool use-jedi = False</vh></v>
<v t="ekr.20240927151701.116"><vh>@bool use-qcompleter = False</vh></v>
<v t="ekr.20240927151701.202"><vh>@bool vim-mode = False</vh></v>
<v t="ekr.20240927151701.117"><vh>@bool warn-about-redefined-shortcuts = True</vh></v>
<v t="ekr.20240927151701.118"><vh>@int auto-justify = 80</vh></v>
<v t="ekr.20240927151701.119"><vh>rst3 path options</vh>
<v t="ekr.20240927151701.120"><vh>@string rst3-write-intermediate-extension = .txt</vh></v>
<v t="ekr.20240927151701.121"><vh>@string rst3-default-path = None</vh></v>
<v t="ekr.20240927151701.122"><vh>@string rst3-stylesheet-name = default.css</vh></v>
<v t="ekr.20240927151701.123"><vh>@string rst3-stylesheet-path = None</vh></v>
<v t="ekr.20240927151701.124"><vh>@string rst3-publish-argv-for-missing-stylesheets = None</vh></v>
</v>
</v>
<v t="ekr.20240927151701.144"><vh>File settings</vh>
<v t="ekr.20240927151701.145"><vh>@bool open-with-clean-filenames = True</vh></v>
<v t="ekr.20240927151701.146"><vh>@bool check-for-changed-external-files = True</vh></v>
<v t="ekr.20240927151701.147"><vh>@bool open-with-save-on-update = False</vh></v>
<v t="ekr.20240927151701.148"><vh>@bool open-with-uses-derived-file-extensions = True</vh></v>
</v>
<v t="ekr.20240927151701.149"><vh>Find settings</vh>
<v t="ekr.20240927151701.150"><vh>@bool auto-scroll-find-tab = False</vh></v>
<v t="ekr.20240927151701.151"><vh>@bool close-find-dialog-after-search = False</vh></v>
<v t="ekr.20240927151701.152"><vh>@bool find-ignore-duplicates = False</vh></v>
<v t="ekr.20240927151701.153"><vh>@bool minibuffer-find-mode = True</vh></v>
<v t="ekr.20240927151701.154"><vh>@bool use-find-dialog = False</vh></v>
</v>
<v t="ekr.20240927151701.155"><vh>Importer settings</vh>
<v t="ekr.20240927151701.156"><vh>@data import-html-tags</vh></v>
<v t="ekr.20240927151701.157"><vh>@data import-xml-tags</vh></v>
</v>
<v t="ekr.20240927153018.1"><vh>Scripts</vh>
<v t="ekr.20240927151701.229"><vh> Recursive import script</vh>
<v t="ekr.20240927151701.230"><vh>&lt;&lt; rust dir_list &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20240927151701.190"><vh>Syntax coloring settings</vh>
<v t="ekr.20240927151701.191"><vh>@@color rest.keyword2 = red</vh></v>
<v t="ekr.20240927151701.192"><vh>@@color rest.keyword4 = blue</vh></v>
<v t="ekr.20240927151701.193"><vh>@@color rest.leokeyword = green</vh></v>
<v t="ekr.20240927151701.194"><vh>@color forth.keyword3 = black</vh></v>
<v t="ekr.20240927151701.195"><vh>@color python.name = @solarized-yellow</vh></v>
<v t="ekr.20240927151701.196"><vh>@font rest.comment1</vh></v>
</v>
<v t="ekr.20240927151701.176"><vh>VR settings</vh>
<v t="ekr.20240927151701.177"><vh>@bool view-rendered-auto-create = False</vh></v>
<v t="ekr.20240927151701.178"><vh>@bool view-rendered-auto-hide = False</vh></v>
<v t="ekr.20240927151701.179"><vh>@string view-rendered-default-kind = rst</vh></v>
</v>
</v>
<v t="ekr.20240927151701.203" descendentVnodeUnknownAttributes="7d710058010000003071017d7102580b0000005f5f626f6f6b6d61726b7371037d7104580700000069735f6475706571054930300a7373732e"><vh>Buttons &amp; commands</vh>
<v t="ekr.20240927151701.206"><vh>@button backup</vh></v>
<v t="ekr.20240928073118.1"><vh>@button cargo-fmt</vh></v>
<v t="ekr.20240927152759.1"><vh>@button cargo-run</vh></v>
<v t="ekr.20240930063740.1"><vh>@@command ga</vh></v>
<v t="ekr.20240930101156.1"><vh>@@command ga-leo</vh></v>
<v t="ekr.20240930063514.1"><vh>@@command gc</vh></v>
<v t="ekr.20240930063546.1"><vh>@command gs</vh></v>
<v t="ekr.20240930064435.1"><vh>@@command git-reset</vh></v>
<v t="ekr.20240930064622.1"><vh>@@command push</vh></v>
<v t="ekr.20240927151701.207"><vh>@@button print-gnx</vh></v>
</v>
</v>
<v t="ekr.20240927154009.1"><vh>Files</vh>
<v t="ekr.20240927151245.1"><vh>@edit Cargo.toml</vh></v>
<v t="ekr.20240927151332.1"><vh>@file src/main.rs</vh></v>
<v t="ekr.20240928161210.1"><vh>@file src/tbo.rs</vh></v>
</v>
<v t="ekr.20241001073040.1"><vh>--- Not used</vh>
<v t="ekr.20240105145241.1"><vh>COPY: class TokenBasedOrange</vh>
<v t="ekr.20240119062227.1"><vh>&lt;&lt; TokenBasedOrange: docstring &gt;&gt;</vh></v>
<v t="ekr.20240111035404.1"><vh>&lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;</vh></v>
<v t="ekr.20240116040458.1"><vh>&lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;</vh></v>
<v t="ekr.20240105145241.2"><vh>tbo.ctor</vh></v>
<v t="ekr.20240126012433.1"><vh>tbo: Checking &amp; dumping</vh>
<v t="ekr.20240106220724.1"><vh>tbo.dump_token_range</vh></v>
<v t="ekr.20240112082350.1"><vh>tbo.internal_error_message</vh></v>
<v t="ekr.20240226131015.1"><vh>tbo.user_error_message</vh></v>
<v t="ekr.20240117053310.1"><vh>tbo.oops</vh></v>
</v>
<v t="ekr.20240105145241.4"><vh>tbo: Entries &amp; helpers</vh>
<v t="ekr.20240105145241.5"><vh>tbo.beautify (main token loop)</vh>
<v t="ekr.20240112023403.1"><vh>&lt;&lt; tbo.beautify: init ivars &gt;&gt;</vh></v>
</v>
<v t="ekr.20240105145241.6"><vh>tbo.beautify_file (entry) (stats &amp; diffs)</vh></v>
<v t="ekr.20240105145241.8"><vh>tbo.init_tokens_from_file</vh></v>
<v t="ekr.20240105140814.12"><vh>tbo.regularize_newlines</vh></v>
<v t="ekr.20240105140814.17"><vh>tbo.write_file</vh></v>
<v t="ekr.20200107040729.1"><vh>tbo.show_diffs</vh></v>
</v>
<v t="ekr.20240105145241.9"><vh>tbo: Visitors &amp; generators</vh>
<v t="ekr.20240105145241.10"><vh>tbo.do_comment</vh>
<v t="ekr.20240420034216.1"><vh>&lt;&lt; do_comment: update comment-related state &gt;&gt;</vh></v>
</v>
<v t="ekr.20240111051726.1"><vh>tbo.do_dedent</vh></v>
<v t="ekr.20240105145241.11"><vh>tbo.do_encoding</vh></v>
<v t="ekr.20240105145241.12"><vh>tbo.do_endmarker</vh></v>
<v t="ekr.20240105145241.14"><vh>tbo.do_indent</vh></v>
<v t="ekr.20240105145241.16"><vh>tbo.do_name &amp; generators</vh>
<v t="ekr.20240418050017.1"><vh>tbo.do_name</vh></v>
<v t="ekr.20240105145241.40"><vh>tbo.gen_word</vh></v>
<v t="ekr.20240107141830.1"><vh>tbo.gen_word_op</vh></v>
</v>
<v t="ekr.20240105145241.17"><vh>tbo.do_newline, do_nl &amp; generators</vh>
<v t="ekr.20240418043826.1"><vh>tbo.do_newline</vh></v>
<v t="ekr.20240418043827.1"><vh>tbo.do_nl</vh></v>
</v>
<v t="ekr.20240105145241.18"><vh>tbo.do_number</vh></v>
<v t="ekr.20240105145241.19"><vh>tbo.do_op &amp; generators</vh>
<v t="ekr.20240418045924.1"><vh>tbo.do_op</vh></v>
<v t="ekr.20240105145241.31"><vh>tbo.gen_colon &amp; helper</vh></v>
<v t="ekr.20240109035004.1"><vh>tbo.gen_dot_op &amp; _next</vh>
<v t="ekr.20240105145241.43"><vh>tbo._next</vh></v>
</v>
<v t="ekr.20240105145241.20"><vh>tbo.gen_equal_op</vh></v>
<v t="ekr.20240105145241.35"><vh>tbo.gen_lt</vh></v>
<v t="ekr.20240105145241.37"><vh>tbo.gen_possible_unary_op &amp; helper</vh>
<v t="ekr.20240109082712.1"><vh>tbo.is_unary_op &amp; _prev</vh>
<v t="ekr.20240115233050.1"><vh>tbo._prev</vh></v>
</v>
</v>
<v t="ekr.20240105145241.36"><vh>tbo.gen_rt</vh></v>
<v t="ekr.20240105145241.38"><vh>tbo.gen_star_op</vh></v>
<v t="ekr.20240105145241.39"><vh>tbo.gen_star_star_op</vh></v>
<v t="ekr.20240105145241.3"><vh>tbo.push_state</vh></v>
</v>
<v t="ekr.20240105145241.21"><vh>tbo.do_string</vh></v>
<v t="ekr.20240105145241.22"><vh>tbo.do_verbatim</vh></v>
<v t="ekr.20240105145241.23"><vh>tbo.do_ws</vh></v>
<v t="ekr.20240105145241.27"><vh>tbo.gen_blank</vh></v>
<v t="ekr.20240105145241.26"><vh>tbo.gen_token</vh></v>
</v>
<v t="ekr.20240110205127.1"><vh>tbo: Scanning</vh>
<v t="ekr.20240128114622.1"><vh>tbo.pre_scan &amp; helpers</vh>
<v t="ekr.20240128230812.1"><vh>&lt;&lt; pre-scan 'newline' tokens &gt;&gt;</vh></v>
<v t="ekr.20240128123117.1"><vh>&lt;&lt; pre-scan 'op' tokens &gt;&gt;</vh></v>
<v t="ekr.20240128231119.1"><vh>&lt;&lt; pre-scan 'name' tokens &gt;&gt;</vh></v>
<v t="ekr.20240129041304.1"><vh>tbo.finish_arg</vh></v>
<v t="ekr.20240128233406.1"><vh>tbo.finish_slice</vh></v>
<v t="ekr.20240129040347.1"><vh>tbo.finish_dict</vh></v>
</v>
<v t="ekr.20240129034209.1"><vh>tbo.is_unary_op_with_prev</vh></v>
<v t="ekr.20240129035336.1"><vh>tbo.is_python_keyword</vh></v>
<v t="ekr.20240106170746.1"><vh>tbo.set_context</vh></v>
</v>
</v>
<v t="ekr.20241001215023.1"><vh>COPY: class ParseState</vh></v>
<v t="ekr.20241003062509.1"><vh>--- old Rust code</vh>
<v t="ekr.20241003055233.1"><vh>beautify_one_file: 'static</vh></v>
<v t="ekr.20241001093308.2"><vh>fn tokenize</vh>
<v t="ekr.20241001093308.3"><vh>&lt;&lt; tokenize: define contents &gt;&gt;</vh></v>
</v>
<v t="ekr.20240929032636.1"><vh>function: entry &amp; helpers</vh>
<v t="ekr.20240930100625.1"><vh>&lt;&lt; 1: read &gt;&gt;</vh></v>
<v t="ekr.20240930100707.1"><vh>&lt;&lt; 2: make input_list &gt;&gt;</vh></v>
<v t="ekr.20240930100553.1"><vh>&lt;&lt; 3: print stats &gt;&gt;</vh></v>
<v t="ekr.20240929033044.1"><vh>function: add_input_token</vh></v>
<v t="ekr.20240929032710.1"><vh>function: fmt_ms</vh></v>
<v t="ekr.20240929024648.113"><vh>function: make_input_list (works)</vh></v>
<v t="ekr.20240930084648.1"><vh>function: read</vh></v>
</v>
<v t="ekr.20240930085546.1"><vh>function: lex_contents</vh></v>
<v t="ekr.20240929031635.1"><vh>function: scan_input_list</vh></v>
<v t="ekr.20241001071914.1"><vh>function: test_loop</vh></v>
<v t="ekr.20241003063446.121"><vh>LB::string_to_static_str</vh></v>
<v t="ekr.20241002164044.1"><vh>make_input_list: Calculate class name w/o match</vh></v>
<v t="ekr.20240929074941.1"><vh>Stats::update_times</vh></v>
</v>
</v>
<v t="ekr.20240927154016.1"><vh>Notes</vh>
<v t="ekr.20240929084852.1"><vh>Ownership and mutation</vh></v>
<v t="ekr.20241001055017.1"><vh>Traits</vh></v>
<v t="ekr.20241001060848.1"><vh>Lifetimes (to do)</vh></v>
<v t="ekr.20241002054443.1"><vh>Errors in LB:beautify</vh></v>
<v t="ekr.20240928185643.1"><vh>Stats</vh></v>
</v>
<v t="ekr.20240927154323.1"><vh>** To do</vh>
<v t="ekr.20241001213229.1"><vh>from tbo.beautify (finish: do not delete)</vh></v>
</v>
<v t="ekr.20241001104914.1"><vh>--- classes</vh>
<v t="ekr.20240929024648.120"><vh>class InputTok</vh></v>
<v t="ekr.20240929074037.1"><vh>class LeoBeautifier</vh>
<v t="ekr.20240929074037.114"><vh> LB::new</vh></v>
<v t="ekr.20240929074037.3"><vh>LB::add_input_token</vh></v>
<v t="ekr.20240929074037.2"><vh>LB::add_output_string</vh></v>
<v t="ekr.20240929074037.113"><vh>LB::beautify</vh>
<v t="ekr.20241001213329.1"><vh>&lt;&lt; LB::beautify: init ivars &gt;&gt;</vh></v>
<v t="ekr.20241002062655.1"><vh>&lt;&lt; LB: beautify: dispatch on input_token.kind &gt;&gt;</vh></v>
</v>
<v t="ekr.20240929074037.4"><vh>LB::beautify_all_files</vh></v>
<v t="ekr.20240929074037.5"><vh>LB::beautify_one_file</vh></v>
<v t="ekr.20240929074037.7"><vh>LB::do_*</vh>
<v t="ekr.20241002071143.1"><vh>tbo.do_ws</vh></v>
<v t="ekr.20240929074037.8"><vh>LB:Handlers with values</vh>
<v t="ekr.20240929074037.9"><vh>LB::do_Comment</vh></v>
<v t="ekr.20240929074037.10"><vh>LB::do_Complex</vh></v>
<v t="ekr.20240929074037.11"><vh>LB::do_Float</vh></v>
<v t="ekr.20240929074037.12"><vh>LB::do_Int</vh></v>
<v t="ekr.20240929074037.13"><vh>LB::do_Name</vh></v>
<v t="ekr.20240929074037.14"><vh>LB::do_String</vh></v>
</v>
<v t="ekr.20240929074037.15"><vh>LB:Handlers using lws</vh>
<v t="ekr.20240929074037.16"><vh>LB::do_Dedent</vh></v>
<v t="ekr.20240929074037.17"><vh>LB::do_Indent</vh></v>
<v t="ekr.20240929074037.18"><vh>LB::do_Newline</vh></v>
<v t="ekr.20240929074037.19"><vh>LB::do_NonLogicalNewline</vh></v>
</v>
<v t="ekr.20240929074037.20"><vh>LB:Handlers w/o values</vh>
<v t="ekr.20240929074037.21"><vh>LB::do_Amper</vh></v>
<v t="ekr.20240929074037.22"><vh>LB::do_AmperEqual</vh></v>
<v t="ekr.20240929074037.23"><vh>LB::do_And</vh></v>
<v t="ekr.20240929074037.24"><vh>LB::do_As</vh></v>
<v t="ekr.20240929074037.25"><vh>LB::do_Assert</vh></v>
<v t="ekr.20240929074037.26"><vh>LB::do_Async</vh></v>
<v t="ekr.20240929074037.27"><vh>LB::do_At</vh></v>
<v t="ekr.20240929074037.28"><vh>LB::do_AtEqual</vh></v>
<v t="ekr.20240929074037.29"><vh>LB::do_Await</vh></v>
<v t="ekr.20240929074037.30"><vh>LB::do_Break</vh></v>
<v t="ekr.20240929074037.31"><vh>LB::do_Case</vh></v>
<v t="ekr.20240929074037.32"><vh>LB::do_CircumFlex</vh></v>
<v t="ekr.20240929074037.33"><vh>LB::do_CircumflexEqual</vh></v>
<v t="ekr.20240929074037.34"><vh>LB::do_Class</vh></v>
<v t="ekr.20240929074037.35"><vh>LB::do_Colon</vh></v>
<v t="ekr.20240929074037.36"><vh>LB::do_ColonEqual</vh></v>
<v t="ekr.20240929074037.37"><vh>LB::do_Comma</vh></v>
<v t="ekr.20240929074037.38"><vh>LB::do_Continue</vh></v>
<v t="ekr.20240929074037.39"><vh>LB::do_Def</vh></v>
<v t="ekr.20240929074037.40"><vh>LB::do_Del</vh></v>
<v t="ekr.20240929074037.41"><vh>LB::do_Dot</vh></v>
<v t="ekr.20240929074037.42"><vh>LB::do_DoubleSlash</vh></v>
<v t="ekr.20240929074037.43"><vh>LB::do_DoubleSlashEqual</vh></v>
<v t="ekr.20240929074037.44"><vh>LB::do_DoubleStar</vh></v>
<v t="ekr.20240929074037.45"><vh>LB::do_DoubleStarEqual</vh></v>
<v t="ekr.20240929074037.46"><vh>LB::do_Elif</vh></v>
<v t="ekr.20240929074037.47"><vh>LB::do_Ellipsis</vh></v>
<v t="ekr.20240929074037.48"><vh>LB::do_Else</vh></v>
<v t="ekr.20240929074037.49"><vh>LB::do_EndOfFile</vh></v>
<v t="ekr.20240929074037.50"><vh>LB::do_EqEqual</vh></v>
<v t="ekr.20240929074037.51"><vh>LB::do_Equal</vh></v>
<v t="ekr.20240929074037.52"><vh>LB::do_Except</vh></v>
<v t="ekr.20240929074037.53"><vh>LB::do_False</vh></v>
<v t="ekr.20240929074037.54"><vh>LB::do_Finally</vh></v>
<v t="ekr.20240929074037.55"><vh>LB::do_For</vh></v>
<v t="ekr.20240929074037.56"><vh>LB::do_From</vh></v>
<v t="ekr.20240929074037.57"><vh>LB::do_Global</vh></v>
<v t="ekr.20240929074037.58"><vh>LB::do_Greater</vh></v>
<v t="ekr.20240929074037.59"><vh>LB::do_GreaterEqual</vh></v>
<v t="ekr.20240929074037.60"><vh>LB::do_If</vh></v>
<v t="ekr.20240929074037.61"><vh>LB::do_Import</vh></v>
<v t="ekr.20240929074037.62"><vh>LB::do_In</vh></v>
<v t="ekr.20240929074037.63"><vh>LB::do_Is</vh></v>
<v t="ekr.20240929074037.64"><vh>LB::do_Lambda</vh></v>
<v t="ekr.20240929074037.65"><vh>LB::do_Lbrace</vh></v>
<v t="ekr.20240929074037.66"><vh>LB::do_LeftShift</vh></v>
<v t="ekr.20240929074037.67"><vh>LB::do_LeftShiftEqual</vh></v>
<v t="ekr.20240929074037.68"><vh>LB::do_Less</vh></v>
<v t="ekr.20240929074037.69"><vh>LB::do_LessEqual</vh></v>
<v t="ekr.20240929074037.70"><vh>LB::do_Lpar</vh></v>
<v t="ekr.20240929074037.71"><vh>LB::do_Lsqb</vh></v>
<v t="ekr.20240929074037.72"><vh>LB::do_Match</vh></v>
<v t="ekr.20240929074037.73"><vh>LB::do_Minus</vh></v>
<v t="ekr.20240929074037.74"><vh>LB::do_MinusEqual</vh></v>
<v t="ekr.20240929074037.75"><vh>LB::do_None</vh></v>
<v t="ekr.20240929074037.76"><vh>LB::do_Nonlocal</vh></v>
<v t="ekr.20240929074037.77"><vh>LB::do_Not</vh></v>
<v t="ekr.20240929074037.78"><vh>LB::do_NotEqual</vh></v>
<v t="ekr.20240929074037.79"><vh>LB::do_Or</vh></v>
<v t="ekr.20240929074037.80"><vh>LB::do_Pass</vh></v>
<v t="ekr.20240929074037.81"><vh>LB::do_Percent</vh></v>
<v t="ekr.20240929074037.82"><vh>LB::do_PercentEqual</vh></v>
<v t="ekr.20240929074037.83"><vh>LB::do_Plus</vh></v>
<v t="ekr.20240929074037.84"><vh>LB::do_PlusEqual</vh></v>
<v t="ekr.20240929074037.85"><vh>LB::do_Raise</vh></v>
<v t="ekr.20240929074037.86"><vh>LB::do_Rarrow</vh></v>
<v t="ekr.20240929074037.87"><vh>LB::do_Rbrace</vh></v>
<v t="ekr.20240929074037.88"><vh>LB::do_Return</vh></v>
<v t="ekr.20240929074037.89"><vh>LB::do_RightShift</vh></v>
<v t="ekr.20240929074037.90"><vh>LB::do_RightShiftEqual</vh></v>
<v t="ekr.20240929074037.91"><vh>LB::do_Rpar</vh></v>
<v t="ekr.20240929074037.92"><vh>LB::do_Rsqb</vh></v>
<v t="ekr.20240929074037.93"><vh>LB::do_Semi</vh></v>
<v t="ekr.20240929074037.94"><vh>LB::do_Slash</vh></v>
<v t="ekr.20240929074037.95"><vh>LB::do_SlashEqual</vh></v>
<v t="ekr.20240929074037.96"><vh>LB::do_Star</vh></v>
<v t="ekr.20240929074037.97"><vh>LB::do_StarEqual</vh></v>
<v t="ekr.20240929074037.98"><vh>LB::do_StartExpression</vh></v>
<v t="ekr.20240929074037.99"><vh>LB::do_StartInteractive</vh></v>
<v t="ekr.20240929074037.100"><vh>LB::do_StarModule</vh></v>
<v t="ekr.20240929074037.101"><vh>LB::do_Tilde</vh></v>
<v t="ekr.20240929074037.102"><vh>LB::do_True</vh></v>
<v t="ekr.20240929074037.103"><vh>LB::do_Try</vh></v>
<v t="ekr.20240929074037.104"><vh>LB::do_Type</vh></v>
<v t="ekr.20240929074037.105"><vh>LB::do_Vbar</vh></v>
<v t="ekr.20240929074037.106"><vh>LB::do_VbarEqual</vh></v>
<v t="ekr.20240929074037.107"><vh>LB::do_While</vh></v>
<v t="ekr.20240929074037.108"><vh>LB::do_With</vh></v>
<v t="ekr.20240929074037.109"><vh>LB::do_Yield</vh></v>
</v>
</v>
<v t="ekr.20240929074037.110"><vh>LB::enabled</vh></v>
<v t="ekr.20240929074037.111"><vh>LB::get_args</vh></v>
<v t="ekr.20240929074037.112"><vh>LB::make_input_list</vh>
<v t="ekr.20241002113506.1"><vh>&lt;&lt; Calculate class_name using match token &gt;&gt;</vh></v>
</v>
<v t="ekr.20240929074037.115"><vh>LB::show_args</vh></v>
<v t="ekr.20240929074037.116"><vh>LB::show_help</vh></v>
<v t="ekr.20240929074037.117"><vh>LB::show_output_list</vh></v>
<v t="ekr.20241002163554.1"><vh>LB::string_to_static_str</vh></v>
</v>
<v t="ekr.20240929074547.1"><vh>class Stats</vh>
<v t="ekr.20241001100954.1"><vh> Stats::new</vh></v>
<v t="ekr.20240929080242.1"><vh>Stats::fmt_ns</vh></v>
<v t="ekr.20240929075236.1"><vh>Stats::report</vh></v>
</v>
</v>
<v t="ekr.20241003084853.1"><vh>--- new code: doesn't work</vh>
<v t="ekr.20241003064504.1"><vh>functions to be added</vh>
<v t="ekr.20241003063446.114"><vh>fn: enabled</vh></v>
<v t="ekr.20241003063446.115"><vh>fn: get_args</vh></v>
<v t="ekr.20241003063446.118"><vh>fn: show_args</vh></v>
<v t="ekr.20241003063446.119"><vh>fn: show_help</vh></v>
<v t="ekr.20241003063446.120"><vh>fn: show_output_list</vh></v>
</v>
<v t="ekr.20241003084902.1"><vh>COPY: pub fn entry </vh></v>
<v t="ekr.20241003064425.1"><vh>Beautifier struct</vh></v>
<v t="ekr.20241003063446.6"><vh>&lt;&lt; fn: beautify: init vars &gt;&gt; (to do)</vh></v>
<v t="ekr.20241003063446.1"><vh>LeoBeautifier functions</vh>
<v t="ekr.20241003063446.3"><vh>fn: add_input_token</vh></v>
<v t="ekr.20241003063446.4"><vh>fn: add_output_string</vh></v>
<v t="ekr.20241003063446.5"><vh>fn: beautify</vh>
<v t="ekr.20241003063446.7"><vh>&lt;&lt; LB: beautify: dispatch on input_token.kind &gt;&gt;</vh></v>
</v>
<v t="ekr.20241003063446.8"><vh>fn: beautify_all_files</vh></v>
<v t="ekr.20241003063446.9"><vh>fn: beautify_one_file</vh></v>
<v t="ekr.20241003063446.116"><vh>fn: make_input_list</vh>
<v t="ekr.20241003063446.117"><vh>&lt;&lt; Calculate class_name using match token &gt;&gt;</vh></v>
</v>
<v t="ekr.20241003063446.10"><vh>fn: do_*</vh>
<v t="ekr.20241003063446.11"><vh>tbo.do_ws</vh></v>
<v t="ekr.20241003063446.12"><vh>LB:Handlers with values</vh>
<v t="ekr.20241003063446.13"><vh>fn: do_Comment</vh></v>
<v t="ekr.20241003063446.14"><vh>fn: do_Complex</vh></v>
<v t="ekr.20241003063446.15"><vh>fn: do_Float</vh></v>
<v t="ekr.20241003063446.16"><vh>fn: do_Int</vh></v>
<v t="ekr.20241003063446.17"><vh>fn: do_Name</vh></v>
<v t="ekr.20241003063446.18"><vh>fn: do_String</vh></v>
</v>
<v t="ekr.20241003063446.19"><vh>LB:Handlers using lws</vh>
<v t="ekr.20241003063446.20"><vh>fn: do_Dedent</vh></v>
<v t="ekr.20241003063446.21"><vh>fn: do_Indent</vh></v>
<v t="ekr.20241003063446.22"><vh>fn: do_Newline</vh></v>
<v t="ekr.20241003063446.23"><vh>fn: do_NonLogicalNewline</vh></v>
</v>
<v t="ekr.20241003063446.24"><vh>LB:Handlers w/o values</vh>
<v t="ekr.20241003063446.25"><vh>fn: do_Amper</vh></v>
<v t="ekr.20241003063446.26"><vh>fn: do_AmperEqual</vh></v>
<v t="ekr.20241003063446.27"><vh>fn: do_And</vh></v>
<v t="ekr.20241003063446.28"><vh>fn: do_As</vh></v>
<v t="ekr.20241003063446.29"><vh>fn: do_Assert</vh></v>
<v t="ekr.20241003063446.30"><vh>fn: do_Async</vh></v>
<v t="ekr.20241003063446.31"><vh>fn: do_At</vh></v>
<v t="ekr.20241003063446.32"><vh>fn: do_AtEqual</vh></v>
<v t="ekr.20241003063446.33"><vh>fn: do_Await</vh></v>
<v t="ekr.20241003063446.34"><vh>fn: do_Break</vh></v>
<v t="ekr.20241003063446.35"><vh>fn: do_Case</vh></v>
<v t="ekr.20241003063446.36"><vh>fn: do_CircumFlex</vh></v>
<v t="ekr.20241003063446.37"><vh>fn: do_CircumflexEqual</vh></v>
<v t="ekr.20241003063446.38"><vh>fn: do_Class</vh></v>
<v t="ekr.20241003063446.39"><vh>fn: do_Colon</vh></v>
<v t="ekr.20241003063446.40"><vh>fn: do_ColonEqual</vh></v>
<v t="ekr.20241003063446.41"><vh>fn: do_Comma</vh></v>
<v t="ekr.20241003063446.42"><vh>fn: do_Continue</vh></v>
<v t="ekr.20241003063446.43"><vh>fn: do_Def</vh></v>
<v t="ekr.20241003063446.44"><vh>fn: do_Del</vh></v>
<v t="ekr.20241003063446.45"><vh>fn: do_Dot</vh></v>
<v t="ekr.20241003063446.46"><vh>fn: do_DoubleSlash</vh></v>
<v t="ekr.20241003063446.47"><vh>fn: do_DoubleSlashEqual</vh></v>
<v t="ekr.20241003063446.48"><vh>fn: do_DoubleStar</vh></v>
<v t="ekr.20241003063446.49"><vh>fn: do_DoubleStarEqual</vh></v>
<v t="ekr.20241003063446.50"><vh>fn: do_Elif</vh></v>
<v t="ekr.20241003063446.51"><vh>fn: do_Ellipsis</vh></v>
<v t="ekr.20241003063446.52"><vh>fn: do_Else</vh></v>
<v t="ekr.20241003063446.53"><vh>fn: do_EndOfFile</vh></v>
<v t="ekr.20241003063446.54"><vh>fn: do_EqEqual</vh></v>
<v t="ekr.20241003063446.55"><vh>fn: do_Equal</vh></v>
<v t="ekr.20241003063446.56"><vh>fn: do_Except</vh></v>
<v t="ekr.20241003063446.57"><vh>fn: do_False</vh></v>
<v t="ekr.20241003063446.58"><vh>fn: do_Finally</vh></v>
<v t="ekr.20241003063446.59"><vh>fn: do_For</vh></v>
<v t="ekr.20241003063446.60"><vh>fn: do_From</vh></v>
<v t="ekr.20241003063446.61"><vh>fn: do_Global</vh></v>
<v t="ekr.20241003063446.62"><vh>fn: do_Greater</vh></v>
<v t="ekr.20241003063446.63"><vh>fn: do_GreaterEqual</vh></v>
<v t="ekr.20241003063446.64"><vh>fn: do_If</vh></v>
<v t="ekr.20241003063446.65"><vh>fn: do_Import</vh></v>
<v t="ekr.20241003063446.66"><vh>fn: do_In</vh></v>
<v t="ekr.20241003063446.67"><vh>fn: do_Is</vh></v>
<v t="ekr.20241003063446.68"><vh>fn: do_Lambda</vh></v>
<v t="ekr.20241003063446.69"><vh>fn: do_Lbrace</vh></v>
<v t="ekr.20241003063446.70"><vh>fn: do_LeftShift</vh></v>
<v t="ekr.20241003063446.71"><vh>fn: do_LeftShiftEqual</vh></v>
<v t="ekr.20241003063446.72"><vh>fn: do_Less</vh></v>
<v t="ekr.20241003063446.73"><vh>fn: do_LessEqual</vh></v>
<v t="ekr.20241003063446.74"><vh>fn: do_Lpar</vh></v>
<v t="ekr.20241003063446.75"><vh>fn: do_Lsqb</vh></v>
<v t="ekr.20241003063446.76"><vh>fn: do_Match</vh></v>
<v t="ekr.20241003063446.77"><vh>fn: do_Minus</vh></v>
<v t="ekr.20241003063446.78"><vh>fn: do_MinusEqual</vh></v>
<v t="ekr.20241003063446.79"><vh>fn: do_None</vh></v>
<v t="ekr.20241003063446.80"><vh>fn: do_Nonlocal</vh></v>
<v t="ekr.20241003063446.81"><vh>fn: do_Not</vh></v>
<v t="ekr.20241003063446.82"><vh>fn: do_NotEqual</vh></v>
<v t="ekr.20241003063446.83"><vh>fn: do_Or</vh></v>
<v t="ekr.20241003063446.84"><vh>fn: do_Pass</vh></v>
<v t="ekr.20241003063446.85"><vh>fn: do_Percent</vh></v>
<v t="ekr.20241003063446.86"><vh>fn: do_PercentEqual</vh></v>
<v t="ekr.20241003063446.87"><vh>fn: do_Plus</vh></v>
<v t="ekr.20241003063446.88"><vh>fn: do_PlusEqual</vh></v>
<v t="ekr.20241003063446.89"><vh>fn: do_Raise</vh></v>
<v t="ekr.20241003063446.90"><vh>fn: do_Rarrow</vh></v>
<v t="ekr.20241003063446.91"><vh>fn: do_Rbrace</vh></v>
<v t="ekr.20241003063446.92"><vh>fn: do_Return</vh></v>
<v t="ekr.20241003063446.93"><vh>fn: do_RightShift</vh></v>
<v t="ekr.20241003063446.94"><vh>fn: do_RightShiftEqual</vh></v>
<v t="ekr.20241003063446.95"><vh>fn: do_Rpar</vh></v>
<v t="ekr.20241003063446.96"><vh>fn: do_Rsqb</vh></v>
<v t="ekr.20241003063446.97"><vh>fn: do_Semi</vh></v>
<v t="ekr.20241003063446.98"><vh>fn: do_Slash</vh></v>
<v t="ekr.20241003063446.99"><vh>fn: do_SlashEqual</vh></v>
<v t="ekr.20241003063446.100"><vh>fn: do_Star</vh></v>
<v t="ekr.20241003063446.101"><vh>fn: do_StarEqual</vh></v>
<v t="ekr.20241003063446.102"><vh>fn: do_StartExpression</vh></v>
<v t="ekr.20241003063446.103"><vh>fn: do_StartInteractive</vh></v>
<v t="ekr.20241003063446.104"><vh>fn: do_StarModule</vh></v>
<v t="ekr.20241003063446.105"><vh>fn: do_Tilde</vh></v>
<v t="ekr.20241003063446.106"><vh>fn: do_True</vh></v>
<v t="ekr.20241003063446.107"><vh>fn: do_Try</vh></v>
<v t="ekr.20241003063446.108"><vh>fn: do_Type</vh></v>
<v t="ekr.20241003063446.109"><vh>fn: do_Vbar</vh></v>
<v t="ekr.20241003063446.110"><vh>fn: do_VbarEqual</vh></v>
<v t="ekr.20241003063446.111"><vh>fn: do_While</vh></v>
<v t="ekr.20241003063446.112"><vh>fn: do_With</vh></v>
<v t="ekr.20241003063446.113"><vh>fn: do_Yield</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20240928161210.1"></v>
<v t="ekr.20241001093308.1"><vh>pub fn entry (test)</vh></v>
</vnodes>
<tnodes>
<t tx="ekr.20200107040729.1">def show_diffs(self, s1: str, s2: str) -&gt; None:  # pragma: no cover
    """Print diffs between strings s1 and s2."""
    filename = self.filename
    lines = list(difflib.unified_diff(
        g.splitLines(s1),
        g.splitLines(s2),
        fromfile=f"Old {filename}",
        tofile=f"New {filename}",
    ))
    print('')
    print(f"Diffs for {filename}")
    for line in lines:
        print(line)
</t>
<t tx="ekr.20240105140814.12">def regularize_newlines(self, s: str) -&gt; str:
    """Regularize newlines within s."""
    return s.replace('\r\n', '\n').replace('\r', '\n')
</t>
<t tx="ekr.20240105140814.17">def write_file(self, filename: str, s: str) -&gt; None:  # pragma: no cover
    """
    Write the string s to the file whose name is given.

    Handle all exceptions.

    Before calling this function, the caller should ensure
    that the file actually has been changed.
    """
    try:
        s2 = g.toEncodedString(s)  # May raise exception.
        with open(filename, 'wb') as f:
            f.write(s2)
    except Exception as e:  # pragma: no cover
        print(f"Error {e!r}: {filename!r}")
</t>
<t tx="ekr.20240105145241.1">class TokenBasedOrange:  # Orange is the new Black.

    &lt;&lt; TokenBasedOrange: docstring &gt;&gt;
    &lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;
    &lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;

    @others
</t>
<t tx="ekr.20240105145241.10">def do_comment(self) -&gt; None:
    """Handle a comment token."""
    val = self.input_token.value
    &lt;&lt; do_comment: update comment-related state &gt;&gt;

    # Generate the comment.
    self.pending_lws = ''
    self.pending_ws = ''
    entire_line = self.input_token.line.lstrip().startswith('#')

    if entire_line:
        # The comment includes all ws.
        # #1496: No further munging needed.
        val = self.input_token.line.rstrip()
        # #3056: Insure one space after '#' in non-sentinel comments.
        #        Do not change bang lines or '##' comments.
        if m := self.comment_pat.match(val):
            i = len(m.group(1))
            val = val[:i] + '# ' + val[i + 1 :]
    else:
        # Exactly two spaces before trailing comments.
        val = '  ' + val.rstrip()
    self.gen_token('comment', val)
</t>
<t tx="ekr.20240105145241.11">def do_encoding(self) -&gt; None:
    """Handle the encoding token."""
</t>
<t tx="ekr.20240105145241.12">def do_endmarker(self) -&gt; None:
    """Handle an endmarker token."""

    # Ensure exactly one newline at the end of file.
    if self.prev_output_kind not in (
        'indent', 'dedent', 'line-indent', 'newline',
    ):
        self.output_list.append('\n')
    self.pending_lws = ''  # Defensive.
    self.pending_ws = ''  # Defensive.
</t>
<t tx="ekr.20240105145241.14">consider_message = 'consider using python/Tools/scripts/reindent.py'

def do_indent(self) -&gt; None:
    """Handle indent token."""

    # Only warn about indentation errors.
    if '\t' in self.input_token.value:  # pragma: no cover
        print(f"Found tab character in {self.filename}")
        print(self.consider_message)
    elif (len(self.input_token.value) % self.tab_width) != 0:  # pragma: no cover
        print(f"Indentation error in {self.filename}")
        print(self.consider_message)

    # Handle the token!
    new_indent = self.input_token.value
    old_indent = self.indent_level * self.tab_width * ' '
    if new_indent &gt; old_indent:
        self.indent_level += 1
    elif new_indent &lt; old_indent:  # pragma: no cover (defensive)
        print(f"\n===== do_indent: can not happen {new_indent!r}, {old_indent!r}")

    self.lws = new_indent
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'indent'
</t>
<t tx="ekr.20240105145241.16"></t>
<t tx="ekr.20240105145241.17"></t>
<t tx="ekr.20240105145241.18">def do_number(self) -&gt; None:
    """Handle a number token."""
    self.gen_blank()
    self.gen_token('number', self.input_token.value)
</t>
<t tx="ekr.20240105145241.19"></t>
<t tx="ekr.20240105145241.2">def __init__(self, settings: Optional[Settings] = None):
    """Ctor for Orange class."""

    # Set default settings.
    if settings is None:
        settings = {}

    # Hard-code 4-space tabs.
    self.tab_width = 4

    # Define tokens even for empty files.
    self.input_token: InputToken = None
    self.input_tokens: list[InputToken] = []
    self.lws: str = ""  # Set only by Indent/Dedent tokens.
    self.pending_lws: str = ""
    self.pending_ws: str = ""

    # Set by gen_token and all do_* methods that bypass gen_token.
    self.prev_output_kind: str = None
    self.prev_output_value: str = None

    # Set ivars from the settings dict *without* using setattr.
    self.all = settings.get('all', False)
    self.beautified = settings.get('beautified', False)
    self.diff = settings.get('diff', False)
    self.report = settings.get('report', False)
    self.write = settings.get('write', False)

    # The list of tokens that tbo._next/_prev skip.
    self.insignificant_tokens = (
        'comment', 'dedent', 'indent', 'newline', 'nl', 'ws',
    )

    # General patterns.
    self.beautify_pat = re.compile(
        r'#\s*pragma:\s*beautify\b|#\s*@@beautify|#\s*@\+node|#\s*@[+-]others|#\s*@[+-]&lt;&lt;')
    self.comment_pat = re.compile(r'^(\s*)#[^@!# \n]')
    self.nobeautify_pat = re.compile(r'\s*#\s*pragma:\s*no\s*beautify\b|#\s*@@nobeautify')

    # Patterns from FastAtRead class, specialized for python delims.
    self.node_pat = re.compile(r'^(\s*)#@\+node:([^:]+): \*(\d+)?(\*?) (.*)$')  # @node
    self.start_doc_pat = re.compile(r'^\s*#@\+(at|doc)?(\s.*?)?$')  # @doc or @
    self.at_others_pat = re.compile(r'^(\s*)#@(\+|-)others\b(.*)$')  # @others

    # Doc parts end with @c or a node sentinel. Specialized for python.
    self.end_doc_pat = re.compile(r"^\s*#@(@(c(ode)?)|([+]node\b.*))$")
</t>
<t tx="ekr.20240105145241.20">def gen_equal_op(self) -&gt; None:

    val = self.input_token.value
    context = self.input_token.context

    if context == 'initializer':
        # Pep 8: Don't use spaces around the = sign when used to indicate
        #        a keyword argument or a default parameter value.
        #        However, when combining an argument annotation with a default value,
        #        *do* use spaces around the = sign.
        self.pending_ws = ''
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.21">def do_string(self) -&gt; None:
    """
    Handle a 'string' token.

    The Tokenizer converts all f-string tokens to a single 'string' token.
    """
    # Careful: continued strings may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    self.gen_token('string', val)
    self.gen_blank()
</t>
<t tx="ekr.20240105145241.22">def do_verbatim(self) -&gt; None:
    """
    Handle one token in verbatim mode.
    End verbatim mode when the appropriate comment is seen.
    """
    kind = self.input_token.kind
    #
    # Careful: tokens may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    if kind == 'comment':
        if self.beautify_pat.match(val):
            self.verbatim = False
        val = val.rstrip()
        self.gen_token('comment', val)
        return
    if kind == 'indent':
        self.indent_level += 1
        self.lws = self.indent_level * self.tab_width * ' '
    if kind == 'dedent':
        self.indent_level -= 1
        self.lws = self.indent_level * self.tab_width * ' '
    self.gen_token('verbatim', val)
</t>
<t tx="ekr.20240105145241.23">def do_ws(self) -&gt; None:
    """
    Handle the "ws" pseudo-token.  See Tokenizer.itok.do_token (the gem).

    Put the whitespace only if if ends with backslash-newline.
    """
    val = self.input_token.value
    last_token = self.input_tokens[self.index - 1]

    if last_token.kind in ('nl', 'newline'):
        self.pending_lws = val
        self.pending_ws = ''
    elif '\\\n' in val:
        self.pending_lws = ''
        self.pending_ws = val
    else:
        self.pending_ws = val
</t>
<t tx="ekr.20240105145241.26">def gen_token(self, kind: str, value: Any) -&gt; None:
    """Add an output token to the code list."""

    if self.pending_lws:
        self.output_list.append(self.pending_lws)
    elif self.pending_ws:
        self.output_list.append(self.pending_ws)

    self.output_list.append(value)
    self.pending_lws = ''
    self.pending_ws = ''
    self.prev_output_value = value
    self.prev_output_kind = kind
</t>
<t tx="ekr.20240105145241.27">def gen_blank(self) -&gt; None:
    """
    Queue a *request* a blank.
    Change *neither* prev_output_kind *nor* pending_lws.
    """

    prev_kind = self.prev_output_kind
    if prev_kind == 'op-no-blanks':
        # A demand that no blank follows this op.
        self.pending_ws = ''
    elif prev_kind == 'hard-blank':
        # Eat any further blanks.
        self.pending_ws = ''
    elif prev_kind in (
        'dedent',
        'file-start',
        'indent',
        'line-indent',
        'newline',
    ):
        # Suppress the blank, but do *not* change the pending ws.
        pass
    elif self.pending_ws:
        # Use the existing pending ws.
        pass
    else:
        self.pending_ws = ' '
</t>
<t tx="ekr.20240105145241.3">def push_state(self, kind: str, value: Union[int, str, None] = None) -&gt; None:
    """Append a state to the state stack."""
    state = ParseState(kind, value)
    self.state_stack.append(state)
</t>
<t tx="ekr.20240105145241.31">def gen_colon(self) -&gt; None:
    """Handle a colon."""
    val = self.input_token.value
    context = self.input_token.context

    self.pending_ws = ''
    if context == 'complex-slice':
        if self.prev_output_value not in '[:':
            self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
    elif context == 'simple-slice':
        self.gen_token('op-no-blanks', val)
    elif context == 'dict':
        self.gen_token('op', val)
        self.gen_blank()
    else:
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.35">def gen_lt(self) -&gt; None:
    """Generate code for a left paren or curly/square bracket."""
    val = self.input_token.value
    assert val in '([{', repr(val)

    # Update state vars.
    if val == '(':
        self.paren_level += 1
    elif val == '[':
        self.square_brackets_stack.append(False)
    else:
        self.curly_brackets_level += 1

    # Generate or suppress the leading blank.
    # Update self.in_arg_list if necessary.
    if self.input_token.context == 'import':
        self.gen_blank()
    elif self.prev_output_kind in ('op', 'word-op'):
        self.gen_blank()
    elif self.prev_output_kind == 'word':
        # Only suppress blanks before '(' or '[' for non-keywords.
        if val == '{' or self.prev_output_value in (
            'if', 'else', 'elif', 'return', 'for', 'while',
        ):
            self.gen_blank()
        elif val == '(':
            self.in_arg_list += 1
            self.pending_ws = ''
        else:
            self.pending_ws = ''
    elif self.prev_output_kind != 'line-indent':
        self.pending_ws = ''

    # Output the token!
    self.gen_token('op-no-blanks', val)
</t>
<t tx="ekr.20240105145241.36">def gen_rt(self) -&gt; None:
    """Generate code for a right paren or curly/square bracket."""
    val = self.input_token.value
    assert val in ')]}', repr(val)

    # Update state vars.
    if val == ')':
        self.paren_level -= 1
        self.in_arg_list = max(0, self.in_arg_list - 1)
    elif val == ']':
        self.square_brackets_stack.pop()
    else:
        self.curly_brackets_level -= 1

    if self.prev_output_kind != 'line-indent':
        self.pending_ws = ''
    self.gen_token('rt', val)
</t>
<t tx="ekr.20240105145241.37">def gen_possible_unary_op(self) -&gt; None:
    """Add a unary or binary op to the token list."""
    val = self.input_token.value
    if self.is_unary_op(self.index, val):
        prev = self.input_token
        if prev.kind == 'lt':
            self.gen_token('op-no-blanks', val)
        else:
            self.gen_blank()
            self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()

</t>
<t tx="ekr.20240105145241.38">def gen_star_op(self) -&gt; None:
    """Put a '*' op, with special cases for *args."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.39">def gen_star_star_op(self) -&gt; None:
    """Put a ** operator, with a special case for **kwargs."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240105145241.4"></t>
<t tx="ekr.20240105145241.40">def gen_word(self, s: str) -&gt; None:
    """Add a word request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word', s)
    self.gen_blank()
</t>
<t tx="ekr.20240105145241.43">def _next(self, i: int) -&gt; Optional[int]:
    """
    Return the next *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.

    The **Global Token Ratio** is tbo.n_scanned_tokens / len(tbo.tokens),
    where tbo.n_scanned_tokens is the total number of calls calls to
    tbo.next or tbo.prev.

    For Leo's sources, this ratio ranges between 0.48 and 1.51!

    The orange_command function warns if this ratio is greater than 2.5.
    Previous versions of this code suffered much higher ratios.
    """
    i += 1
    while i &lt; len(self.input_tokens):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            # g.trace(f"token: {token!r}")
            return i
        i += 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20240105145241.5">def no_visitor(self) -&gt; None:  # pragma: no cover
    self.oops(f"Unknown kind: {self.input_token.kind!r}")

def beautify(self,
    contents: str, filename: str, input_tokens: list[InputToken],
) -&gt; str:
    """
    The main line. Create output tokens and return the result as a string.

    beautify_file and beautify_file_def call this method.
    """
    &lt;&lt; tbo.beautify: init ivars &gt;&gt;

    try:
        # Pre-scan the token list, setting context.s
        self.pre_scan()

        # Init ivars first.
        self.input_token = None
        self.pending_lws = ''
        self.pending_ws = ''
        self.prev_output_kind = None
        self.prev_output_value = None

        # Init state.
        self.gen_token('file-start', '')
        self.push_state('file-start')

        # The main loop:
        prev_line_number: int = 0
        for self.index, self.input_token in enumerate(input_tokens):
            # Set global for visitors.
            if prev_line_number != self.input_token.line_number:
                prev_line_number = self.input_token.line_number
            # Call the proper visitor.
            if self.verbatim:
                self.do_verbatim()
            else:
                func = getattr(self, f"do_{self.input_token.kind}", self.no_visitor)
                func()

        # Return the result.
        result = ''.join(self.output_list)
        return result

    # Make no change if there is any error.
    except InternalBeautifierError as e:  # pragma: no cover
        # oops calls self.internal_error_message to creates e.
        print(repr(e))
    except AssertionError as e:  # pragma: no cover
        print(self.internal_error_message(repr(e)))
    return contents
</t>
<t tx="ekr.20240105145241.6">def beautify_file(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    TokenBasedOrange: Beautify the the given external file.

    Return True if the file was beautified.
    """
    if 0:
        print(
            f"all: {int(self.all)} "
            f"beautified: {int(self.beautified)} "
            f"diff: {int(self.diff)} "
            f"report: {int(self.report)} "
            f"write: {int(self.write)} "
            f"{g.shortFileName(filename)}"
        )
    self.filename = filename
    contents, tokens = self.init_tokens_from_file(filename)
    if not (contents and tokens):
        return False  # Not an error.
    if not isinstance(tokens[0], InputToken):
        self.oops(f"Not an InputToken: {tokens[0]!r}")

    # Beautify the contents, returning the original contents on any error.
    results = self.beautify(contents, filename, tokens)

    # Ignore changes only to newlines.
    if self.regularize_newlines(contents) == self.regularize_newlines(results):
        return False

    # Print reports.
    if self.beautified:  # --beautified.
        print(f"tbo: beautified: {g.shortFileName(filename)}")
    if self.diff:  # --diff.
        print(f"Diffs: {filename}")
        self.show_diffs(contents, results)

    # Write the (changed) file .
    if self.write:  # --write.
        self.write_file(filename, results)
    return True
</t>
<t tx="ekr.20240105145241.8">def init_tokens_from_file(self, filename: str) -&gt; tuple[
    str, list[InputToken]
]:  # pragma: no cover
    """
    Create the list of tokens for the given file.
    Return (contents, encoding, tokens).
    """
    self.indent_level = 0
    self.filename = filename
    contents = g.readFile(filename)
    if not contents:
        self.input_tokens = []
        return '', []
    self.input_tokens = input_tokens = Tokenizer().make_input_tokens(contents)
    return contents, input_tokens
</t>
<t tx="ekr.20240105145241.9"># Visitors (tbo.do_* methods) handle input tokens.
# Generators (tbo.gen_* methods) create zero or more output tokens.
</t>
<t tx="ekr.20240106170746.1">def set_context(self, i: int, context: str) -&gt; None:
    """
    Set self.input_tokens[i].context, but only if it does not already exist!

    See the docstring for pre_scan for details.
    """

    trace = False  # Do not delete the trace below.

    valid_contexts = (
        'annotation', 'arg', 'complex-slice', 'simple-slice',
        'dict', 'import', 'initializer',
    )
    if context not in valid_contexts:
        self.oops(f"Unexpected context! {context!r}")  # pragma: no cover

    token = self.input_tokens[i]

    if trace:  # pragma: no cover
        token_s = f"&lt;{token.kind}: {token.show_val(12)}&gt;"
        ignore_s = 'Ignore' if token.context else ' ' * 6
        print(f"{i:3} {ignore_s} token: {token_s} context: {context}")

    if not token.context:
        token.context = context
</t>
<t tx="ekr.20240106220724.1">def dump_token_range(self, i1: int, i2: int, tag: Optional[str] = None) -&gt; None:  # pragma: no cover
    """Dump the given range of input tokens."""
    if tag:
        print(tag)
    for token in self.input_tokens[i1 : i2 + 1]:
        print(token.dump())
</t>
<t tx="ekr.20240107141830.1">def gen_word_op(self, s: str) -&gt; None:
    """Add a word-op request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word-op', s)
    self.gen_blank()
</t>
<t tx="ekr.20240109035004.1">def gen_dot_op(self) -&gt; None:
    """Handle the '.' input token."""
    context = self.input_token.context

    # Get the previous significant **input** token.
    # This is the only call to next(i) anywhere!
    next_i = self._next(self.index)
    next = 'None' if next_i is None else self.input_tokens[next_i]
    import_is_next = next and next.kind == 'name' and next.value == 'import'

    if context == 'import':
        if (
            self.prev_output_kind == 'word'
            and self.prev_output_value in ('from', 'import')
        ):
            self.gen_blank()
            op = 'op' if import_is_next else 'op-no-blanks'
            self.gen_token(op, '.')
        elif import_is_next:
            self.gen_token('op', '.')
            self.gen_blank()
        else:
            self.pending_ws = ''
            self.gen_token('op-no-blanks', '.')
    else:
        self.pending_ws = ''
        self.gen_token('op-no-blanks', '.')
</t>
<t tx="ekr.20240109082712.1">def is_unary_op(self, i: int, val: str) -&gt; bool:

    if val == '~':
        return True
    if val not in '+-':  # pragma: no cover
        return False

    # Get the previous significant **input** token.
    # This is the only call to _prev(i) anywhere!
    prev_i = self._prev(i)
    prev_token = None if prev_i is None else self.input_tokens[prev_i]
    kind = prev_token.kind if prev_token else ''
    value = prev_token.value if prev_token else ''

    if kind in ('number', 'string'):
        return_val = False
    elif kind == 'op' and value in ')]':
        return_val = False
    elif kind == 'op' and value in '{([:':
        return_val = True
    elif kind != 'name':
        return_val = True
    else:
        # The hard case: prev_token is a 'name' token.
        # Any Python keyword indicates a unary operator.
        return_val = keyword.iskeyword(value) or keyword.issoftkeyword(value)
    return return_val
</t>
<t tx="ekr.20240110205127.1"># The parser calls scanner methods to move through the list of input tokens.
</t>
<t tx="ekr.20240111035404.1">__slots__ = [
    # Command-line arguments.
    'all', 'beautified', 'diff', 'report', 'write',

    # Global data.
    'contents', 'filename', 'input_tokens', 'output_list', 'tab_width',
    'insignificant_tokens',  # New.

    # Token-related data for visitors.
    'index', 'input_token', 'line_number',
    'pending_lws', 'pending_ws', 'prev_output_kind', 'prev_output_value',  # New.

    # Parsing state for visitors.
    'decorator_seen', 'in_arg_list', 'in_doc_part', 'state_stack', 'verbatim',

    # Whitespace state. Don't even *think* about changing these!
    'curly_brackets_level', 'indent_level', 'lws', 'paren_level', 'square_brackets_stack',

    # Regular expressions.
    'at_others_pat', 'beautify_pat', 'comment_pat', 'end_doc_pat',
    'nobeautify_pat', 'node_pat', 'start_doc_pat',
]
</t>
<t tx="ekr.20240111051726.1">def do_dedent(self) -&gt; None:
    """Handle dedent token."""
    # Note: other methods use self.indent_level.
    self.indent_level -= 1
    self.lws = self.indent_level * self.tab_width * ' '
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'dedent'
</t>
<t tx="ekr.20240112023403.1"># Debugging vars...
self.contents = contents
self.filename = filename
self.line_number: Optional[int] = None

# The input and output lists...
self.output_list: list[str] = []
self.input_tokens = input_tokens  # The list of input tokens.

# State vars for whitespace.
self.curly_brackets_level = 0  # Number of unmatched '{' tokens.
self.paren_level = 0  # Number of unmatched '(' tokens.
self.square_brackets_stack: list[bool] = []  # A stack of bools, for self.gen_word().
self.indent_level = 0  # Set only by do_indent and do_dedent.

# Parse state.
self.decorator_seen = False  # Set by do_name for do_op.
self.in_arg_list = 0  # &gt; 0 if in an arg list of a def.
self.in_doc_part = False
self.state_stack: list[ParseState] = []  # Stack of ParseState objects.

# Leo-related state.
self.verbatim = False  # True: don't beautify.

# Ivars describing the present input token...
self.index = 0  # The index within the tokens array of the token being scanned.
self.lws = ''  # Leading whitespace. Required!
</t>
<t tx="ekr.20240112082350.1">def internal_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about an error in the beautifier itself."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        # '\n\n'
        'Error in token-based beautifier!\n'
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
        "Please report this message to Leo's developers"
    )
</t>
<t tx="ekr.20240115233050.1">def _prev(self, i: int) -&gt; Optional[int]:
    """
    Return the previous *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.
    """
    i -= 1
    while i &gt;= 0:
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            return i
        i -= 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20240116040458.1">insignificant_kinds = (
    'comment', 'dedent', 'encoding', 'endmarker', 'indent', 'newline', 'nl', 'ws',
)

# 'name' tokens that may appear in expressions.
operator_keywords = (
    'await',  # Debatable.
    'and', 'in', 'not', 'not in', 'or',  # Operators.
    'True', 'False', 'None',  # Values.
)
</t>
<t tx="ekr.20240117053310.1">def oops(self, message: str) -&gt; None:  # pragma: no cover
    """Raise InternalBeautifierError."""
    raise InternalBeautifierError(self.internal_error_message(message))
</t>
<t tx="ekr.20240119062227.1">@language rest
@wrap

"""
Leo's token-based beautifier, three times faster than the beautifier in leoAst.py.

**Design**

The *pre_scan* method is the heart of the algorithm. It sets context
for the `:`, `=`, `**` and `.` tokens *without* using the parse tree.
*pre_scan* calls three *finishers*.

Each finisher uses a list of *relevant earlier tokens* to set the
context for one kind of (input) token. Finishers look behind (in the
stream of input tokens) with essentially no cost.

After the pre-scan, *tbo.beautify* (the main loop) calls *visitors*
for each separate type of *input* token.

Visitors call *code generators* to generate strings in the output
list, using *lazy evaluation* to generate whitespace.
"""
</t>
<t tx="ekr.20240126012433.1"></t>
<t tx="ekr.20240128114622.1">def pre_scan(self) -&gt; None:
    """
    Scan the entire file in one iterative pass, adding context to a few
    kinds of tokens as follows:

    Token   Possible Contexts (or None)
    =====   ===========================
    ':'     'annotation', 'dict', 'complex-slice', 'simple-slice'
    '='     'annotation', 'initializer'
    '*'     'arg'
    '**'    'arg'
    '.'     'import'
    """

    # The main loop.
    in_import = False
    scan_stack: list[ScanState] = []
    prev_token: Optional[InputToken] = None
    for i, token in enumerate(self.input_tokens):
        kind, value = token.kind, token.value
        if kind in 'newline':
            &lt;&lt; pre-scan 'newline' tokens &gt;&gt;
        elif kind == 'op':
            &lt;&lt; pre-scan 'op' tokens &gt;&gt;
        elif kind == 'name':
            &lt;&lt; pre-scan 'name' tokens &gt;&gt;
        # Remember the previous significant token.
        if kind not in self.insignificant_kinds:
            prev_token = token
    # Sanity check.
    if scan_stack:  # pragma: no cover
        print('pre_scan: non-empty scan_stack')
        print(scan_stack)
</t>
<t tx="ekr.20240128123117.1">top_state: Optional[ScanState] = scan_stack[-1] if scan_stack else None

# Handle '[' and ']'.
if value == '[':
    scan_stack.append(ScanState('slice', token))
elif value == ']':
    assert top_state and top_state.kind == 'slice'
    self.finish_slice(i, top_state)
    scan_stack.pop()

# Handle '{' and '}'.
if value == '{':
    scan_stack.append(ScanState('dict', token))
elif value == '}':
    assert top_state and top_state.kind == 'dict'
    self.finish_dict(i, top_state)
    scan_stack.pop()

# Handle '(' and ')'
elif value == '(':
    if self.is_python_keyword(prev_token) or prev_token and prev_token.kind != 'name':
        state_kind = '('
    else:
        state_kind = 'arg'
    scan_stack.append(ScanState(state_kind, token))
elif value == ')':
    assert top_state and top_state.kind in ('(', 'arg'), repr(top_state)
    if top_state.kind == 'arg':
        self.finish_arg(i, top_state)
    scan_stack.pop()

# Handle interior tokens in 'arg' and 'slice' states.
if top_state:
    if top_state.kind in ('dict', 'slice') and value == ':':
        top_state.value.append(i)
    if top_state.kind == 'arg' and value in '**=:,':
        top_state.value.append(i)

# Handle '.' and '(' tokens inside 'import' and 'from' statements.
if in_import and value in '(.':
    self.set_context(i, 'import')
</t>
<t tx="ekr.20240128230812.1"># 'import' and 'from x import' statements may span lines.
# 'ws' tokens represent continued lines like this:   ws: ' \\\n    '
if in_import and not scan_stack:
    in_import = False
</t>
<t tx="ekr.20240128231119.1">prev_is_yield = prev_token and prev_token.kind == 'name' and prev_token.value == 'yield'
if value in ('from', 'import') and not prev_is_yield:
    # 'import' and 'from x import' statements should be at the outer level.
    assert not scan_stack, scan_stack
    in_import = True
</t>
<t tx="ekr.20240128233406.1">def finish_slice(self, end: int, state: ScanState) -&gt; None:
    """Set context for all ':' when scanning from '[' to ']'."""

    # Sanity checks.
    assert state.kind == 'slice', repr(state)
    token = state.token
    assert token.value == '[', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Do nothing if there are no ':' tokens in the slice.
    if not colons:
        return

    # Compute final context by scanning the tokens.
    final_context = 'simple-slice'
    inter_colon_tokens = 0
    prev = token
    for i in range(i1 + 1, end - 1):
        token = self.input_tokens[i]
        kind, value = token.kind, token.value
        if kind not in self.insignificant_kinds:
            if kind == 'op':
                if value == '.':
                    # Ignore '.' tokens and any preceding 'name' token.
                    if prev and prev.kind == 'name':  # pragma: no cover
                        inter_colon_tokens -= 1
                elif value == ':':
                    inter_colon_tokens = 0
                elif value in '-+':
                    # Ignore unary '-' or '+' tokens.
                    if not self.is_unary_op_with_prev(prev, token):
                        inter_colon_tokens += 1
                        if inter_colon_tokens &gt; 1:
                            final_context = 'complex-slice'
                            break
                elif value == '~':
                    # '~' is always a unary op.
                    pass
                else:
                    # All other ops contribute.
                    inter_colon_tokens += 1
                    if inter_colon_tokens &gt; 1:
                        final_context = 'complex-slice'
                        break
            else:
                inter_colon_tokens += 1
                if inter_colon_tokens &gt; 1:
                    final_context = 'complex-slice'
                    break
            prev = token

    # Set the context of all outer-level ':' tokens.
    for i in colons:
        self.set_context(i, final_context)
</t>
<t tx="ekr.20240129034209.1">def is_unary_op_with_prev(self, prev: Optional[InputToken], token: InputToken) -&gt; bool:
    """
    Return True if token is a unary op in the context of prev, the previous
    significant token.
    """
    if token.value == '~':  # pragma: no cover
        return True
    if prev is None:
        return True  # pragma: no cover
    assert token.value in '**-+', repr(token.value)
    if prev.kind in ('number', 'string'):
        return_val = False
    elif prev.kind == 'op' and prev.value in ')]':
         # An unnecessary test?
        return_val = False  # pragma: no cover
    elif prev.kind == 'op' and prev.value in '{([:,':
        return_val = True
    elif prev.kind != 'name':
        # An unnecessary test?
        return_val = True  # pragma: no cover
    else:
        # prev is a'name' token.
        return self.is_python_keyword(token)
    return return_val
</t>
<t tx="ekr.20240129035336.1">def is_python_keyword(self, token: Optional[InputToken]) -&gt; bool:
    """Return True if token is a 'name' token referring to a Python keyword."""
    if not token or token.kind != 'name':
        return False
    return keyword.iskeyword(token.value) or keyword.issoftkeyword(token.value)
</t>
<t tx="ekr.20240129040347.1">def finish_dict(self, end: int, state: Optional[ScanState]) -&gt; None:
    """
    Set context for all ':' when scanning from '{' to '}'

    Strictly speaking, setting this context is unnecessary because
    tbo.gen_colon generates the same code regardless of this context.

    In other words, this method can be a do-nothing!
    """

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'dict', repr(state)
    token = state.token
    assert token.value == '{', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Set the context for all ':' tokens.
    for i in colons:
        self.set_context(i, 'dict')
</t>
<t tx="ekr.20240129041304.1">def finish_arg(self, end: int, state: Optional[ScanState]) -&gt; None:
    """Set context for all ':' when scanning from '(' to ')'."""

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'arg', repr(state)
    token = state.token
    assert token.value == '(', repr(token)
    values = state.value
    assert isinstance(values, list), repr(values)
    i1 = token.index
    assert i1 &lt; end, (i1, end)
    if not values:
        return

    # Compute the context for each *separate* '=' token.
    equal_context = 'initializer'
    for i in values:
        token = self.input_tokens[i]
        assert token.kind == 'op', repr(token)
        if token.value == ',':
            equal_context = 'initializer'
        elif token.value == ':':
            equal_context = 'annotation'
        elif token.value == '=':
            self.set_context(i, equal_context)
            equal_context = 'initializer'

    # Set the context of all outer-level ':', '*', and '**' tokens.
    prev: Optional[InputToken] = None
    for i in range(i1, end):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_kinds:
            if token.kind == 'op':
                if token.value in ('*', '**'):
                    if self.is_unary_op_with_prev(prev, token):
                        self.set_context(i, 'arg')
                elif token.value == '=':
                    # The code above has set the context.
                    assert token.context in ('initializer', 'annotation'), (i, repr(token.context))
                elif token.value == ':':
                    self.set_context(i, 'annotation')
            prev = token
</t>
<t tx="ekr.20240226131015.1">def user_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about a user error."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
    )
</t>
<t tx="ekr.20240418043826.1">def do_newline(self) -&gt; None:
    """
    do_newline: Handle a regular newline.

    From https://docs.python.org/3/library/token.html

    NEWLINE tokens end *logical* lines of Python code.
    """

    self.output_list.append('\n')
    self.pending_lws = ''  # Set only by 'dedent', 'indent' or 'ws' tokens.
    self.pending_ws = ''
    self.prev_output_kind = 'newline'
    self.prev_output_value = '\n'
</t>
<t tx="ekr.20240418043827.1">def do_nl(self) -&gt; None:
    """
    do_nl: Handle a continuation line.

    From https://docs.python.org/3/library/token.html

    NL tokens end *physical* lines. They appear when when a logical line of
    code spans multiple physical lines.
    """
    return self.do_newline()
</t>
<t tx="ekr.20240418045924.1">def do_op(self) -&gt; None:
    """Handle an op token."""
    val = self.input_token.value

    if val == '.':
        self.gen_dot_op()
    elif val == '@':
        self.gen_token('op-no-blanks', val)
        self.push_state('decorator')
    elif val == ':':
        # Treat slices differently.
        self.gen_colon()
    elif val in ',;':
        # Pep 8: Avoid extraneous whitespace immediately before
        # comma, semicolon, or colon.
        self.pending_ws = ''
        self.gen_token('op', val)
        self.gen_blank()
    elif val in '([{':
        # Pep 8: Avoid extraneous whitespace immediately inside
        # parentheses, brackets or braces.
        self.gen_lt()
    elif val in ')]}':
        # Ditto.
        self.gen_rt()
    elif val == '=':
        self.gen_equal_op()
    elif val in '~+-':
        self.gen_possible_unary_op()
    elif val == '*':
        self.gen_star_op()
    elif val == '**':
        self.gen_star_star_op()
    else:
        # Pep 8: always surround binary operators with a single space.
        # '==','+=','-=','*=','**=','/=','//=','%=','!=','&lt;=','&gt;=','&lt;','&gt;',
        # '^','~','*','**','&amp;','|','/','//',
        # Pep 8: If operators with different priorities are used, consider
        # adding whitespace around the operators with the lowest priorities.
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20240418050017.1">def do_name(self) -&gt; None:
    """Handle a name token."""
    name = self.input_token.value
    if name in self.operator_keywords:
        self.gen_word_op(name)
    else:
        self.gen_word(name)
</t>
<t tx="ekr.20240420034216.1"># Leo-specific code...
if self.node_pat.match(val):
    # Clear per-node state.
    self.in_doc_part = False
    self.verbatim = False
    self.decorator_seen = False
    # Do *not* clear other state, which may persist across @others.
        # self.curly_brackets_level = 0
        # self.in_arg_list = 0
        # self.indent_level = 0
        # self.lws = ''
        # self.paren_level = 0
        # self.square_brackets_stack = []
        # self.state_stack = []
else:
    # Keep track of verbatim mode.
    if self.beautify_pat.match(val):
        self.verbatim = False
    elif self.nobeautify_pat.match(val):
        self.verbatim = True
    # Keep trace of @doc parts, to honor the convention for splitting lines.
    if self.start_doc_pat.match(val):
        self.in_doc_part = True
    if self.end_doc_pat.match(val):
        self.in_doc_part = False
</t>
<t tx="ekr.20240927151701.1"></t>
<t tx="ekr.20240927151701.100"></t>
<t tx="ekr.20240927151701.101"></t>
<t tx="ekr.20240927151701.102">vertical (v) or horizontal (h)

myLeoSettings.leo: vertical</t>
<t tx="ekr.20240927151701.103"></t>
<t tx="ekr.20240927151701.104"></t>
<t tx="ekr.20240927151701.105">@language rest
@wrap

See #3456.

</t>
<t tx="ekr.20240927151701.106"></t>
<t tx="ekr.20240927151701.107"></t>
<t tx="ekr.20240927151701.108"># leonine</t>
<t tx="ekr.20240927151701.109"></t>
<t tx="ekr.20240927151701.110"></t>
<t tx="ekr.20240927151701.111"></t>
<t tx="ekr.20240927151701.112"></t>
<t tx="ekr.20240927151701.113">True: (Recommended) Make a "Recovered Nodes" node whenever
Leo reads a file that has been changed outside of Leo.
</t>
<t tx="ekr.20240927151701.114"></t>
<t tx="ekr.20240927151701.115"></t>
<t tx="ekr.20240927151701.116"></t>
<t tx="ekr.20240927151701.117"></t>
<t tx="ekr.20240927151701.118"></t>
<t tx="ekr.20240927151701.119"></t>
<t tx="ekr.20240927151701.120"></t>
<t tx="ekr.20240927151701.121"></t>
<t tx="ekr.20240927151701.122"></t>
<t tx="ekr.20240927151701.123"></t>
<t tx="ekr.20240927151701.124"></t>
<t tx="ekr.20240927151701.126">Set to True to enable node appearance modifications
See tree-declutter-patterns
</t>
<t tx="ekr.20240927151701.144"></t>
<t tx="ekr.20240927151701.145">Only supported with the mod_tempfname.py plugin.

True: The plugin will store temporary files utilizing cleaner
file names (no unique number is appended to the node's headline text).
Unique temporary directory paths are used to insure unique files are
created by creating temporary directories reflecting each node's ancestor
nodes in the Leo outline. Note: Do not have multiple sibling nodes (nodes
having the same parent node) in Leo with the same headline text. There will
be a conflict if both are opened in an external editor at the same time.

False: The plugin will store temporary files with an appended
unique number to insure unique temporary filenames.
</t>
<t tx="ekr.20240927151701.146">True: check all @&lt;file&gt; nodes in the outline for changes in corresponding external files.</t>
<t tx="ekr.20240927151701.147"></t>
<t tx="ekr.20240927151701.148"></t>
<t tx="ekr.20240927151701.149"></t>
<t tx="ekr.20240927151701.150"></t>
<t tx="ekr.20240927151701.151"></t>
<t tx="ekr.20240927151701.152">It is *strange* to set this to True!</t>
<t tx="ekr.20240927151701.153">@language rest

To test #2041 &amp; #2094

The @bool use-find-dialog and @bool minibuffer-find-mode settings comprise
a tri-state setting, as shown in this table:
    
minibuffer-find-mode    use-find-dialog     mode: Ctrl-F puts focus in
--------------------    ---------------     --------------------------
    True                    Ignored         minibuffer
    False                   True            dialog
    False                   False           Find tab in the log pane

*All modes*

- Start the search with Ctrl-F (start-search).
- Enter the find pattern.
- (Optional) Use &lt;Tab&gt; to enter the search pattern.
- Use &lt;Enter&gt; to start the search.

*dialog and find tab modes*

- Non-functional "buttons" remind you of key bindings.

*minibuffer mode*

- Use Ctrl-G as always to leave the minibuffer.
- The Find tab is not made visible, but the status area shows the settings.</t>
<t tx="ekr.20240927151701.154">@language rest

The @bool use-find-dialog and @bool minibuffer-find-mode settings comprise
a tri-state setting, as shown in this table:
    
minibuffer-find-mode    use-find-dialog     mode: Ctrl-F puts focus in
--------------------    ---------------     --------------------------
    True                    Ignored         minibuffer
    False                   True            dialog
    False                   False           Find tab in the log pane

*All modes*

- Start the seas with Ctrl-F (start-search).
- Enter the find pattern.
- (Optional) Use &lt;Tab&gt; to enter the search pattern.
- Use &lt;Enter&gt; to start the search.

*dialog and find tab modes*

- Non-functional "buttons" remind you of key bindings.

*minibuffer mode*

- Use Ctrl-G as always to leave the minibuffer.
- The Find tab is not made visible, but the status area shows the settings.</t>
<t tx="ekr.20240927151701.155">Added on-popover to import-html-tags (for leovue)</t>
<t tx="ekr.20240927151701.156"># lowercase html tags, one per line.
# *** Add ons-popover tag for LeoVue.

a
abbr
acronym
address
applet
area
b
base
basefont
bdo
big
blockquote
body
br
button
caption
center
cite
code
col
colgroup
dd
del
dfn
dir
div
dl
dt
em
fieldset
font
form
frame
frameset
head
h1
h2
h3
h4
h5
h6
hr
html
i
iframe
img
input
ins
kbd
label
legend
li
link
map
menu
meta
noframes
noscript
object
ol
ons-popover
optgroup
option
p
param
pre
q
s
samp
script
select
small
span
strike
strong
style
sub
sup
table
tbody
td
textarea
tfoot
th
thead
title
tr
tt
u
ul
var</t>
<t tx="ekr.20240927151701.157"># lowercase xml tags, one per line.

html
body
head
div
table
</t>
<t tx="ekr.20240927151701.170" annotate="7d71002858080000007072696f7269747971014d0f27580a000000707269736574646174657102580a000000323032312d30332d33307103752e"># Recommended plugins, from leoSettings.leo:

plugins_menu.py
mod_scripting.py
nav_qt.py
viewrendered.py


# contextmenu.py      # Required by the vim.py and xemacs.py plugins.
</t>
<t tx="ekr.20240927151701.176"></t>
<t tx="ekr.20240927151701.177"># True: show vr pane when opening a file.</t>
<t tx="ekr.20240927151701.178"># True: hide the vr pane for text-only renderings.</t>
<t tx="ekr.20240927151701.179"></t>
<t tx="ekr.20240927151701.184"></t>
<t tx="ekr.20240927151701.190">Only difference from myLeoSettings.leo

Note: EKRWinowsDark.leo defines comment1_font

All three @color settings work.
The @font setting does not work.
</t>
<t tx="ekr.20240927151701.191">Bold</t>
<t tx="ekr.20240927151701.192">Italics</t>
<t tx="ekr.20240927151701.193"></t>
<t tx="ekr.20240927151701.194"># bold keywords defined in forth-bold-words</t>
<t tx="ekr.20240927151701.195"></t>
<t tx="ekr.20240927151701.196"># Note: the default font size is 12.
rest_comment1_family = None
rest_comment1_size = 12pt
rest_comment1_slant = italic
rest_comment1_weight = None
</t>
<t tx="ekr.20240927151701.202"># Note: Use jj instead of escape to end insert mode.</t>
<t tx="ekr.20240927151701.203" __bookmarks="7d7100580700000069735f6475706571014930300a732e"></t>
<t tx="ekr.20240927151701.206">@language python

"""
Back up this .leo file.

os.environ['LEO_BACKUP'] must be the path to an existing (writable) directory.
"""
c.backup_helper(sub_dir='ekr-tbo-in-rust')
</t>
<t tx="ekr.20240927151701.207">@language python

print(p.gnx)</t>
<t tx="ekr.20240927151701.229">@language python
"""Recursively import all python files in a directory and clean the result."""
@tabwidth -4 # For a better match.
g.cls()
&lt;&lt; rust dir_list &gt;&gt;

dir_ = r'C:\Python\Python3.12\Lib\site-packages\coverage'
dir_ = r'C:\Python\Python3.12\Lib\site-packages\mypyc'
dir_ = r'C:\Python\Python3.12\Lib\site-packages\findimports.py'
dir_ = r'C:\Repos\ruff\crates'

c.recursiveImport(
    dir_=dir_,
    kind = '@clean', # '@auto', '@clean', '@nosent','@file',
    recursive = True,
    safe_at_file = True,
    # '.html', '.js', '.json', '.py', '.rs', '.svg', '.ts', '.tsx']
    # '.codon', '.cpp', '.cc', '.el', '.scm',
    theTypes = ['.py', 'rs'],
    verbose = False,
)
if 1:
    last = c.lastTopLevel()
    last.expand()
    if last.hasChildren():
        last.firstChild().expand()
    c.redraw(last)
print('Done')</t>
<t tx="ekr.20240927151701.230">dir_list = (
    r'C:\Repos\RustPython\common\src',
    r'C:\Repos\RustPython\compiler\codegen\src',
    r'C:\Repos\RustPython\compiler\core\src',
    r'C:\Repos\RustPython\compiler\src',
    r'C:\Repos\RustPython\compiler\codegen\src',  # compile.rs: AST to bytecode.
    r'C:\Repos\RustPython\compiler\core\src', # bytecode.rs: implements bytecodes.
    
    r'C:\Repos\RustPython\derive\src',
    r'C:\Repos\RustPython\derive-impl\src',
    r'C:\Repos\RustPython\pylib\src',
    r'C:\Repos\RustPython\src',
    r'C:\Repos\RustPython\stdlib\src',
    r'C:\Repos\RustPython\vm\src', # compiler.rs.
    r'C:\Repos\RustPython\vm\src\stdlib', # *****ast.rs  Also, many .rs versions of stdlib.
    r'C:\Repos\RustPython\vm\src\vm',  # compile.rs.
    r'C:\Repos\RustPython\vm\src\stdlib\ast', # gen.rs automatically generated by ast/asdl_rs.py.
)
</t>
<t tx="ekr.20240927151701.44"></t>
<t tx="ekr.20240927151701.45"></t>
<t tx="ekr.20240927151701.46">
</t>
<t tx="ekr.20240927151701.47"></t>
<t tx="ekr.20240927151701.48"># This node contains the commands needed to execute a program in a particular language.

# Format: language-name: command

# Create a temporary file if c.p is not any kind of @&lt;file&gt; node.

# Compute the final command as follows:

# 1. If command contains &lt;FILE&gt;, replace &lt;FILE&gt; with the full path to the external file.
# 2. If command contains &lt;NO-FILE&gt;, just remove &lt;NO-FILE&gt;.
# 3. Otherwise, append the full path to the external file to the command.

go: go run . &lt;NO-FILE&gt;
python: python
rust: rustc
</t>
<t tx="ekr.20240927151701.49"># This node contains the regex pattern to determine the line number in error messages.
# Format: language-name: regex pattern
#
# Patterns must define two groups, in either order:
# One group, containing only digits, defines the line number.
# The other group defines the file name.

go: ^\s*(.*):([0-9]+):([0-9]+):.+$
python: ^\s*File "(.+)", line ([0-9]+), in .+$
rust: ^\s*--&gt; (.+):([0-9]+):([0-9]+)\s*$</t>
<t tx="ekr.20240927151701.50">cargo-run
backup
</t>
<t tx="ekr.20240927151701.51"># legacy: (default) Leo's legacy layout
# big-tree: replaces @bool big-outline-pane</t>
<t tx="ekr.20240927151701.63"></t>
<t tx="ekr.20240927151701.64"></t>
<t tx="ekr.20240927151701.65"># The headline must be: @outline-data tree-abbreviations

# A list tree abbreviation names.

# For each abbreviation name, there should be corresponding child node,
# the **abbreviation node** whose headline matches the abbreviation name.

# When a tree abbreviation fires, Leo pastes all the descendants of
# the abbreviation node as the last children of the presently selected node.

importer;;
per-commander-plugin;;
demo;;
</t>
<t tx="ekr.20240927151701.66"></t>
<t tx="ekr.20240927151701.67"></t>
<t tx="ekr.20240927151701.68"></t>
<t tx="ekr.20240927151701.69">'''
A template for demonstrations based on plugins/demo.py.
The demo;; abbreviation will create this tree.
'''
&lt;&lt; imports &gt;&gt;
@others
# Use the *same* command/key binding for demo-start and demo.next.
try:
    if getattr(g.app, 'demo', None):
        g.app.demo.next()
    else:
        g.cls()
        print('starting demo')
        demo = MyDemo(c, trace=False)
        demo.bind('callout', callout)
        demo.bind('title', title)
        demo.start(script_string=script_string)
except Exception:
    g.app.demo = None
    raise
</t>
<t tx="ekr.20240927151701.70">if c.isChanged(): c.save()
import imp
from leo.core.leoQt import QtGui
import leo.plugins.demo as demo_module
imp.reload(demo_module)</t>
<t tx="ekr.20240927151701.71"># A short example. Change as needed.
script_string = '''\
callout('Callout 1 centered')
title('This is title 1')
###
callout('Callout 2 (700, 200)', position=[700, 200])
title('This is title 2')
demo.next()
'''
</t>
<t tx="ekr.20240927151701.72">class MyDemo (demo_module.Demo):
    
    def setup_script(self):
        '''Delete all previously shown widgets.'''
        self.delete_widgets()</t>
<t tx="ekr.20240927151701.73">def callout(text, **keys):
    w = demo_module.Callout(text, **keys)
    
def title(text, **keys):
    w = demo_module.Title(text, **keys)
</t>
<t tx="ekr.20240927151701.74"></t>
<t tx="ekr.20240927151701.75">&lt;&lt; docstring &gt;&gt;
### From leoSettings.leo
# Created 2017/05/30
@language python
@tabwidth -4
__version__ = '0.0'
&lt;&lt; version history &gt;&gt;
&lt;&lt; imports &gt;&gt;
@others</t>
<t tx="ekr.20240927151701.76">'''
&lt;|docstring|&gt;
'''
</t>
<t tx="ekr.20240927151701.77">@
Put notes about each version here.
&lt;|Initial version notes|&gt;</t>
<t tx="ekr.20240927151701.78">import leo.core.leoGlobals as g

&lt;|imports|&gt;</t>
<t tx="ekr.20240927151701.79">def init ():
        
    ok = g.app.gui.guiName() in ('qt','qttabs')
    if ok:
        if 1: # Create the commander class *before* the frame is created.
            g.registerHandler('before-create-leo-frame',onCreate)
        else: # Create the commander class *after* the frame is created.
            g.registerHandler('after-create-leo-frame',onCreate)
        g.plugin_signon(__name__)   
    return ok
</t>
<t tx="ekr.20240927151701.80">def onCreate (tag, keys):
    
    c = keys.get('c')
    if c:
        thePluginController = pluginController(c)
</t>
<t tx="ekr.20240927151701.81">class &lt;|Controller Class Name|&gt;:
    
    @others</t>
<t tx="ekr.20240927151701.82">def __init__ (self,c):
    
    self.c = c
    # Warning: hook handlers must use keywords.get('c'), NOT self.c.
    &lt;|ivars|&gt;</t>
<t tx="ekr.20240927151701.83"></t>
<t tx="ekr.20240927151701.84">'''
The @auto importer for the {|{x=get_language()}|} language.

Created {|{x=time.strftime("%Y/%m/%d")}|} by the `importer;;` abbreviation.
'''
import leo.plugins.importers.linescanner as linescanner
Importer = linescanner.Importer
@others
importer_dict = {
    'class': {|{x=cap_name}|}_Importer,
    'extensions': [&lt;|comma-separated lists of extensions|&gt;],
        # Example: ['.c', '.cc', '.c++', '.cpp', '.cxx', '.h', '.h++']
}
@language python
@tabwidth -4


</t>
<t tx="ekr.20240927151701.85">class {|{x=cap_name}|}_Importer(Importer):
    '''The importer for the {|{x=name}|} language.'''

    def __init__(self, importCommands):
        '''{|{x=cap_name}|}_Importer.__init__'''
        # Init the base class.
        Importer.__init__(self,
            importCommands,
            language = '{|{x=name}|}',
            state_class = {|{x=cap_name}|}_ScanState,
            strict = &lt;|True leading whitespace is significant. Otherwise False|&gt;,
        )
        
    @others
</t>
<t tx="ekr.20240927151701.86"># These can be overridden in subclasses.
</t>
<t tx="ekr.20240927151701.87">### define an override if desired...

if 0: # The base class
    def clean_headline(self, s):
        '''Return a cleaned up headline s.'''
        return s.strip()
        
# A more complex example, for the C language.

# def clean_headline(self, s):
    # '''Return a cleaned up headline s.'''
    # import re
    # type1 = r'(static|extern)*'
    # type2 = r'(void|int|float|double|char)*'
    # class_pattern = r'\s*(%s)\s*class\s+(\w+)' % (type1)
    # pattern = r'\s*(%s)\s*(%s)\s*(\w+)' % (type1, type2)
    # m = re.match(class_pattern, s)
    # if m:
        # prefix1 = '%s ' % (m.group(1)) if m.group(1) else ''
        # return '%sclass %s' % (prefix1, m.group(2))
    # m = re.match(pattern, s)
    # if m:
        # prefix1 = '%s ' % (m.group(1)) if m.group(1) else ''
        # prefix2 = '%s ' % (m.group(2)) if m.group(2) else ''
        # h = m.group(3) or '&lt;no c function name&gt;'
        # return '%s%s%s' % (prefix1, prefix2, h)
    # else:
        # return s
</t>
<t tx="ekr.20240927151701.88">def clean_nodes(self, parent):
    '''
    Clean all nodes in parent's tree.
    Subclasses override this as desired.
    See perl_i.clean_nodes for an example.
    '''
    pass
</t>
<t tx="ekr.20240927151701.89">class {|{x=cap_name}|}_ScanState:
    '''A class representing the state of the {|{x=name}|} line-oriented scan.'''
    
    def __init__(self, d=None):
        '''{|{x=cap_name}|}_ScanState.__init__'''
        if d:
            prev = d.get('prev')
            self.context = prev.context
            ### Adjust these by hand.
            self.curlies = prev.curlies
        else:
            self.context = ''
            ### Adjust these by hand.
            self.curlies = 0

    def __repr__(self):
        '''{|{x=cap_name}|}_ScanState.__repr__'''
        ### Adjust these by hand.
        return "{|{x=cap_name}|}_ScanState context: %r curlies: %s" % (
            self.context, self.curlies)

    __str__ = __repr__

    @others

</t>
<t tx="ekr.20240927151701.9" __bookmarks="7d7100580700000069735f6475706571014930300a732e">@language rest
@wrap

The @settings tree contains all active settings. 

Settings outside this tree have no effect.</t>
<t tx="ekr.20240927151701.90">def level(self):
    '''{|{x=cap_name}|}_ScanState.level.'''
    return &lt;|self.curlies|&gt;
        ### Examples:
        # self.indent # for python, coffeescript.
        # self.curlies
        # (self, curlies, self.parens)
</t>
<t tx="ekr.20240927151701.91">def update(self, data):
    '''
    {|{x=cap_name}|}_ScanState.update

    Update the state using the 6-tuple returned by v2_scan_line.
    Return i = data[1]
    '''
    context, i, delta_c, delta_p, delta_s, bs_nl = data
    # All ScanState classes must have a context ivar.
    self.context = context
    self.curlies += delta_c  
    ### Update {|{x=cap_name}|}_ScanState ivars
    # self.bs_nl = bs_nl
    # self.parens += delta_p
    # self.squares += delta_s
    return i
</t>
<t tx="ekr.20240927151701.92"></t>
<t tx="ekr.20240927151701.93"></t>
<t tx="ekr.20240927151701.94">True: same as recent_files_group, except that even files (basenames) which are unique
have their containing path listed in the submenu - so visual clutter is reduced
but you can still see where things come from before you load them.

False: don't use submenus for multiple path entries, unless recent_files_group
is true (and recent_files_omit_directories is False)
</t>
<t tx="ekr.20240927151701.95"></t>
<t tx="ekr.20240927151701.96">True: show user tips on startup.</t>
<t tx="ekr.20240927151701.97"></t>
<t tx="ekr.20240927151701.98"></t>
<t tx="ekr.20240927151701.99"></t>
<t tx="ekr.20240927152759.1">@language python
g.cls()
import os
import subprocess

if c.changed:
    c.save()
command = 'cargo run'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240927153018.1"></t>
<t tx="ekr.20240927154009.1"></t>
<t tx="ekr.20240927154016.1">@language rest
@nowrap

code:

ekr-tbo-in-rust: https://github.com/edreamleo/ekr-tbo-in-rust
ruff_python_parser: https://github.com/astral-sh/ruff/tree/main/crates/ruff_python_parser/src
lexer.rs: https://github.com/astral-sh/ruff/blob/main/crates/ruff_python_parser/src/lexer.rs

docs:
Rust Book: https://doc.rust-lang.org/stable/book/title-page.html

**Summary**

- LB::make_input_list is too slow.
  This code can not be significantly faster than Leo's python beautifier!
  
- LB:beautify is not the problem. It is fast enough.
</t>
<t tx="ekr.20240927154323.1">@language rest
@wrap

*** Use functional style???
</t>
<t tx="ekr.20240928073118.1">@language python
g.cls()
import os
import subprocess

if c.changed:
    c.save()
command = 'cargo fmt'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240928185643.1">@language rest
@wrap

Python (with extra tracing code in tbo.init_tokens_from_file:

&gt; python -c "import leo.core.leoTokens" --all --report leo\core\leoFrame.py
tbo: 0.03 sec. dirty: 0   checked: 1   beautified: 0   in leo\core\leoFrame.py

       read:   0.28 ms
make_tokens:  29.45 ms
      total:  29.73 ms
      
Rust, with nanosecond resolution.

 leoFrame.py

leoFrame.py

     files: 1, tokens: 14619, ws tokens: 5156
       read:    0.5 ms
make_tokens:   14.2 ms  Full code, without slices.
make_tokens:   11.4 ms Full code, with slices
make_tokens:   10.7 ms  Empty loop
   beautify:    7.3 ms
      write:    0.00 ms
      total:   22.08 ms
      
This only 10% faster than the python code.
</t>
<t tx="ekr.20240929024648.113">fn make_input_list(
    contents: &amp;str,
    input_list: &amp;mut Vec&lt;InputTok&gt;,
) -&gt; (usize, usize) {
    // Add InputToks to the input_list for every token given by the RustPython lex.
    // The gem: Generate "ws" pseudo-tokens for all whitespace.
    let mut tokens_n: usize = 0;
    let mut ws_tokens_n: usize = 0;
    let mut prev_start: usize = 0;
    for token_tuple in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        use Tok::*;
        tokens_n += 1;
        let (token, range) = token_tuple;
        let tok_value = &amp;contents[range];
        let start_i: usize = usize::from(range.start());
        let end_i: usize = usize::from(range.end());
        
        // The gem: create a whitespace pseudo-tokens.
        if start_i &gt; prev_start {
            let ws = &amp;contents[prev_start..start_i];
            add_input_token(input_list, "ws", ws);
            ws_tokens_n += 1
        }
        prev_start = end_i;

        // Variants names are necessary, but otherwise not used.
        #[allow(unused_variables)]
        
        let class_name = match token {
            // Tokens with values...
            // Use tok_value for *all* values.
            Comment(value) =&gt; "Comment",  // No idea why parens are needed here.
            Complex { real, imag } =&gt; "Complex",
            Float { value } =&gt; "Float",
            Int { value } =&gt; "Int",
            Name { name } =&gt; "Name",
            Tok::String { value, kind, triple_quoted } =&gt; "String",
            
            // Common tokens...
            Class =&gt; "Class",
            Dedent =&gt; "Dedent",
            Def =&gt; "Def",
            Indent =&gt; "Indent",
            Newline =&gt; "Newline",
            NonLogicalNewline =&gt; "NonLogicalNewline",

            // All other tokens...
            Amper =&gt; "Amper",
            AmperEqual =&gt; "AmperEqual",
            And =&gt; "And",
            As =&gt; "As",
            Assert =&gt; "Assert",
            Async =&gt; "Async",
            At =&gt; "At",
            AtEqual =&gt; "AtEqual",
            Await =&gt; "Await",
            Break =&gt; "Break",
            Case =&gt; "Case",
            CircumFlex =&gt; "CircumFlex",
            CircumflexEqual =&gt; "CircumflexEqual",
            Colon =&gt; "Colon",
            ColonEqual =&gt; "ColonEqual",
            Comma =&gt; "Comma",
            Continue =&gt; "Continue",
            Del =&gt; "Del",
            Dot =&gt; "Dot",
            DoubleSlash =&gt; "DoubleSlash",
            DoubleSlashEqual =&gt; "DoubleSlashEqual",
            DoubleStar =&gt; "DoubleStar",
            DoubleStarEqual =&gt; "DoubleStarEqual",
            Elif =&gt; "Elif",
            Ellipsis =&gt; "Ellipsis",
            Else =&gt; "Else",
            EndOfFile =&gt; "EndOfFile",
            EqEqual =&gt; "EqEqual",
            Equal =&gt; "Equal",
            Except =&gt; "Except",
            False =&gt; "False",
            Finally =&gt; "Finally",
            For =&gt; "For",
            From =&gt; "From",
            Global =&gt; "Global",
            Greater =&gt; "Greater",
            GreaterEqual =&gt; "GreaterEqual",
            If =&gt; "If",
            Import =&gt; "Import",
            In =&gt; "In",
            Is =&gt; "Is",
            Lambda =&gt; "Lambda",
            Lbrace =&gt; "Lbrace",
            LeftShift =&gt; "LeftShift",
            LeftShiftEqual =&gt; "LeftShiftEqual",
            Less =&gt; "Less",
            LessEqual =&gt; "LessEqual",
            Lpar =&gt; "Lpar",
            Lsqb =&gt; "Lsqb",
            Match =&gt; "Match",
            Minus =&gt; "Minus",
            MinusEqual =&gt; "MinusEqual",
            None =&gt; "None",
            Nonlocal =&gt; "Nonlocal",
            Not =&gt; "Not",
            NotEqual =&gt; "NotEqual",
            Or =&gt; "Or",
            Pass =&gt; "Pass",
            Percent =&gt; "Percent",
            PercentEqual =&gt; "PercentEqual",
            Plus =&gt; "Plus",
            PlusEqual =&gt; "PlusEqual",
            Raise =&gt; "Raise",
            Rarrow =&gt; "Rarrow",
            Rbrace =&gt; "Rbrace",
            Return =&gt; "Return",
            RightShift =&gt; "RightShift",
            RightShiftEqual =&gt; "RightShiftEqual",
            Rpar =&gt; "Rpar",
            Rsqb =&gt; "Rsqb",
            Semi =&gt; "Semi",
            Slash =&gt; "Slash",
            SlashEqual =&gt; "SlashEqual",
            Star =&gt; "Star",
            StarEqual =&gt; "StarEqual",
            StartExpression =&gt; "StartExpression",
            StartInteractive =&gt; "StartInteractive",
            StartModule =&gt; "StartModule",
            Tilde =&gt; "Tilde",
            True =&gt; "True",
            Try =&gt; "Try",
            Type =&gt; "Type",
            Vbar =&gt; "Vbar",
            VbarEqual =&gt; "VbarEqual",
            While =&gt; "While",
            With =&gt; "With",
            Yield =&gt; "Yield",
        };
        // add_input_token(&amp;mut input_list, class_name, tok_value);
        add_input_token(input_list, class_name, tok_value);
    }
    return (tokens_n, ws_tokens_n);
}
</t>
<t tx="ekr.20240929024648.120">// Only Clone is valid for String.
#[derive(Clone)]
struct InputTok {
    kind: String,
    value: String,
}

impl fmt::Debug for InputTok {
    fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result {
        let kind_s = format!("{:?}", self.kind);
        let mut value = self.value.to_string();
        if true {
            return write!(f, "{value} ");
        } else {
            // Debug format.
            value.truncate(60);
            // repr format is not useful.
            // let value_s = format!("{:?}", value);
            let value_s = format!("{}", value);
            return write!(f, "InputTok: {kind_s:&gt;10}: {value_s}");
        }
    }
}
</t>
<t tx="ekr.20240929031635.1">fn scan_input_list(contents: String, tokens: Vec&lt;(Tok, TextRange)&gt;) -&gt; usize {

    let mut count: usize = 0;
    for (token, range) in tokens {
        // Range is a TextRange.
        count += 1;
        // To do: Find gaps in the ranges.
        let start_i = usize::from(range.start());
        let end_i = usize::from(range.end());
        if false {
            if count &lt; 20 {
                println!("{start_i:&gt;3}..{end_i:3} token: {token:?}");
            }
        }
    }
    return count;
}
</t>
<t tx="ekr.20240929032636.1">pub fn entry() {
    // leoFrame.py is a typical size
    let file_path = "C:\\Repos\\leo-editor\\leo\\core\\leoFrame.py";
    let short_file_name = "leoFrame.py";
    &lt;&lt; 1: read &gt;&gt;
    &lt;&lt; 2: Make input_list &gt;&gt;
    &lt;&lt; 3: print stats &gt;&gt;
}
</t>
<t tx="ekr.20240929032710.1">fn fmt_ms(t: u128) -&gt; String {
    //! Convert microseconds to fractional milliseconds.
    let ms = t / 1000;
    let micro = (t % 1000) / 10;
    return f!("{ms}.{micro:02}");  // Two-digits for fraction.
}

</t>
<t tx="ekr.20240929033044.1">fn add_input_token (input_list: &amp;mut Vec&lt;InputTok&gt;, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    let new_tok = InputTok {
        kind: kind.to_string(),
        value: value.to_string()
    };
    input_list.push(new_tok);
}
</t>
<t tx="ekr.20240929074037.1">#[derive(Debug)]
pub struct Beautifier {
    // Set in LB:beautify_one_file...
    args: Vec&lt;String&gt;,
    files_list: Vec&lt;String&gt;,
    input_list: Vec&lt;InputTok&gt;,
    output_list: Vec&lt;String&gt;,
    stats: Stats,
    // Set in LB:beautify...
    // Debugging
    line_number: i32, // Use -1 instead of None?
    // State vars for whitespace.
    curly_brackets_level: i32,
    indent_level: i32,
    paren_level: i32,
    square_brackets_stack: Vec&lt;bool&gt;,
    // Parse state.
    decorator_seen: bool, // Set by do_name for do_op.
    in_arg_list: i32,     // &gt; 0 if in an arg list of a def.
    in_doc_part: bool,
    // To do
    // state_stack = Vec&lt;ParseState&gt;,  // list[ParseState] = []  # Stack of ParseState objects.
    // Leo-related state.
    verbatim: bool,
    // Ivars describing the present input token.
    index: u32,
    lws: String,
}

///// Temporary.
#[allow(dead_code)]
#[allow(non_snake_case)]
impl Beautifier {
    @others
}
</t>
<t tx="ekr.20240929074037.10">fn do_Complex(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Complex", tok_value);
}
</t>
<t tx="ekr.20240929074037.100">fn do_StartModule(&amp;mut self) {
    // self.add_output_string("StartModule", "");
    println!("do_StartModule");
}
</t>
<t tx="ekr.20240929074037.101">fn do_Tilde(&amp;mut self) {
    self.add_output_string("Tilde", "~");
}
</t>
<t tx="ekr.20240929074037.102">fn do_True(&amp;mut self) {
    self.add_output_string("True", "True");
}
</t>
<t tx="ekr.20240929074037.103">fn do_Try(&amp;mut self) {
    self.add_output_string("Try", "try");
}
</t>
<t tx="ekr.20240929074037.104">fn do_Type(&amp;mut self) {
    self.add_output_string("Type", "type");
}
</t>
<t tx="ekr.20240929074037.105">fn do_Vbar(&amp;mut self) {
    self.add_output_string("Vbar", "|");
}
</t>
<t tx="ekr.20240929074037.106">fn do_VbarEqual(&amp;mut self) {
    self.add_output_string("VbarEqual", "|=");
}
</t>
<t tx="ekr.20240929074037.107">fn do_While(&amp;mut self) {
    self.add_output_string("While", "while");
}
</t>
<t tx="ekr.20240929074037.108">fn do_With(&amp;mut self) {
    self.add_output_string("With", "with");
}
</t>
<t tx="ekr.20240929074037.109">fn do_Yield(&amp;mut self) {
    self.add_output_string("Yield", "yield");
}
</t>
<t tx="ekr.20240929074037.11">fn do_Float(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Float", tok_value);
}
</t>
<t tx="ekr.20240929074037.110">fn enabled(&amp;self, arg: &amp;str) -&gt; bool {
    //! Beautifier::enabled: return true if the given command-line argument is enabled.
    //! Example:  x.enabled("--report");
    return self.args.contains(&amp;arg.to_string());
}
</t>
<t tx="ekr.20240929074037.111">fn get_args(&amp;mut self) {
    //! Beautifier::get_args: Set the args and files_list ivars.
    let args: Vec&lt;String&gt; = env::args().collect();
    let valid_args = vec![
        "--all",
        "--beautified",
        "--diff",
        "-h",
        "--help",
        "--report",
        "--write",
    ];
    for (i, arg) in args.iter().enumerate() {
        if i &gt; 0 {
            if valid_args.contains(&amp;arg.as_str()) {
                self.args.push(arg.to_string())
            } else if arg.as_str().starts_with("--") || arg.as_str().starts_with("--") {
                println!("Ignoring invalid arg: {arg}");
            } else {
                println!("File: {arg}");
                self.files_list.push(arg.to_string());
            }
        }
    }
}
</t>
<t tx="ekr.20240929074037.112">fn make_input_list(&amp;mut self, contents: &amp;str) {
    // Add InputToks to the input_list for every token given by the RustPython lex.
    let mut n_tokens: u64 = 0;
    let mut n_ws_tokens: u64 = 0;
    let mut prev_start: usize = 0;
    for token_tuple in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        use Tok::*;
        n_tokens += 1;
        let (token, range) = token_tuple;
        let tok_value = &amp;contents[range];

        // The gem: create a whitespace pseudo-tokens.
        // This code adds maybe about 1 ms when beautifying leoFrame.py.
        // With the gem: 14.1 - 14.5 ms. Without: 13.1 - 13.7 ms.
        let start_i = usize::from(range.start());
        let end_i = usize::from(range.end());
        if start_i &gt; prev_start {
            let ws = &amp;contents[prev_start..start_i];
            self.add_input_token("ws", ws);
            n_ws_tokens += 1
        }
        prev_start = end_i;

        &lt;&lt; Calculate class_name using match token &gt;&gt;
        self.add_input_token(class_name, tok_value);
    }
    // Update counts.
    self.stats.n_tokens += n_tokens;
    self.stats.n_ws_tokens += n_ws_tokens;
}
</t>
<t tx="ekr.20240929074037.113">fn beautify(&amp;mut self) -&gt; String {
    //! Beautify the input_tokens, creating the output_list.
    &lt;&lt; LB::beautify: init ivars &gt;&gt;
    if true {
        for input_token in self.input_list.clone() {
            &lt;&lt; LB: beautify: dispatch on input_token.kind &gt;&gt;
        }
    }
    // return ''.join(self.output_list);
    let mut result = String::new();
    for s in &amp;self.output_list {
        result.push_str(&amp;s);
    }
    return result;
}
</t>
<t tx="ekr.20240929074037.114">pub fn new() -&gt; Beautifier {
    let mut x = Beautifier {
        // Set in beautify_one_file
        args: Vec::new(),
        files_list: Vec::new(),
        input_list: Vec::new(),
        output_list: Vec::new(),
        stats: Stats::new(),
        // Set in LB::beautify.
        // state_stack = Vec&lt;ParseState&gt;,  // list[ParseState] = []  # Stack of ParseState objects.
        curly_brackets_level: 0,
        decorator_seen: false,
        in_arg_list: 0,
        in_doc_part: false,
        indent_level: 0,
        index: 0,
        line_number: 0,
        lws: String::new(),
        paren_level: 0,
        square_brackets_stack: Vec::new(),
        verbatim: false,
    };
    x.get_args();
    return x;
}
</t>
<t tx="ekr.20240929074037.115">fn show_args(&amp;self) {
    println!("Command-line arguments...");
    for (i, arg) in self.args.iter().enumerate() {
        if i &gt; 0 {
            println!("  {arg}");
        }
    }
    for file_arg in self.files_list.iter() {
        println!("  {file_arg}");
    }
}
</t>
<t tx="ekr.20240929074037.116">fn show_help(&amp;self) {
    //! Beautifier::show_help: print the help messages.
    println!(
        "{}",
        textwrap::dedent(
            "
        Beautify or diff files.

        -h --help:      Print this help message and exit.
        --all:          Beautify all files, even unchanged files.
        --beautified:   Report beautified files individually, even if not written.
        --diff:         Show diffs instead of changing files.
        --report:       Print summary report.
        --write:        Write beautifed files (dry-run mode otherwise).
    "
        )
    );
}
</t>
<t tx="ekr.20240929074037.117">fn show_output_list(&amp;self) {
    println!("\nOutput list...");
    for (i, arg) in self.output_list.iter().enumerate() {
        if i &gt; 0 {
            print!("{:?}", arg);
        }
    }
}
</t>
<t tx="ekr.20240929074037.12">fn do_Int(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Int", tok_value);
}
</t>
<t tx="ekr.20240929074037.13">fn do_Name(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Name", tok_value);
}
</t>
<t tx="ekr.20240929074037.14">fn do_String(&amp;mut self, tok_value: &amp;str) {
    // correct.
    // print!("{tok_value}");

    // incorrect.
    // let quote = if *triple_quoted {"'''"} else {"'"};
    // print!("{:?}:{quote}{value}{quote}", kind);

    self.add_output_string("String", tok_value);
}
</t>
<t tx="ekr.20240929074037.15"></t>
<t tx="ekr.20240929074037.16">fn do_Dedent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Dedent", tok_value);
}
</t>
<t tx="ekr.20240929074037.17">fn do_Indent(&amp;mut self, tok_value: &amp;str) {
    self.add_output_string("Indent", tok_value);
}
</t>
<t tx="ekr.20240929074037.18">fn do_Newline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240929074037.19">fn do_NonLogicalNewline(&amp;mut self) {
    self.add_output_string("Indent", "\n");
}
</t>
<t tx="ekr.20240929074037.2">#[allow(unused_variables)]
fn add_output_string(&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add value to the output list.
    //! kind is for debugging.
    if !value.is_empty() {
        self.output_list.push(value.to_string())
    }
}
</t>
<t tx="ekr.20240929074037.20"></t>
<t tx="ekr.20240929074037.21">fn do_Amper(&amp;mut self) {
    self.add_output_string("Amper", "&amp;");
}
</t>
<t tx="ekr.20240929074037.22">fn do_AmperEqual(&amp;mut self) {
    self.add_output_string("AmperEqual", "&amp;=");
}
</t>
<t tx="ekr.20240929074037.23">fn do_And(&amp;mut self) {
    self.add_output_string("And", "and");
}
</t>
<t tx="ekr.20240929074037.24">fn do_As(&amp;mut self) {
    self.add_output_string("As", "as");
}
</t>
<t tx="ekr.20240929074037.25">fn do_Assert(&amp;mut self) {
    self.add_output_string("Assert", "assert");
}
</t>
<t tx="ekr.20240929074037.26">fn do_Async(&amp;mut self) {
    self.add_output_string("Async", "async");
}
</t>
<t tx="ekr.20240929074037.27">fn do_At(&amp;mut self) {
    self.add_output_string("At", "@");
}
</t>
<t tx="ekr.20240929074037.28">fn do_AtEqual(&amp;mut self) {
    self.add_output_string("AtEqual", "@=");
}
</t>
<t tx="ekr.20240929074037.29">fn do_Await(&amp;mut self) {
    self.add_output_string("Await", "await");
}
</t>
<t tx="ekr.20240929074037.3">// #[allow(dead_code)]
fn add_input_token(&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    self.input_list.push(InputTok {
        kind: kind.to_string(),
        value: value.to_string(),
    });
}
</t>
<t tx="ekr.20240929074037.30">fn do_Break(&amp;mut self) {
    self.add_output_string("Break", "break");
}
</t>
<t tx="ekr.20240929074037.31">fn do_Case(&amp;mut self) {
    self.add_output_string("Case", "case");
}
</t>
<t tx="ekr.20240929074037.32">fn do_CircumFlex(&amp;mut self) {
    self.add_output_string("CircumFlex", "^");
}
</t>
<t tx="ekr.20240929074037.33">fn do_CircumflexEqual(&amp;mut self) {
    self.add_output_string("CircumflexEqual", "^=");
}
</t>
<t tx="ekr.20240929074037.34">fn do_Class(&amp;mut self) {
    self.add_output_string("Class", "class");
}
</t>
<t tx="ekr.20240929074037.35">fn do_Colon(&amp;mut self) {
    self.add_output_string("Colon", ":");
}
</t>
<t tx="ekr.20240929074037.36">fn do_ColonEqual(&amp;mut self) {
    self.add_output_string("ColonEqual", ":=");
}
</t>
<t tx="ekr.20240929074037.37">fn do_Comma(&amp;mut self) {
    self.add_output_string("Comma", ",");
}
</t>
<t tx="ekr.20240929074037.38">fn do_Continue(&amp;mut self) {
    self.add_output_string("Continue", "continue");
}
</t>
<t tx="ekr.20240929074037.39">fn do_Def(&amp;mut self) {
    self.add_output_string("Def", "def");
}
</t>
<t tx="ekr.20240929074037.4">pub fn beautify_all_files(&amp;mut self) {
    // for file_name in self.files_list.clone() {
    for file_name in self.files_list.clone() {
        self.beautify_one_file(&amp;file_name);
    }
}

</t>
<t tx="ekr.20240929074037.40">fn do_Del(&amp;mut self) {
    self.add_output_string("Del", "del");
}
</t>
<t tx="ekr.20240929074037.41">fn do_Dot(&amp;mut self) {
    self.add_output_string("Dot", ".");
}
</t>
<t tx="ekr.20240929074037.42">fn do_DoubleSlash(&amp;mut self) {
    self.add_output_string("DoubleSlash", "//");
}
</t>
<t tx="ekr.20240929074037.43">fn do_DoubleSlashEqual(&amp;mut self) {
    self.add_output_string("DoubleSlashEqual", "//=");
}
</t>
<t tx="ekr.20240929074037.44">fn do_DoubleStar(&amp;mut self) {
    self.add_output_string("DoubleStar", "**");
}
</t>
<t tx="ekr.20240929074037.45">fn do_DoubleStarEqual(&amp;mut self) {
    self.add_output_string("DoubleStarEqual", "**=");
}
</t>
<t tx="ekr.20240929074037.46">fn do_Elif(&amp;mut self) {
    self.add_output_string("Elif", "elif");
}
</t>
<t tx="ekr.20240929074037.47">fn do_Ellipsis(&amp;mut self) {
    self.add_output_string("Ellipsis", "...");
}
</t>
<t tx="ekr.20240929074037.48">fn do_Else(&amp;mut self) {
    self.add_output_string("Else", "else");
}
</t>
<t tx="ekr.20240929074037.49">fn do_EndOfFile(&amp;mut self) {
    self.add_output_string("EndOfFile", "EOF");
}
</t>
<t tx="ekr.20240929074037.5">fn beautify_one_file(&amp;mut self, file_name: &amp;str) {
    self.stats.n_files += 1;
    if true {
        // Testing only: print the short file name.
        let file_path = path::Path::new(file_name);
        let os_str = file_path.file_name().unwrap(); // &amp;OsStr
        let short_file_name = os_str.to_str().unwrap();
        println!("{short_file_name}");
    }
    // Read the file into contents (a String).
    self.output_list = Vec::new();
    let t1 = std::time::Instant::now();
    let contents = fs::read_to_string(file_name).expect("Error reading{file_name}");
    self.stats.read_time += t1.elapsed().as_nanos();
    // Make the list of input tokens
    let t2 = std::time::Instant::now();
    self.make_input_list(&amp;contents);
    self.stats.make_tokens_time += t2.elapsed().as_nanos();
    // Beautify.
    let t3 = std::time::Instant::now();
    self.beautify();
    self.stats.beautify_time += t3.elapsed().as_nanos();
}
</t>
<t tx="ekr.20240929074037.50">fn do_EqEqual(&amp;mut self) {
    self.add_output_string("EqEqual", "==");
}
</t>
<t tx="ekr.20240929074037.51">fn do_Equal(&amp;mut self) {
    self.add_output_string("Equal", "=");
}
</t>
<t tx="ekr.20240929074037.52">fn do_Except(&amp;mut self) {
    self.add_output_string("Except", "except");
}
</t>
<t tx="ekr.20240929074037.53">fn do_False(&amp;mut self) {
    self.add_output_string("False", "False");
}
</t>
<t tx="ekr.20240929074037.54">fn do_Finally(&amp;mut self) {
    self.add_output_string("Finally", "finally");
}
</t>
<t tx="ekr.20240929074037.55">fn do_For(&amp;mut self) {
    self.add_output_string("For", "for");
}
</t>
<t tx="ekr.20240929074037.56">fn do_From(&amp;mut self) {
    self.add_output_string("From", "from");
}
</t>
<t tx="ekr.20240929074037.57">fn do_Global(&amp;mut self) {
    self.add_output_string("Global", "global");
}
</t>
<t tx="ekr.20240929074037.58">fn do_Greater(&amp;mut self) {
    self.add_output_string("Greater", "&gt;");
}
</t>
<t tx="ekr.20240929074037.59">fn do_GreaterEqual(&amp;mut self) {
    self.add_output_string("GreaterEqual", "&gt;-");
}
</t>
<t tx="ekr.20240929074037.60">fn do_If(&amp;mut self) {
    self.add_output_string("If", "if");
}
</t>
<t tx="ekr.20240929074037.61">fn do_Import(&amp;mut self) {
    self.add_output_string("Import", "import");
}
</t>
<t tx="ekr.20240929074037.62">fn do_In(&amp;mut self) {
    self.add_output_string("In", "in");
}
</t>
<t tx="ekr.20240929074037.63">fn do_Is(&amp;mut self) {
    self.add_output_string("Is", "is");
}
</t>
<t tx="ekr.20240929074037.64">fn do_Lambda(&amp;mut self) {
    self.add_output_string("Lambda", "lambda");
}
</t>
<t tx="ekr.20240929074037.65">fn do_Lbrace(&amp;mut self) {
    self.add_output_string("Lbrace", "[");
}
</t>
<t tx="ekr.20240929074037.66">fn do_LeftShift(&amp;mut self) {
    self.add_output_string("LeftShift", "&lt;&lt;");
}
</t>
<t tx="ekr.20240929074037.67">fn do_LeftShiftEqual(&amp;mut self) {
    self.add_output_string("LeftShiftEqual", "&lt;&lt;=");
}
</t>
<t tx="ekr.20240929074037.68">fn do_Less(&amp;mut self) {
    self.add_output_string("Less", "&lt;");
}
</t>
<t tx="ekr.20240929074037.69">fn do_LessEqual(&amp;mut self) {
    self.add_output_string("LessEqual", "&lt;=");
}
</t>
<t tx="ekr.20240929074037.7"></t>
<t tx="ekr.20240929074037.70">fn do_Lpar(&amp;mut self) {
    self.add_output_string("Lpar", "(");
}
</t>
<t tx="ekr.20240929074037.71">fn do_Lsqb(&amp;mut self) {
    self.add_output_string("Lsqb", "[");
}
</t>
<t tx="ekr.20240929074037.72">fn do_Match(&amp;mut self) {
    self.add_output_string("Match", "match");
}
</t>
<t tx="ekr.20240929074037.73">fn do_Minus(&amp;mut self) {
    self.add_output_string("Minus", "-");
}
</t>
<t tx="ekr.20240929074037.74">fn do_MinusEqual(&amp;mut self) {
    self.add_output_string("MinusEqual", "-=");
}
</t>
<t tx="ekr.20240929074037.75">fn do_None(&amp;mut self) {
    self.add_output_string("None", "None");
}
</t>
<t tx="ekr.20240929074037.76">fn do_Nonlocal(&amp;mut self) {
    self.add_output_string("Nonlocal", "nonlocal");
}
</t>
<t tx="ekr.20240929074037.77">fn do_Not(&amp;mut self) {
    self.add_output_string("Not", "not");
}
</t>
<t tx="ekr.20240929074037.78">fn do_NotEqual(&amp;mut self) {
    self.add_output_string("NotEqual", "!=");
}
</t>
<t tx="ekr.20240929074037.79">fn do_Or(&amp;mut self) {
    self.add_output_string("Or", "or");
}
</t>
<t tx="ekr.20240929074037.8"></t>
<t tx="ekr.20240929074037.80">fn do_Pass(&amp;mut self) {
    self.add_output_string("Pass", "pass");
}
</t>
<t tx="ekr.20240929074037.81">fn do_Percent(&amp;mut self) {
    self.add_output_string("Percent", "%");
}
</t>
<t tx="ekr.20240929074037.82">fn do_PercentEqual(&amp;mut self) {
    self.add_output_string("PercentEqual", "%=");
}
</t>
<t tx="ekr.20240929074037.83">fn do_Plus(&amp;mut self) {
    self.add_output_string("Plus", "+");
}
</t>
<t tx="ekr.20240929074037.84">fn do_PlusEqual(&amp;mut self) {
    self.add_output_string("PlusEqual", "+=");
}
</t>
<t tx="ekr.20240929074037.85">fn do_Raise(&amp;mut self) {
    self.add_output_string("Raise", "raise");
}
</t>
<t tx="ekr.20240929074037.86">fn do_Rarrow(&amp;mut self) {
    self.add_output_string("Rarrow", "-&gt;");
}
</t>
<t tx="ekr.20240929074037.87">fn do_Rbrace(&amp;mut self) {
    self.add_output_string("Rbrace", "]");
}
</t>
<t tx="ekr.20240929074037.88">fn do_Return(&amp;mut self) {
    self.add_output_string("Return", "return");
}
</t>
<t tx="ekr.20240929074037.89">fn do_RightShift(&amp;mut self) {
    self.add_output_string("RightShift", "&gt;&gt;");
}
</t>
<t tx="ekr.20240929074037.9">fn do_Comment(&amp;mut self, tok_value: &amp;str) {
    // print!("{tok_value}");  // Correct.
    // print!("{value} ");  // Wrong!
    self.add_output_string("Comment", tok_value);
}
</t>
<t tx="ekr.20240929074037.90">fn do_RightShiftEqual(&amp;mut self) {
    self.add_output_string("RightShiftEqual", "&gt;&gt;=");
}
</t>
<t tx="ekr.20240929074037.91">fn do_Rpar(&amp;mut self) {
    self.add_output_string("Rpar", ")");
}
</t>
<t tx="ekr.20240929074037.92">fn do_Rsqb(&amp;mut self) {
    self.add_output_string("Rsqb", "]");
}
</t>
<t tx="ekr.20240929074037.93">fn do_Semi(&amp;mut self) {
    self.add_output_string("Semi", ";");
}
</t>
<t tx="ekr.20240929074037.94">fn do_Slash(&amp;mut self) {
    self.add_output_string("Slash", "/");
}
</t>
<t tx="ekr.20240929074037.95">fn do_SlashEqual(&amp;mut self) {
    self.add_output_string("SlashEqual", "/=");
}
</t>
<t tx="ekr.20240929074037.96">fn do_Star(&amp;mut self) {
    self.add_output_string("Star", "*");
}
</t>
<t tx="ekr.20240929074037.97">fn do_StarEqual(&amp;mut self) {
    self.add_output_string("StarEqual", "*=");
}
</t>
<t tx="ekr.20240929074037.98">fn do_StartExpression(&amp;mut self) {
    // self.add_output_string("StartExpression", "");
}
</t>
<t tx="ekr.20240929074037.99">fn do_StartInteractive(&amp;mut self) {
    // self.add_output_string("StartModule", "");
}
</t>
<t tx="ekr.20240929074547.1">#[derive(Debug)]
pub struct Stats {
    // Cumulative statistics for all files.
    n_files: u64,     // Number of files.
    n_tokens: u64,    // Number of tokens.
    n_ws_tokens: u64, // Number of pseudo-ws tokens.

    // Timing stat, in microseconds...
    beautify_time: u128,
    make_tokens_time: u128,
    read_time: u128,
    write_time: u128,
}

// #[allow(dead_code)]
// #[allow(non_snake_case)]
impl Stats {
    @others
}
</t>
<t tx="ekr.20240929074941.1">fn update_times(
    &amp;mut self,
    beautify: u128,
    make_tokens: u128,
    read_time: u128,
    write_time: u128,
) {
    // Update cumulative timing stats.
    self.beautify_time += beautify;
    self.make_tokens_time += make_tokens;
    self.read_time += read_time;
    self.write_time += write_time;
}
</t>
<t tx="ekr.20240929075236.1">fn report(&amp;mut self) {
    // Cumulative counts.
    let n_files = self.n_files;
    let n_tokens = self.n_tokens;
    let n_ws_tokens = self.n_ws_tokens;
    // Print cumulative timing stats, in ms.
    let read_time = self.fmt_ns(self.read_time);
    let make_tokens_time = self.fmt_ns(self.make_tokens_time);
    let beautify_time = self.fmt_ns(self.beautify_time);
    let write_time = self.fmt_ns(self.write_time);
    let total_time = self
        .fmt_ns(self.make_tokens_time + self.read_time + self.beautify_time + self.write_time);
    println!("");
    println!("     files: {n_files}, tokens: {n_tokens}, ws tokens: {n_ws_tokens}");
    println!("       read: {read_time:&gt;7} ms");
    println!("make_tokens: {make_tokens_time:&gt;7} ms");
    println!("   beautify: {beautify_time:&gt;7} ms");
    println!("      write: {write_time:&gt;7} ms");
    println!("      total: {total_time:&gt;7} ms");
}
</t>
<t tx="ekr.20240929080242.1">fn fmt_ns(&amp;mut self, t: u128) -&gt; String {
    //! Convert nanoseconds to fractional milliseconds.
    let ms = t / 1000000;
    let micro = (t % 1000000) / 10000; // 2-places only.
                                       // println!("t: {t:8} ms: {ms:03} micro: {micro:02}");
    return f!("{ms:4}.{micro:02}");
}

</t>
<t tx="ekr.20240929084852.1">@language rest
@wrap

Whether a value is on the stack or the heap affects how the language behaves.

Rules:
- Each value in Rust has an *owner*.
- There can only be one owner at a time.
- When the owner goes out of scope, the value will be dropped.

String literals can't be mutated.  String objects are on the heap and can be mutated.
The *drop* function releases heap objects.

Move is a shallow copy:
@language rust
    let s1 = String::from("hello");
    let s2 = s1;
    println!("{s1}, world!");  // wrong.
@language rest

*clone* makes a deep copy.

Stack-only vars have *copy trait*.
Can't add `copy` trait if object implements `drop`.

Ownership and functions:

Passing a variable to a function will move or copy, just as assignment does. 

Return Values and Scope:

Returning values can also transfer ownership.

References and borrowing:

'&amp;' represents a reference.
References are immutable by defaault.
If you have a mutable reference to a value, you can have no other references to that value. 

Rules of reference:

- At any time, you can have either one mutable reference or any number of immutable references.
- References must always be valid.
</t>
<t tx="ekr.20240930063514.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git commit'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930063546.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git status'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930063740.1">@language python
import os
import subprocess

if c.changed:
    c.save()
for command in ('git add *.rs', 'git status'):
    subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930064435.1">@language python
import os
import subprocess

if c.changed:
    c.save()
command = 'git reset'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930064622.1">@language python
import os
import subprocess
if c.changed:
    c.save()
command = 'git push'
subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20240930084648.1">fn read(file_path: &amp;str) -&gt; String {
    let error_s = f!("Can not read {file_path}");
    return fs::read_to_string(file_path).expect(&amp;error_s);
}
</t>
<t tx="ekr.20240930085546.1">fn lex_contents(contents: &amp;str) -&gt; Vec&lt;(Tok, TextRange)&gt; {
    return lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;();
}
</t>
<t tx="ekr.20240930100553.1">// Compute cumulative stats.
let total_time = fmt_ms(t1.elapsed().as_micros());
let tokens_n = input_list.len();
println!("");
println!("tbo: {short_file_name}");
println!("{n_tokens} lex tokens, {ws_tokens_n} ws_tokens, len(input_list): {tokens_n}");
println!("");
println!("       read: {read_time:&gt;5} ms");
// println!("        lex: {lex_time:&gt;5} ms");
println!("make_tokens: {loop_time:&gt;5} ms");
println!("      total: {total_time:&gt;5} ms");
</t>
<t tx="ekr.20240930100625.1">let t1 = Instant::now();
let contents = read(&amp;file_path);
let read_time = fmt_ms(t1.elapsed().as_micros());
</t>
<t tx="ekr.20240930100707.1">let t4 = Instant::now();
let mut input_list: Vec&lt;InputTok&gt; = Vec::new();
let (n_tokens, ws_tokens_n) = make_input_list(&amp;contents, &amp;mut input_list); ////, tokens);
let loop_time = fmt_ms(t4.elapsed().as_micros());
</t>
<t tx="ekr.20240930101156.1">@language python
import os
import subprocess

if c.changed:
    c.save()
for command in ('git add *.leo', 'git status'):
    subprocess.Popen(command, shell=True).communicate()
</t>
<t tx="ekr.20241001055017.1">@language rust

// A trait defines the functionality a particular type has and can share with other types. 
// Traits are like interfaces.

pub trait Summary {
    fn summarize(&amp;self) -&gt; String;
}

impl Summary for NewsArticle {
    fn summarize(&amp;self) -&gt; String {
        format!("{}, by {} ({})",, self.author, self.location)
    }
}

// Traits can have default implementations for some or all functions.
pub trait Summary {
    fn summarize_author(&amp;self) -&gt; String;

    fn summarize(&amp;self) -&gt; String {
        format!("(Read more from {}...)", self.summarize_author())
    }
}

// ***&amp; Traits as parameters: var: &amp;impl TraitName.

pub fn notify(item1: &amp;impl Summary, item2: &amp;impl Summary) {...}

// *** Trait-bound syntax.  &lt;T: Trait&gt;(item: &amp;T).

pub fn notify&lt;T: Summary&gt;(item1: &amp;T, item2: &amp;T) {...}

// *** Multiple trait bounds Trail + Trait

pub fn notify(item: &amp;(impl Summary + Display)) {...}

pub fn notify&lt;T: Summary + Display&gt;(item: &amp;T) {...}

// *** Where syntax:

fn some_function&lt;T: Display + Clone, U: Clone + Debug&gt;(t: &amp;T, u: &amp;U) -&gt; i32 {...}

fn some_function&lt;T, U&gt;(t: &amp;T, u: &amp;U) -&gt; i32
where
    T: Display + Clone,
    U: Clone + Debug,
{...}

// *** Returning types that implement traits:

// The function can only return a single type.

fn returns_summarizable() -&gt; impl Summary {...}

// The ability to specify a return type only by the trait it implements is
// especially useful in the context of closures and iterators.

// *** We can call o.to_string method defined by the ToString trait on
//     any type that implements the Display trait. 
</t>
<t tx="ekr.20241001060848.1"></t>
<t tx="ekr.20241001071914.1">fn test_loop(contents: &amp;str) {
    let mut n_tokens = 0;
    for token in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        if n_tokens &lt; 10 {
            println!("token: {token:?}")
        }
        n_tokens += 1;
    }
    println!("tokens: {n_tokens}")
}
</t>
<t tx="ekr.20241001073040.1"></t>
<t tx="ekr.20241001093308.1">pub fn entry() {
    // Test code for Vec.
    let mut v: Vec&lt;i32&gt; = Vec::new();
    push(&amp;mut v, 1);
    push(&amp;mut v, 2);
    let mut i = 3;
    while i &lt; 10 {
        push(&amp;mut v, i);
        i += 1
    }
    println!("");
    println!("v: {v:?}");
}

fn push(v: &amp;mut Vec&lt;i32&gt;, val: i32) {
    v.push(val);
}
</t>
<t tx="ekr.20241001093308.2">fn tokenize() {
    &lt;&lt; tokenize: define contents &gt;&gt;
    println!("fn tokenize");
    println!("\nSource:\n{contents}");

    for debug in [true, false].iter() {

        println!("{}", if *debug {"Tokens..."} else {"\nBeautified:"});

        let results = lex(contents, Mode::Module);  // An iterator yielding Option(Tok).
        let mut count = 0;
        let mut lws = String::new();
        for (i, result) in results.enumerate() {
            use Tok::*;
            let token = result.ok().unwrap();
            let (ref tok_class, tok_range) = token;
            let tok_value = &amp;contents[tok_range];

            if *debug {
                let s = format!("{tok_class}");
                print!("\nToken: {s:20} {:?}", tok_value);
            }
            else {
                // Comment(value), Name(name)
                #[allow(unused_variables)]
                match tok_class {
                    Comment(value) =&gt; {
                        // print!("{value} ");  // Wrong!
                        print!("{tok_value}");
                    },
                    Dedent =&gt; {
                        lws.pop();
                        lws.pop();
                        print!("{lws}");
                    },
                    Def =&gt; {
                        print!("{tok_value} ");
                    },
                    Indent =&gt; {
                        lws.push_str("    ");
                        print!("{lws}");
                    },
                    Name {name} =&gt; {
                        print!("{tok_value} ");
                    },
                    Newline =&gt; {
                        print!("{tok_value}");
                        print!("{lws}");
                        if false {  // old
                            println!("");
                            print!("{lws}");
                        }
                    },
                    NonLogicalNewline =&gt; {
                        println!("");
                        print!("{lws}");
                    },
                    Return =&gt; {
                        print!("{tok_value} ");
                    },
                    Tok::String {value, kind, triple_quoted} =&gt; {
                        // correct.
                        print!("{tok_value}");
                        if false {  // incorrect.
                            let quote = if *triple_quoted {"'''"} else {"'"};
                            print!("{:?}:{quote}{value}{quote}", kind);
                        }
                    },
                    _ =&gt; {
                        print!("{tok_value}");
                        if false {
                            // to_string quotes values!
                            let s = tok_class.to_string().replace("'", "");
                            print!("{s}");
                        }
                    },
                }
            }
            count = i
        }
        if *debug {
            println!("\n{count} tokens")
        }
    }
}
</t>
<t tx="ekr.20241001093308.3">let contents = r#"
def test():
# Comment 1.
print('abc')
# Comment 2.
"#;

// print("xyz")
// print(rf'pdb')
// print(fr'pdb2')
// return bool(i &amp; 1)
</t>
<t tx="ekr.20241001100954.1">pub fn new() -&gt; Stats {
    let x = Stats {
        // Cumulative counts.
        n_files: 0,     // Number of files.
        n_tokens: 0,    // Number of tokens.
        n_ws_tokens: 0, // Number of pseudo-ws tokens.

        // Timing stats, in nanoseconds...
        beautify_time: 0,
        make_tokens_time: 0,
        read_time: 0,
        write_time: 0,
    };
    return x;
}
</t>
<t tx="ekr.20241001104914.1"></t>
<t tx="ekr.20241001213229.1">@language rust

    // def no_visitor(self) -&gt; String:  # pragma: no cover
    //    self.oops(f"Unknown kind: {self.input_token.kind!r}")

    &lt;&lt; LB::beautify: init ivars &gt;&gt;

    try:
        // Pre-scan the token list, setting context.s
        self.pre_scan();

        // Init ivars first.
        self.input_token = None;  // ???
        self.pending_lws = "";  // ???
        self.pending_ws = "";  // ???
        self.prev_output_kind = None;    // ???
        self.prev_output_value = None;  // ???

        // Init state.
        self.gen_token("file-start", "");
        self.push_state("file-start");

        // The main loop:
        prev_line_number: i32 = 0;
        for (self.index, self.input_token) in enumerate(input_tokens):
            // Set global for visitors.
            if prev_line_number != self.input_token.line_number {
                prev_line_number = self.input_token.line_number;
            }
            // Call the proper visitor.
            if self.verbatim {
                self.do_verbatim();
            } else {
                // Use match ???
                func = getattr(self, f"do_{self.input_token.kind}", self.no_visitor)
                func()
            }

        // Return the result. ???
        // result = ''.join(self.output_list);
        let mut result = String::new();
        for s in self.output_list {
            result.push_str(s);
        }
        return result;
</t>
<t tx="ekr.20241001213329.1">// Debugging vars...
self.line_number = 0; // was None?

// State vars for whitespace.
self.curly_brackets_level = 0; // Number of unmatched '{' tokens.
self.paren_level = 0; // Number of unmatched '(' tokens.
self.square_brackets_stack = Vec::new(); // A stack of bools, for self.gen_word().
self.indent_level = 0; // Set only by do_indent and do_dedent.

// Parse state.
self.decorator_seen = false; // Set by do_name for do_op.
self.in_arg_list = 0; // &gt; 0 if in an arg list of a def.
self.in_doc_part = false;

// To do.
// self.state_stack = Vec::new();  // list[ParseState] = []  # Stack of ParseState objects.

// Leo-related state.
self.verbatim = false; // True: don't beautify.

// Ivars describing the present input token...
self.index = 0; // The index within the tokens array of the token being scanned.
self.lws = String::new(); // Leading whitespace. Required!
</t>
<t tx="ekr.20241001215023.1">class ParseState:
    """
    A class representing items in the parse state stack.

    The present states:

    'file-start': Ensures the stack stack is never empty.

    'decorator': The last '@' was a decorator.

        do_op():    push_state('decorator')
        do_name():  pops the stack if state.kind == 'decorator'.

    'indent': The indentation level for 'class' and 'def' names.

        do_name():      push_state('indent', self.level)
        do_dendent():   pops the stack once or
                        twice if state.value == self.level.
    """

    def __init__(self, kind: str, value: Union[int, str, None]) -&gt; None:
        self.kind = kind
        self.value = value

    def __repr__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover

    def __str__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover
</t>
<t tx="ekr.20241002054443.1">@language rust

// The clone in LB:beautify appears to be necessary.

// All these FAIL in LB::beautify
 
self.add_output_string(&amp;"Test", &amp;"Value");  // mutable borrow occurs here.
self.output_list.push(&amp;input_token.value); //  expected `String`, found `&amp;String`
self.output_list.push("Value");  // expected `String`, found `&amp;str`
self.output_list.push(&amp;"Value");  // expected `String`, found `&amp;&amp;str`

// Works.
self.output_list.push(input_token.value.to_string());  // Converts str to String
self.output_list.push("value".to_string());  // Converts str to String

// Dispatch

// let kind = &amp;input_token.kind.to_string();  // expected `&amp;String`, found `&amp;str`
// let kind = &amp;input_token.kind; // expected `&amp;String`, found `&amp;str`

// immutable borrow occurs here
    // let kind_s = &amp;input_token.kind;
    // let kind = kind_s.as_str();
    </t>
<t tx="ekr.20241002062655.1">let kind = input_token.kind.as_str();
let value = input_token.kind.as_str();
match kind {
    // Some of these could be replaced by inline code.
    "And" =&gt; self.do_And(),
    "As" =&gt; self.do_As(),
    "Assert" =&gt; self.do_Assert(),
    "At" =&gt; self.do_At(),
    "Break" =&gt; self.do_Break(),
    "Class" =&gt; self.do_Class(),
    "Colon" =&gt; self.do_Colon(),
    "ColonEqual" =&gt; self.do_ColonEqual(),
    "Comma" =&gt; self.do_Comma(),
    "Comment" =&gt; self.do_Comment(value),
    "Continue" =&gt; self.do_Continue(),
    "Dedent" =&gt; self.do_Dedent(value),
    "Def" =&gt; self.do_Def(),
    "Del" =&gt; self.do_Del(),
    "Dot" =&gt; self.do_Dot(),
    "DoubleStar" =&gt; self.do_DoubleStar(),
    "Elif" =&gt; self.do_Elif(),
    "Else" =&gt; self.do_Else(),
    "Equal" =&gt; self.do_Equal(),
    "EqEqual" =&gt; self.do_EqEqual(),
    "Except" =&gt; self.do_Except(),
    "Greater" =&gt; self.do_Greater(),
    "GreaterEqual" =&gt; self.do_GreaterEqual(),
    "False" =&gt; self.do_False(),
    "Finally" =&gt; self.do_Finally(),
    "Float" =&gt; self.do_Float(value),
    "For" =&gt; self.do_For(),
    "From" =&gt; self.do_From(),
    "If" =&gt; self.do_If(),
    "In" =&gt; self.do_In(),
    "Import" =&gt; self.do_Import(),
    "Indent" =&gt; self.do_Indent(value),
    "Int" =&gt; self.do_Int(value),
    "Is" =&gt; self.do_Is(),
    "Less" =&gt; self.do_Less(),
    "LessEqual" =&gt; self.do_LessEqual(),
    "Lbrace" =&gt; self.do_Lbrace(),
    "Lpar" =&gt; self.do_Lpar(),
    "Lsqb" =&gt; self.do_Lsqb(),
    "Minus" =&gt; self.do_Minus(),
    "MinusEqual" =&gt; self.do_MinusEqual(),
    "Name" =&gt; self.do_Name(value),
    "Newline" =&gt; self.do_Newline(),
    "None" =&gt; self.do_None(),
    "NonLogicalNewline" =&gt; self.do_NonLogicalNewline(),
    "Not" =&gt; self.do_Not(),
    "NotEqual" =&gt; self.do_NotEqual(),
    "Or" =&gt; self.do_Or(),
    "Pass" =&gt; self.do_Pass(),
    "Percent" =&gt; self.do_Percent(),
    "Plus" =&gt; self.do_Plus(),
    "PlusEqual" =&gt; self.do_PlusEqual(),
    "Raise" =&gt; self.do_Raise(),
    "Rarrow" =&gt; self.do_Rarrow(),
    "Rbrace" =&gt; self.do_Rbrace(),
    "Return" =&gt; self.do_Return(),
    "Rpar" =&gt; self.do_Rpar(),
    "Rsqb" =&gt; self.do_Rsqb(),
    "Star" =&gt; self.do_Star(),
    "String" =&gt; self.do_String(value),
    "True" =&gt; self.do_True(),
    "Try" =&gt; self.do_Try(),
    "While" =&gt; self.do_While(),
    "With" =&gt; self.do_With(),
    "ws" =&gt; self.do_ws(kind, value),
    _ =&gt; println!("No visitor for: {kind}"),
}
</t>
<t tx="ekr.20241002071143.1">// *** Temporary
#[allow(unused_variables)]
fn do_ws(&amp;mut self, kind: &amp;str, value: &amp;str) {
    //! Handle the "ws" pseudo-token.
    //! Put the whitespace only if if ends with backslash-newline.

    // To do.

    // let last_token = self.input_tokens[self.index - 1];
    // let is_newline = kind in ("nl", "newline");
    // if is_newline {
    // self.pending_lws = val;
    // self.pending_ws = "";
    // }
    // else if "\\\n" in val {
    // self.pending_lws = "";
    // self.pending_ws = val;
    // }
    // else {
    // self.pending_ws = val
    // }
}
</t>
<t tx="ekr.20241002113506.1">// Variant names are necessary, but otherwise not used.
#[allow(unused_variables)]
let class_name = match token {
    // Tokens with values...
    Comment(value) =&gt; "Comment",
    Complex { real, imag } =&gt; "Complex",
    Float { value } =&gt; "Float",
    Int { value } =&gt; "Int",
    Name { name } =&gt; "Name",
    Tok::String {
        value,
        kind,
        triple_quoted,
    } =&gt; "String",

    // Common tokens...
    Class =&gt; "Class",
    Dedent =&gt; "Dedent",
    Def =&gt; "Def",
    Indent =&gt; "Indent",
    Newline =&gt; "Newline",
    NonLogicalNewline =&gt; "NonLogicalNewline",

    // All other tokens...
    Amper =&gt; "Amper",
    AmperEqual =&gt; "AmperEqual",
    And =&gt; "And",
    As =&gt; "As",
    Assert =&gt; "Assert",
    Async =&gt; "Async",
    At =&gt; "At",
    AtEqual =&gt; "AtEqual",
    Await =&gt; "Await",
    Break =&gt; "Break",
    Case =&gt; "Case",
    CircumFlex =&gt; "CircumFlex",
    CircumflexEqual =&gt; "CircumflexEqual",
    Colon =&gt; "Colon",
    ColonEqual =&gt; "ColonEqual",
    Comma =&gt; "Comma",
    Continue =&gt; "Continue",
    Del =&gt; "Del",
    Dot =&gt; "Dot",
    DoubleSlash =&gt; "DoubleSlash",
    DoubleSlashEqual =&gt; "DoubleSlashEqual",
    DoubleStar =&gt; "DoubleStar",
    DoubleStarEqual =&gt; "DoubleStarEqual",
    Elif =&gt; "Elif",
    Ellipsis =&gt; "Ellipsis",
    Else =&gt; "Else",
    EndOfFile =&gt; "EndOfFile",
    EqEqual =&gt; "EqEqual",
    Equal =&gt; "Equal",
    Except =&gt; "Except",
    False =&gt; "False",
    Finally =&gt; "Finally",
    For =&gt; "For",
    From =&gt; "From",
    Global =&gt; "Global",
    Greater =&gt; "Greater",
    GreaterEqual =&gt; "GreaterEqual",
    If =&gt; "If",
    Import =&gt; "Import",
    In =&gt; "In",
    Is =&gt; "Is",
    Lambda =&gt; "Lambda",
    Lbrace =&gt; "Lbrace",
    LeftShift =&gt; "LeftShift",
    LeftShiftEqual =&gt; "LeftShiftEqual",
    Less =&gt; "Less",
    LessEqual =&gt; "LessEqual",
    Lpar =&gt; "Lpar",
    Lsqb =&gt; "Lsqb",
    Match =&gt; "Match",
    Minus =&gt; "Minus",
    MinusEqual =&gt; "MinusEqual",
    None =&gt; "None",
    Nonlocal =&gt; "Nonlocal",
    Not =&gt; "Not",
    NotEqual =&gt; "NotEqual",
    Or =&gt; "Or",
    Pass =&gt; "Pass",
    Percent =&gt; "Percent",
    PercentEqual =&gt; "PercentEqual",
    Plus =&gt; "Plus",
    PlusEqual =&gt; "PlusEqual",
    Raise =&gt; "Raise",
    Rarrow =&gt; "Rarrow",
    Rbrace =&gt; "Rbrace",
    Return =&gt; "Return",
    RightShift =&gt; "RightShift",
    RightShiftEqual =&gt; "RightShiftEqual",
    Rpar =&gt; "Rpar",
    Rsqb =&gt; "Rsqb",
    Semi =&gt; "Semi",
    Slash =&gt; "Slash",
    SlashEqual =&gt; "SlashEqual",
    Star =&gt; "Star",
    StarEqual =&gt; "StarEqual",
    StartExpression =&gt; "StartExpression",
    StartInteractive =&gt; "StartInteractive",
    StartModule =&gt; "StartModule",
    Tilde =&gt; "Tilde",
    True =&gt; "True",
    Try =&gt; "Try",
    Type =&gt; "Type",
    Vbar =&gt; "Vbar",
    VbarEqual =&gt; "VbarEqual",
    While =&gt; "While",
    With =&gt; "With",
    Yield =&gt; "Yield",
};
</t>
<t tx="ekr.20241002163554.1">fn string_to_static_str(&amp;self, s: String) -&gt; &amp;'static str {
    Box::leak(s.into_boxed_str())
}

</t>
<t tx="ekr.20241002164044.1">
    // Calculating the class name directly is slower.

    // Furthermore, ':?' lists any fields that the token has.
    let temp_class_name = f!("{token:?}");

    // println!("{temp_class_name}");
    let class_name: &amp;'static str = self.string_to_static_str(temp_class_name);
    self.add_input_token(class_name, tok_value);
</t>
<t tx="ekr.20241003055233.1">    // Don't use static lifetime!  It won't work with multiple files.
    let temp_contents = fs::read_to_string(file_name).expect("Error reading{file_name}");
    // https://stackoverflow.com/questions/23975391/how-to-convert-a-string-into-a-static-str
    let contents: &amp;'static str = self.string_to_static_str(temp_contents);
    let read_time = t1.elapsed().as_nanos();</t>
<t tx="ekr.20241003062509.1"></t>
<t tx="ekr.20241003063446.1"></t>
<t tx="ekr.20241003063446.10"></t>
<t tx="ekr.20241003063446.100">fn do_Star(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Star", "*");
}
</t>
<t tx="ekr.20241003063446.101">fn do_StarEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "StarEqual", "*=");
}
</t>
<t tx="ekr.20241003063446.102">fn do_StartExpression(output_list: Vec&lt;String&gt;) {
    // add_output_string(output_list, "StartExpression", "");
}
</t>
<t tx="ekr.20241003063446.103">fn do_StartInteractive(output_list: Vec&lt;String&gt;) {
    // add_output_string(output_list, "StartModule", "");
}
</t>
<t tx="ekr.20241003063446.104">fn do_StartModule(output_list: Vec&lt;String&gt;) {
    // add_output_string(output_list, "StartModule", "");
    println!("do_StartModule");
}
</t>
<t tx="ekr.20241003063446.105">fn do_Tilde(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Tilde", "~");
}
</t>
<t tx="ekr.20241003063446.106">fn do_True(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "True", "True");
}
</t>
<t tx="ekr.20241003063446.107">fn do_Try(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Try", "try");
}
</t>
<t tx="ekr.20241003063446.108">fn do_Type(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Type", "type");
}
</t>
<t tx="ekr.20241003063446.109">fn do_Vbar(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Vbar", "|");
}
</t>
<t tx="ekr.20241003063446.11">// *** Temporary
#[allow(unused_variables)]
fn do_ws(output_list: Vec&lt;String&gt;, kind: &amp;str, value: &amp;str) {
    //! Handle the "ws" pseudo-token.
    //! Put the whitespace only if if ends with backslash-newline.

    // To do.

    // let last_token = self.input_tokens[self.index - 1];
    // let is_newline = kind in ("nl", "newline");
    // if is_newline {
    // self.pending_lws = val;
    // self.pending_ws = "";
    // }
    // else if "\\\n" in val {
    // self.pending_lws = "";
    // self.pending_ws = val;
    // }
    // else {
    // self.pending_ws = val
    // }
}
</t>
<t tx="ekr.20241003063446.110">fn do_VbarEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "VbarEqual", "|=");
}
</t>
<t tx="ekr.20241003063446.111">fn do_While(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "While", "while");
}
</t>
<t tx="ekr.20241003063446.112">fn do_With(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "With", "with");
}
</t>
<t tx="ekr.20241003063446.113">fn do_Yield(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Yield", "yield");
}
</t>
<t tx="ekr.20241003063446.114">fn enabled(&amp;self, arg: &amp;str) -&gt; bool {
    //! Beautifier::enabled: return true if the given command-line argument is enabled.
    //! Example:  x.enabled("--report");
    return self.args.contains(&amp;arg.to_string());
}
</t>
<t tx="ekr.20241003063446.115">fn get_args(&amp;mut self) {
    //! Beautifier::get_args: Set the args and files_list ivars.
    let args: Vec&lt;String&gt; = env::args().collect();
    let valid_args = vec![
        "--all",
        "--beautified",
        "--diff",
        "-h",
        "--help",
        "--report",
        "--write",
    ];
    for (i, arg) in args.iter().enumerate() {
        if i &gt; 0 {
            if valid_args.contains(&amp;arg.as_str()) {
                self.args.push(arg.to_string())
            } else if arg.as_str().starts_with("--") || arg.as_str().starts_with("--") {
                println!("Ignoring invalid arg: {arg}");
            } else {
                println!("File: {arg}");
                self.files_list.push(arg.to_string());
            }
        }
    }
}
</t>
<t tx="ekr.20241003063446.116">fn make_input_list(contents: &amp;str, stats: &amp;Stats) -&gt; Vec&lt;InputTok&gt; {
    // Add InputToks to the input_list for every token given by the RustPython lex.
    let mut n_tokens: u64 = 0;
    let mut n_ws_tokens: u64 = 0;
    let mut prev_start: usize = 0;
    let mut input_list: Vec&lt;InputTok&gt; = Vec::new();

    for token_tuple in lex(&amp;contents, Mode::Module)
        .map(|tok| tok.expect("Failed to lex"))
        .collect::&lt;Vec&lt;_&gt;&gt;()
    {
        use Tok::*;
        n_tokens += 1;
        let (token, range) = token_tuple;
        let tok_value = &amp;contents[range];

        // The gem: create a whitespace pseudo-tokens.
        // This code adds maybe about 1 ms when beautifying leoFrame.py.
        // With the gem: 14.1 - 14.5 ms. Without: 13.1 - 13.7 ms.
        let start_i = usize::from(range.start());
        let end_i = usize::from(range.end());
        if start_i &gt; prev_start {
            let ws = &amp;contents[prev_start..start_i];
            add_input_token(&amp;mut input_list, "ws", ws);
            n_ws_tokens += 1
        }
        prev_start = end_i;

        &lt;&lt; Calculate class_name using match token &gt;&gt;
        add_input_token(&amp;mut input_list, class_name, tok_value);
    }
    // Update counts.
    stats.n_tokens += n_tokens;
    stats.n_ws_tokens += n_ws_tokens;
    return input_list;
}
</t>
<t tx="ekr.20241003063446.117">// Variant names are necessary, but otherwise not used.
#[allow(unused_variables)]
let class_name = match token {
    // Tokens with values...
    Comment(value) =&gt; "Comment",
    Complex { real, imag } =&gt; "Complex",
    Float { value } =&gt; "Float",
    Int { value } =&gt; "Int",
    Name { name } =&gt; "Name",
    Tok::String {
        value,
        kind,
        triple_quoted,
    } =&gt; "String",

    // Common tokens...
    Class =&gt; "Class",
    Dedent =&gt; "Dedent",
    Def =&gt; "Def",
    Indent =&gt; "Indent",
    Newline =&gt; "Newline",
    NonLogicalNewline =&gt; "NonLogicalNewline",

    // All other tokens...
    Amper =&gt; "Amper",
    AmperEqual =&gt; "AmperEqual",
    And =&gt; "And",
    As =&gt; "As",
    Assert =&gt; "Assert",
    Async =&gt; "Async",
    At =&gt; "At",
    AtEqual =&gt; "AtEqual",
    Await =&gt; "Await",
    Break =&gt; "Break",
    Case =&gt; "Case",
    CircumFlex =&gt; "CircumFlex",
    CircumflexEqual =&gt; "CircumflexEqual",
    Colon =&gt; "Colon",
    ColonEqual =&gt; "ColonEqual",
    Comma =&gt; "Comma",
    Continue =&gt; "Continue",
    Del =&gt; "Del",
    Dot =&gt; "Dot",
    DoubleSlash =&gt; "DoubleSlash",
    DoubleSlashEqual =&gt; "DoubleSlashEqual",
    DoubleStar =&gt; "DoubleStar",
    DoubleStarEqual =&gt; "DoubleStarEqual",
    Elif =&gt; "Elif",
    Ellipsis =&gt; "Ellipsis",
    Else =&gt; "Else",
    EndOfFile =&gt; "EndOfFile",
    EqEqual =&gt; "EqEqual",
    Equal =&gt; "Equal",
    Except =&gt; "Except",
    False =&gt; "False",
    Finally =&gt; "Finally",
    For =&gt; "For",
    From =&gt; "From",
    Global =&gt; "Global",
    Greater =&gt; "Greater",
    GreaterEqual =&gt; "GreaterEqual",
    If =&gt; "If",
    Import =&gt; "Import",
    In =&gt; "In",
    Is =&gt; "Is",
    Lambda =&gt; "Lambda",
    Lbrace =&gt; "Lbrace",
    LeftShift =&gt; "LeftShift",
    LeftShiftEqual =&gt; "LeftShiftEqual",
    Less =&gt; "Less",
    LessEqual =&gt; "LessEqual",
    Lpar =&gt; "Lpar",
    Lsqb =&gt; "Lsqb",
    Match =&gt; "Match",
    Minus =&gt; "Minus",
    MinusEqual =&gt; "MinusEqual",
    None =&gt; "None",
    Nonlocal =&gt; "Nonlocal",
    Not =&gt; "Not",
    NotEqual =&gt; "NotEqual",
    Or =&gt; "Or",
    Pass =&gt; "Pass",
    Percent =&gt; "Percent",
    PercentEqual =&gt; "PercentEqual",
    Plus =&gt; "Plus",
    PlusEqual =&gt; "PlusEqual",
    Raise =&gt; "Raise",
    Rarrow =&gt; "Rarrow",
    Rbrace =&gt; "Rbrace",
    Return =&gt; "Return",
    RightShift =&gt; "RightShift",
    RightShiftEqual =&gt; "RightShiftEqual",
    Rpar =&gt; "Rpar",
    Rsqb =&gt; "Rsqb",
    Semi =&gt; "Semi",
    Slash =&gt; "Slash",
    SlashEqual =&gt; "SlashEqual",
    Star =&gt; "Star",
    StarEqual =&gt; "StarEqual",
    StartExpression =&gt; "StartExpression",
    StartInteractive =&gt; "StartInteractive",
    StartModule =&gt; "StartModule",
    Tilde =&gt; "Tilde",
    True =&gt; "True",
    Try =&gt; "Try",
    Type =&gt; "Type",
    Vbar =&gt; "Vbar",
    VbarEqual =&gt; "VbarEqual",
    While =&gt; "While",
    With =&gt; "With",
    Yield =&gt; "Yield",
};
</t>
<t tx="ekr.20241003063446.118">fn show_args(&amp;self) {
    println!("Command-line arguments...");
    for (i, arg) in self.args.iter().enumerate() {
        if i &gt; 0 {
            println!("  {arg}");
        }
    }
    for file_arg in self.files_list.iter() {
        println!("  {file_arg}");
    }
}
</t>
<t tx="ekr.20241003063446.119">fn show_help(&amp;self) {
    //! Beautifier::show_help: print the help messages.
    println!(
        "{}",
        textwrap::dedent(
            "
        Beautify or diff files.

        -h --help:      Print this help message and exit.
        --all:          Beautify all files, even unchanged files.
        --beautified:   Report beautified files individually, even if not written.
        --diff:         Show diffs instead of changing files.
        --report:       Print summary report.
        --write:        Write beautifed files (dry-run mode otherwise).
    "
        )
    );
}
</t>
<t tx="ekr.20241003063446.12"></t>
<t tx="ekr.20241003063446.120">fn show_output_list(&amp;self) {
    println!("\nOutput list...");
    for (i, arg) in self.output_list.iter().enumerate() {
        if i &gt; 0 {
            print!("{:?}", arg);
        }
    }
}
</t>
<t tx="ekr.20241003063446.121">fn string_to_static_str(&amp;self, s: String) -&gt; &amp;'static str {
    Box::leak(s.into_boxed_str())
}

</t>
<t tx="ekr.20241003063446.13">fn do_Comment(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    // print!("{tok_value}");  // Correct.
    // print!("{value} ");  // Wrong!
    add_output_string(output_list, "Comment", tok_value);
}
</t>
<t tx="ekr.20241003063446.14">fn do_Complex(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    add_output_string(output_list, "Complex", tok_value);
}
</t>
<t tx="ekr.20241003063446.15">fn do_Float(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    add_output_string(output_list, "Float", tok_value);
}
</t>
<t tx="ekr.20241003063446.16">fn do_Int(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    add_output_string(output_list, "Int", tok_value);
}
</t>
<t tx="ekr.20241003063446.17">fn do_Name(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    add_output_string(output_list, "Name", tok_value);
}
</t>
<t tx="ekr.20241003063446.18">fn do_String(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    // correct.
    // print!("{tok_value}");

    // incorrect.
    // let quote = if *triple_quoted {"'''"} else {"'"};
    // print!("{:?}:{quote}{value}{quote}", kind);

    add_output_string(output_list, "String", tok_value);
}
</t>
<t tx="ekr.20241003063446.19"></t>
<t tx="ekr.20241003063446.20">fn do_Dedent(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    add_output_string(output_list, "Dedent", tok_value);
}
</t>
<t tx="ekr.20241003063446.21">fn do_Indent(output_list: Vec&lt;String&gt;, tok_value: &amp;str) {
    add_output_string(output_list, "Indent", tok_value);
}
</t>
<t tx="ekr.20241003063446.22">fn do_Newline(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Indent", "\n");
}
</t>
<t tx="ekr.20241003063446.23">fn do_NonLogicalNewline(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Indent", "\n");
}
</t>
<t tx="ekr.20241003063446.24"></t>
<t tx="ekr.20241003063446.25">fn do_Amper(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Amper", "&amp;");
}
</t>
<t tx="ekr.20241003063446.26">fn do_AmperEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "AmperEqual", "&amp;=");
}
</t>
<t tx="ekr.20241003063446.27">fn do_And(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "And", "and");
}
</t>
<t tx="ekr.20241003063446.28">fn do_As(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "As", "as");
}
</t>
<t tx="ekr.20241003063446.29">fn do_Assert(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Assert", "assert");
}
</t>
<t tx="ekr.20241003063446.3">// #[allow(dead_code)]
fn add_input_token(input_list: &amp;mut Vec&lt;InputTok&gt;, kind: &amp;str, value: &amp;str) {
    //! Add one token to the output list.
    input_list.push(InputTok {
        kind: kind.to_string(),
        value: value.to_string(),
    });
}
</t>
<t tx="ekr.20241003063446.30">fn do_Async(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Async", "async");
}
</t>
<t tx="ekr.20241003063446.31">fn do_At(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "At", "@");
}
</t>
<t tx="ekr.20241003063446.32">fn do_AtEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "AtEqual", "@=");
}
</t>
<t tx="ekr.20241003063446.33">fn do_Await(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Await", "await");
}
</t>
<t tx="ekr.20241003063446.34">fn do_Break(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Break", "break");
}
</t>
<t tx="ekr.20241003063446.35">fn do_Case(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Case", "case");
}
</t>
<t tx="ekr.20241003063446.36">fn do_CircumFlex(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "CircumFlex", "^");
}
</t>
<t tx="ekr.20241003063446.37">fn do_CircumflexEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "CircumflexEqual", "^=");
}
</t>
<t tx="ekr.20241003063446.38">fn do_Class(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Class", "class");
}
</t>
<t tx="ekr.20241003063446.39">fn do_Colon(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Colon", ":");
}
</t>
<t tx="ekr.20241003063446.4">#[allow(unused_variables)]
fn add_output_string(mut output_list: Vec&lt;String&gt;, kind: &amp;str, value: &amp;str) {
    //! Add value to the output list.
    //! kind is for debugging.
    if !value.is_empty() {
        output_list.push(value.to_string())
    }
}
</t>
<t tx="ekr.20241003063446.40">fn do_ColonEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "ColonEqual", ":=");
}
</t>
<t tx="ekr.20241003063446.41">fn do_Comma(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Comma", ",");
}
</t>
<t tx="ekr.20241003063446.42">fn do_Continue(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Continue", "continue");
}
</t>
<t tx="ekr.20241003063446.43">fn do_Def(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Def", "def");
}
</t>
<t tx="ekr.20241003063446.44">fn do_Del(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Del", "del");
}
</t>
<t tx="ekr.20241003063446.45">fn do_Dot(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Dot", ".");
}
</t>
<t tx="ekr.20241003063446.46">fn do_DoubleSlash(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "DoubleSlash", "//");
}
</t>
<t tx="ekr.20241003063446.47">fn do_DoubleSlashEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "DoubleSlashEqual", "//=");
}
</t>
<t tx="ekr.20241003063446.48">fn do_DoubleStar(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "DoubleStar", "**");
}
</t>
<t tx="ekr.20241003063446.49">fn do_DoubleStarEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "DoubleStarEqual", "**=");
}
</t>
<t tx="ekr.20241003063446.5">fn beautify(input_list: &amp;Vec&lt;InputTok&gt;) -&gt; String {
    //! Beautify the input_tokens, creating the output_list.
    // &lt; &lt; fn: beautify: init vars &gt;&gt;
    let output_list: Vec&lt;String&gt; = Vec::new();
    if true {
        for input_token in input_list {
            &lt;&lt; LB: beautify: dispatch on input_token.kind &gt;&gt;
        }
    }
    // return ''.join(self.output_list);
    let mut result = String::new();
    for s in output_list {
        result.push_str(&amp;s);
    }
    return result;
}
</t>
<t tx="ekr.20241003063446.50">fn do_Elif(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Elif", "elif");
}
</t>
<t tx="ekr.20241003063446.51">fn do_Ellipsis(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Ellipsis", "...");
}
</t>
<t tx="ekr.20241003063446.52">fn do_Else(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Else", "else");
}
</t>
<t tx="ekr.20241003063446.53">fn do_EndOfFile(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "EndOfFile", "EOF");
}
</t>
<t tx="ekr.20241003063446.54">fn do_EqEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "EqEqual", "==");
}
</t>
<t tx="ekr.20241003063446.55">fn do_Equal(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Equal", "=");
}
</t>
<t tx="ekr.20241003063446.56">fn do_Except(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Except", "except");
}
</t>
<t tx="ekr.20241003063446.57">fn do_False(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "False", "False");
}
</t>
<t tx="ekr.20241003063446.58">fn do_Finally(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Finally", "finally");
}
</t>
<t tx="ekr.20241003063446.59">fn do_For(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "For", "for");
}
</t>
<t tx="ekr.20241003063446.6">// Debugging vars...
line_number = 0; // was None?

// State vars for whitespace.
curly_brackets_level = 0; // Number of unmatched '{' tokens.
paren_level = 0; // Number of unmatched '(' tokens.
square_brackets_stack = Vec::new(); // A stack of bools, for gen_word().
indent_level = 0; // Set only by do_indent and do_dedent.

// Parse state.
decorator_seen = false; // Set by do_name for do_op.
in_arg_list = 0; // &gt; 0 if in an arg list of a def.
in_doc_part = false;

// To do.
// state_stack = Vec::new();  // list[ParseState] = []  # Stack of ParseState objects.

// Leo-related state.
verbatim = false; // True: don't beautify.

// Ivars describing the present input token...
index = 0; // The index within the tokens array of the token being scanned.
lws = String::new(); // Leading whitespace. Required!
</t>
<t tx="ekr.20241003063446.60">fn do_From(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "From", "from");
}
</t>
<t tx="ekr.20241003063446.61">fn do_Global(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Global", "global");
}
</t>
<t tx="ekr.20241003063446.62">fn do_Greater(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Greater", "&gt;");
}
</t>
<t tx="ekr.20241003063446.63">fn do_GreaterEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "GreaterEqual", "&gt;-");
}
</t>
<t tx="ekr.20241003063446.64">fn do_If(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "If", "if");
}
</t>
<t tx="ekr.20241003063446.65">fn do_Import(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Import", "import");
}
</t>
<t tx="ekr.20241003063446.66">fn do_In(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "In", "in");
}
</t>
<t tx="ekr.20241003063446.67">fn do_Is(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Is", "is");
}
</t>
<t tx="ekr.20241003063446.68">fn do_Lambda(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Lambda", "lambda");
}
</t>
<t tx="ekr.20241003063446.69">fn do_Lbrace(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Lbrace", "[");
}
</t>
<t tx="ekr.20241003063446.7">let kind = input_token.kind.as_str();
let value = input_token.kind.as_str();
match kind {
    // Some of these could be replaced by inline code.
    "And" =&gt; do_And(output_list),
    "As" =&gt; do_As(output_list),
    "Assert" =&gt; do_Assert(output_list),
    "At" =&gt; do_At(output_list),
    "Break" =&gt; do_Break(output_list),
    "Class" =&gt; do_Class(output_list),
    "Colon" =&gt; do_Colon(output_list),
    "ColonEqual" =&gt; do_ColonEqual(output_list),
    "Comma" =&gt; do_Comma(output_list),
    "Comment" =&gt; do_Comment(output_list, value),
    "Continue" =&gt; do_Continue(output_list),
    "Dedent" =&gt; do_Dedent(output_list, value),
    "Def" =&gt; do_Def(output_list),
    "Del" =&gt; do_Del(output_list),
    "Dot" =&gt; do_Dot(output_list),
    "DoubleStar" =&gt; do_DoubleStar(output_list),
    "Elif" =&gt; do_Elif(output_list),
    "Else" =&gt; do_Else(output_list),
    "Equal" =&gt; do_Equal(output_list),
    "EqEqual" =&gt; do_EqEqual(output_list),
    "Except" =&gt; do_Except(output_list),
    "Greater" =&gt; do_Greater(output_list),
    "GreaterEqual" =&gt; do_GreaterEqual(output_list),
    "False" =&gt; do_False(output_list),
    "Finally" =&gt; do_Finally(output_list),
    "Float" =&gt; do_Float(output_list, value),
    "For" =&gt; do_For(output_list),
    "From" =&gt; do_From(output_list),
    "If" =&gt; do_If(output_list),
    "In" =&gt; do_In(output_list),
    "Import" =&gt; do_Import(output_list),
    "Indent" =&gt; do_Indent(output_list, value),
    "Int" =&gt; do_Int(output_list, value),
    "Is" =&gt; do_Is(output_list),
    "Less" =&gt; do_Less(output_list),
    "LessEqual" =&gt; do_LessEqual(output_list),
    "Lbrace" =&gt; do_Lbrace(output_list),
    "Lpar" =&gt; do_Lpar(output_list),
    "Lsqb" =&gt; do_Lsqb(output_list),
    "Minus" =&gt; do_Minus(output_list),
    "MinusEqual" =&gt; do_MinusEqual(output_list),
    "Name" =&gt; do_Name(output_list, value),
    "Newline" =&gt; do_Newline(output_list),
    "None" =&gt; do_None(output_list),
    "NonLogicalNewline" =&gt; do_NonLogicalNewline(output_list),
    "Not" =&gt; do_Not(output_list),
    "NotEqual" =&gt; do_NotEqual(output_list),
    "Or" =&gt; do_Or(output_list),
    "Pass" =&gt; do_Pass(output_list),
    "Percent" =&gt; do_Percent(output_list),
    "Plus" =&gt; do_Plus(output_list),
    "PlusEqual" =&gt; do_PlusEqual(output_list),
    "Raise" =&gt; do_Raise(output_list),
    "Rarrow" =&gt; do_Rarrow(output_list),
    "Rbrace" =&gt; do_Rbrace(output_list),
    "Return" =&gt; do_Return(output_list),
    "Rpar" =&gt; do_Rpar(output_list),
    "Rsqb" =&gt; do_Rsqb(output_list),
    "Star" =&gt; do_Star(output_list),
    "String" =&gt; do_String(output_list, value),
    "True" =&gt; do_True(output_list),
    "Try" =&gt; do_Try(output_list),
    "While" =&gt; do_While(output_list),
    "With" =&gt; do_With(output_list),
    "ws" =&gt; do_ws(output_list, kind, value),
    _ =&gt; println!("No visitor for: {kind}"),
}
</t>
<t tx="ekr.20241003063446.70">fn do_LeftShift(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "LeftShift", "&lt;&lt;");
}
</t>
<t tx="ekr.20241003063446.71">fn do_LeftShiftEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "LeftShiftEqual", "&lt;&lt;=");
}
</t>
<t tx="ekr.20241003063446.72">fn do_Less(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Less", "&lt;");
}
</t>
<t tx="ekr.20241003063446.73">fn do_LessEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "LessEqual", "&lt;=");
}
</t>
<t tx="ekr.20241003063446.74">fn do_Lpar(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Lpar", "(");
}
</t>
<t tx="ekr.20241003063446.75">fn do_Lsqb(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Lsqb", "[");
}
</t>
<t tx="ekr.20241003063446.76">fn do_Match(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Match", "match");
}
</t>
<t tx="ekr.20241003063446.77">fn do_Minus(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Minus", "-");
}
</t>
<t tx="ekr.20241003063446.78">fn do_MinusEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "MinusEqual", "-=");
}
</t>
<t tx="ekr.20241003063446.79">fn do_None(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "None", "None");
}
</t>
<t tx="ekr.20241003063446.8">pub fn beautify_all_files(files_list: Vec&lt;String&gt;, stats: &amp;Stats) {
    // for file_name in self.files_list.clone() {
    for file_name in files_list {
        beautify_one_file(&amp;file_name, &amp;stats);
    }
}

</t>
<t tx="ekr.20241003063446.80">fn do_Nonlocal(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Nonlocal", "nonlocal");
}
</t>
<t tx="ekr.20241003063446.81">fn do_Not(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Not", "not");
}
</t>
<t tx="ekr.20241003063446.82">fn do_NotEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "NotEqual", "!=");
}
</t>
<t tx="ekr.20241003063446.83">fn do_Or(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Or", "or");
}
</t>
<t tx="ekr.20241003063446.84">fn do_Pass(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Pass", "pass");
}
</t>
<t tx="ekr.20241003063446.85">fn do_Percent(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Percent", "%");
}
</t>
<t tx="ekr.20241003063446.86">fn do_PercentEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "PercentEqual", "%=");
}
</t>
<t tx="ekr.20241003063446.87">fn do_Plus(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Plus", "+");
}
</t>
<t tx="ekr.20241003063446.88">fn do_PlusEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "PlusEqual", "+=");
}
</t>
<t tx="ekr.20241003063446.89">fn do_Raise(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Raise", "raise");
}
</t>
<t tx="ekr.20241003063446.9">fn beautify_one_file(file_name: &amp;str, stats: &amp;Stats) {
    stats.n_files += 1;
    if true {
        // Testing only: print the short file name.
        let file_path = path::Path::new(file_name);
        let os_str = file_path.file_name().unwrap(); // &amp;OsStr
        let short_file_name = os_str.to_str().unwrap();
        println!("{short_file_name}");
    }
    // Read the file into contents (a String).
    let t1 = std::time::Instant::now();
    let contents = fs::read_to_string(file_name).expect("Error reading{file_name}");
    stats.read_time += t1.elapsed().as_nanos();
    // Make the list of input tokens.
    let t2 = std::time::Instant::now();
    let input_list: Vec&lt;InputTok&gt; = make_input_list(&amp;contents, &amp;stats);
    stats.make_tokens_time += t2.elapsed().as_nanos();
    // Beautify.
    let t3 = std::time::Instant::now();
    let results: String = beautify(&amp;input_list);
    let n =results.len();
    println!("len(results): {n}");
    stats.beautify_time += t3.elapsed().as_nanos();
}
</t>
<t tx="ekr.20241003063446.90">fn do_Rarrow(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Rarrow", "-&gt;");
}
</t>
<t tx="ekr.20241003063446.91">fn do_Rbrace(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Rbrace", "]");
}
</t>
<t tx="ekr.20241003063446.92">fn do_Return(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Return", "return");
}
</t>
<t tx="ekr.20241003063446.93">fn do_RightShift(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "RightShift", "&gt;&gt;");
}
</t>
<t tx="ekr.20241003063446.94">fn do_RightShiftEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "RightShiftEqual", "&gt;&gt;=");
}
</t>
<t tx="ekr.20241003063446.95">fn do_Rpar(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Rpar", ")");
}
</t>
<t tx="ekr.20241003063446.96">fn do_Rsqb(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Rsqb", "]");
}
</t>
<t tx="ekr.20241003063446.97">fn do_Semi(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Semi", ";");
}
</t>
<t tx="ekr.20241003063446.98">fn do_Slash(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "Slash", "/");
}
</t>
<t tx="ekr.20241003063446.99">fn do_SlashEqual(output_list: Vec&lt;String&gt;) {
    add_output_string(output_list, "SlashEqual", "/=");
}
</t>
<t tx="ekr.20241003064425.1">#[derive(Debug)]
pub struct Beautifier {
    // Set in LB:beautify_one_file...
    args: Vec&lt;String&gt;,
    files_list: Vec&lt;String&gt;,
    input_list: Vec&lt;InputTok&gt;,
    output_list: Vec&lt;String&gt;,
    stats: Stats,
    // Set in LB:beautify...
    // Debugging
    line_number: i32, // Use -1 instead of None?
    // State vars for whitespace.
    curly_brackets_level: i32,
    indent_level: i32,
    paren_level: i32,
    square_brackets_stack: Vec&lt;bool&gt;,
    // Parse state.
    decorator_seen: bool, // Set by do_name for do_op.
    in_arg_list: i32,     // &gt; 0 if in an arg list of a def.
    in_doc_part: bool,
    // To do
    // state_stack = Vec&lt;ParseState&gt;,  // list[ParseState] = []  # Stack of ParseState objects.
    // Leo-related state.
    verbatim: bool,
    // Ivars describing the present input token.
    index: u32,
    lws: String,
}

///// Temporary.
#[allow(dead_code)]
#[allow(non_snake_case)]
impl Beautifier {
    @others
}
</t>
<t tx="ekr.20241003064504.1"></t>
<t tx="ekr.20241003084853.1"></t>
<t tx="ekr.20241003084902.1">pub fn entry() {
    // Main line of beautifier.
    //** let mut x = Beautifier::new();
   
    // testing.
    println!("");
    let mut stats = Stats {
        n_files: 0, n_tokens: 0, n_ws_tokens: 0,
        beautify_time: 0, make_tokens_time: 0, read_time: 0, write_time: 0,
    };

    for file_path in [
        "C:\\Repos\\leo-editor\\leo\\core\\leoAst.py",
        // "C:\\Repos\\leo-editor\\leo\\core\\leoTokens.py",
        // "C:\\Repos\\leo-editor\\leo\\core\\leoApp.py"
    ] {
        beautify_one_file(&amp;file_path, &amp;stats);
    }
    stats.report();
        
    //*** Not yet.
        // } else {
            // if x.enabled("--help") || x.enabled("-h") {
                // x.show_help();
                // return;
            // }
            // x.show_args();
            // x.beautify_all_files();
        // }
}
</t>
</tnodes>
</leo_file>
